{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1135322",
   "metadata": {},
   "source": [
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "**w01_d03_EDA_quality_preprocessing.ipynb**\n",
    "\n",
    "**Author:** Alberto Diaz Durana  \n",
    "**Date:** November 2025  \n",
    "**Purpose:** Clean data, detect outliers with 3 methods, analyze store performance, understand item coverage\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook accomplishes the following:\n",
    "\n",
    "- Load guayas_sample_300k.pkl and validate characteristics\n",
    "- Handle missing values (onpromotion → fill with False)\n",
    "- Detect outliers using 3 methods: IQR, Z-score, Isolation Forest\n",
    "- Analyze store-level performance (11 stores comparison)\n",
    "- Create item coverage matrix (product availability across stores)\n",
    "- Fill calendar gaps for complete daily time series\n",
    "- Extract date features (year, month, day, day_of_week)\n",
    "\n",
    "---\n",
    "\n",
    "## Business Context\n",
    "\n",
    "**Why data quality matters:**\n",
    "\n",
    "Clean, well-understood data is critical for reliable forecasting. Before modeling, we must:\n",
    "- Eliminate data quality issues (missing values, outliers, negative sales)\n",
    "- Understand store performance patterns (identify top/bottom performers)\n",
    "- Map product availability (which items sell where)\n",
    "- Ensure complete temporal coverage (no date gaps)\n",
    "\n",
    "**Three-method outlier detection:**\n",
    "- **IQR:** Robust, non-parametric (1.5×IQR rule)\n",
    "- **Z-score:** Statistical, parametric (|z| > 3.0)\n",
    "- **Isolation Forest:** ML-based, multivariate patterns\n",
    "- **Triangulation:** High confidence when all 3 methods agree\n",
    "\n",
    "**Deliverables:**\n",
    "- Clean dataset (no missing values, no negatives, outliers flagged)\n",
    "- Store performance report (sales by store, type, city, cluster)\n",
    "- Item coverage matrix (2,296 items × 11 stores)\n",
    "- Complete daily calendar (no date gaps)\n",
    "- Date features for temporal analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Input Dependencies\n",
    "\n",
    "From Day 2:\n",
    "- guayas_sample_300k.pkl (300K rows, 11 stores, 2,296 items)\n",
    "- stores.csv (store metadata)\n",
    "- items.csv (product metadata)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31372c2",
   "metadata": {},
   "source": [
    "## 1. Setup & Load Data\n",
    "\n",
    "**Objective:** Import libraries, load sample dataset, validate characteristics\n",
    "\n",
    "**Activities:**\n",
    "- Import pandas, numpy, matplotlib, seaborn, scikit-learn\n",
    "- Configure environment and paths\n",
    "- Load guayas_sample_300k.pkl\n",
    "- Display shape, dtypes, memory usage\n",
    "- Show first/last rows\n",
    "- Summary statistics\n",
    "\n",
    "**Expected output:** Dataset loaded, initial validation complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Configure environment\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"Package Versions:\")\n",
    "print(f\"  pandas: {pd.__version__}\")\n",
    "print(f\"  numpy: {np.__version__}\")\n",
    "print(f\"  scikit-learn: {IsolationForest.__module__.split('.')[0]}\")\n",
    "print(\"\\nOK - Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c78fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine paths\n",
    "current_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "project_root = current_dir.parent if current_dir.name == 'notebooks' else current_dir\n",
    "\n",
    "# Define path constants\n",
    "DATA_PROCESSED = project_root / 'data' / 'processed'\n",
    "DATA_RAW = project_root / 'data' / 'raw'\n",
    "OUTPUTS = project_root / 'outputs' / 'figures' / 'eda'\n",
    "\n",
    "# Verify paths\n",
    "assert DATA_PROCESSED.exists(), f\"ERROR - Path not found: {DATA_PROCESSED}\"\n",
    "assert DATA_RAW.exists(), f\"ERROR - Path not found: {DATA_RAW}\"\n",
    "\n",
    "print(\"OK - Paths validated:\")\n",
    "print(f\"  Project root: {project_root.resolve()}\")\n",
    "print(f\"  DATA_PROCESSED: {DATA_PROCESSED.resolve()}\")\n",
    "print(f\"  OUTPUTS: {OUTPUTS.resolve()}\")\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"\\nRandom seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample dataset\n",
    "print(\"Loading guayas_sample_300k.pkl...\")\n",
    "df = pd.read_pickle(DATA_PROCESSED / 'guayas_sample_300k.pkl')\n",
    "\n",
    "print(f\"OK - Dataset loaded\")\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "print(\"\\nColumns:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "print(\"\\nValue Counts:\")\n",
    "print(f\"\\nUnique stores: {df['store_nbr'].nunique()}\")\n",
    "print(f\"Unique items: {df['item_nbr'].nunique()}\")\n",
    "print(f\"Unique families: {df['family'].nunique()}\")\n",
    "print(f\"\\nFamily distribution:\")\n",
    "print(df['family'].value_counts())\n",
    "\n",
    "print(\"\\nDate range:\")\n",
    "print(f\"  First date: {df['date'].min()}\")\n",
    "print(f\"  Last date: {df['date'].max()}\")\n",
    "\n",
    "print(\"\\nNegative sales check:\")\n",
    "negative_count = (df['unit_sales'] < 0).sum()\n",
    "print(f\"  Negative unit_sales: {negative_count} rows ({negative_count/len(df)*100:.3f}%)\")\n",
    "if negative_count > 0:\n",
    "    print(f\"  Min value: {df['unit_sales'].min():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b47ade0",
   "metadata": {},
   "source": [
    "## 2. Missing Value Analysis\n",
    "\n",
    "**Objective:** Handle missing values in onpromotion column\n",
    "\n",
    "**Activities:**\n",
    "- Visualize missing pattern (if useful)\n",
    "- Fill onpromotion NaN with 0.0 (False - assume no promotion)\n",
    "- Validate no missing values remain\n",
    "- Document decision in decision log\n",
    "\n",
    "**Expected output:** \n",
    "- Clean dataset with no missing values\n",
    "- Decision documented (DEC-003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze onpromotion missing pattern\n",
    "print(\"onpromotion Missing Value Analysis:\")\n",
    "print(f\"  Total missing: {df['onpromotion'].isnull().sum():,} ({df['onpromotion'].isnull().sum()/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nonpromotion value distribution (before filling):\")\n",
    "print(df['onpromotion'].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nMissing by family:\")\n",
    "missing_by_family = df.groupby('family')['onpromotion'].apply(lambda x: x.isnull().sum())\n",
    "print(missing_by_family)\n",
    "\n",
    "print(\"\\nMissing by year:\")\n",
    "df['year_temp'] = pd.to_datetime(df['date']).dt.year\n",
    "missing_by_year = df.groupby('year_temp')['onpromotion'].apply(lambda x: x.isnull().sum())\n",
    "print(missing_by_year)\n",
    "df.drop('year_temp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1297dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing onpromotion with 0.0 (False - assume no promotion)\n",
    "print(\"Filling onpromotion NaN with 0.0 (assume no promotion)...\")\n",
    "print(f\"Before: {df['onpromotion'].isnull().sum()} missing values\")\n",
    "\n",
    "df['onpromotion'] = df['onpromotion'].fillna(0.0)\n",
    "\n",
    "print(f\"After: {df['onpromotion'].isnull().sum()} missing values\")\n",
    "\n",
    "print(\"\\nonpromotion value distribution (after filling):\")\n",
    "print(df['onpromotion'].value_counts())\n",
    "\n",
    "print(\"\\nPromotion rate:\")\n",
    "promo_rate = (df['onpromotion'] == 1.0).sum() / len(df) * 100\n",
    "print(f\"  Items on promotion: {(df['onpromotion'] == 1.0).sum():,} ({promo_rate:.2f}%)\")\n",
    "print(f\"  Items not on promotion: {(df['onpromotion'] == 0.0).sum():,} ({100-promo_rate:.2f}%)\")\n",
    "\n",
    "print(\"\\nValidate no missing values in dataset:\")\n",
    "print(df.isnull().sum().sum())\n",
    "print(\"OK - No missing values remain\" if df.isnull().sum().sum() == 0 else \"WARNING - Missing values still present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2194cad8",
   "metadata": {},
   "source": [
    "## 3. Outlier Detection - Three Methods\n",
    "\n",
    "**Objective:** Detect outliers using IQR, Z-score, and Isolation Forest; handle negative sales\n",
    "\n",
    "**Activities:**\n",
    "- Handle negative sales (13 rows → clip to 0)\n",
    "- IQR method: 1.5×IQR rule per store-item group\n",
    "- Z-score method: |z| > 3.0 per store-item group\n",
    "- Isolation Forest: ML-based multivariate detection\n",
    "- Compare methods with Venn diagram\n",
    "- Identify high-confidence outliers (all 3 agree)\n",
    "\n",
    "**Expected output:** \n",
    "- Cleaned unit_sales (no negatives)\n",
    "- Outlier flags from 3 methods\n",
    "- Method comparison visualization\n",
    "- Decision log entry (DEC-004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ed5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Handle negative sales (returns)\n",
    "print(\"Step 1: Handling Negative Sales (Returns)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "negative_mask = df['unit_sales'] < 0\n",
    "negative_count = negative_mask.sum()\n",
    "\n",
    "print(f\"Negative sales found: {negative_count} rows ({negative_count/len(df)*100:.4f}%)\")\n",
    "print(\"Note: Negative values represent product returns\")\n",
    "\n",
    "if negative_count > 0:\n",
    "    print(f\"  Min value: {df['unit_sales'].min():.2f}\")\n",
    "    print(f\"\\n  Negative sales details:\")\n",
    "    print(df[negative_mask][['date', 'store_nbr', 'item_nbr', 'unit_sales', 'family']].to_string())\n",
    "    \n",
    "    # Clip to 0 (business decision: forecast demand, not net sales)\n",
    "    df.loc[negative_mask, 'unit_sales'] = 0.0\n",
    "    print(f\"\\nBusiness Decision: Clipped {negative_count} returns to 0\")\n",
    "    print(\"Rationale: Forecasting demand (purchases), not net sales (purchases - returns)\")\n",
    "    print(f\"New min value: {df['unit_sales'].min():.2f}\")\n",
    "else:\n",
    "    print(\"  No negative values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2232e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: IQR Method (Interquartile Range)\n",
    "print(\"\\nStep 2: IQR Method - Robust Outlier Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate Q1, Q3, IQR per store-item group\n",
    "Q1 = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform('quantile', 0.25)\n",
    "Q3 = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform('quantile', 0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Flag outliers using 1.5×IQR rule\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "df['outlier_iqr'] = (df['unit_sales'] < lower_bound) | (df['unit_sales'] > upper_bound)\n",
    "\n",
    "iqr_count = df['outlier_iqr'].sum()\n",
    "print(f\"IQR outliers detected: {iqr_count:,} ({iqr_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nIQR Statistics:\")\n",
    "print(f\"  Mean IQR: {IQR.mean():.2f}\")\n",
    "print(f\"  Median IQR: {IQR.median():.2f}\")\n",
    "print(f\"  Mean upper bound: {upper_bound.mean():.2f}\")\n",
    "\n",
    "print(f\"\\nSample outliers (IQR method):\")\n",
    "print(df[df['outlier_iqr']][['date', 'store_nbr', 'item_nbr', 'unit_sales', 'family']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf59c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Z-Score Method (Statistical)\n",
    "print(\"\\nStep 3: Z-Score Method - Statistical Outlier Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate z-scores per store-item group\n",
    "z_scores = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
    "    lambda x: (x - x.mean()) / x.std() if x.std() > 0 else 0\n",
    ")\n",
    "\n",
    "# Flag extreme outliers (|z| > 3.0)\n",
    "df['outlier_zscore'] = abs(z_scores) > 3.0\n",
    "\n",
    "zscore_count = df['outlier_zscore'].sum()\n",
    "print(f\"Z-score outliers detected: {zscore_count:,} ({zscore_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nZ-Score Statistics:\")\n",
    "print(f\"  Mean |z|: {abs(z_scores).mean():.2f}\")\n",
    "print(f\"  Max |z|: {abs(z_scores).max():.2f}\")\n",
    "print(f\"  % with |z| > 2: {(abs(z_scores) > 2).sum() / len(df) * 100:.2f}%\")\n",
    "print(f\"  % with |z| > 3: {(abs(z_scores) > 3).sum() / len(df) * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\nSample outliers (Z-score method):\")\n",
    "df_temp = df.copy()\n",
    "df_temp['z_score'] = z_scores\n",
    "print(df_temp[df_temp['outlier_zscore']][['date', 'store_nbr', 'item_nbr', 'unit_sales', 'z_score', 'family']].head(10))\n",
    "df_temp.drop('z_score', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Isolation Forest Method (ML-based multivariate)\n",
    "print(\"\\nStep 4: Isolation Forest - ML-Based Multivariate Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare features for Isolation Forest\n",
    "# Convert date to numeric for ML model\n",
    "df['date_numeric'] = pd.to_datetime(df['date']).astype('int64') // 10**9  # Unix timestamp\n",
    "df['day_of_week'] = pd.to_datetime(df['date']).dt.dayofweek\n",
    "df['month'] = pd.to_datetime(df['date']).dt.month\n",
    "\n",
    "# Select features for anomaly detection\n",
    "features_for_iso = ['store_nbr', 'item_nbr', 'unit_sales', 'day_of_week', 'month', 'onpromotion']\n",
    "X = df[features_for_iso].copy()\n",
    "\n",
    "print(f\"Training Isolation Forest on {len(X):,} samples...\")\n",
    "print(f\"Features: {features_for_iso}\")\n",
    "\n",
    "# Train Isolation Forest\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=0.05,  # Expect 5% anomalies\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_estimators=100,\n",
    "    max_samples='auto',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Predict anomalies (-1 = outlier, 1 = inlier)\n",
    "predictions = iso_forest.fit_predict(X)\n",
    "df['outlier_forest'] = (predictions == -1)\n",
    "\n",
    "forest_count = df['outlier_forest'].sum()\n",
    "print(f\"Isolation Forest outliers detected: {forest_count:,} ({forest_count/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nSample outliers (Isolation Forest method):\")\n",
    "print(df[df['outlier_forest']][['date', 'store_nbr', 'item_nbr', 'unit_sales', 'family']].head(10))\n",
    "\n",
    "# Clean up temporary columns\n",
    "df.drop(['date_numeric'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compare Three Methods\n",
    "print(\"\\nStep 5: Method Comparison & Consensus\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count outliers per method\n",
    "print(\"Outlier counts by method:\")\n",
    "print(f\"  IQR:              {df['outlier_iqr'].sum():>7,} ({df['outlier_iqr'].sum()/len(df)*100:>5.2f}%)\")\n",
    "print(f\"  Z-score:          {df['outlier_zscore'].sum():>7,} ({df['outlier_zscore'].sum()/len(df)*100:>5.2f}%)\")\n",
    "print(f\"  Isolation Forest: {df['outlier_forest'].sum():>7,} ({df['outlier_forest'].sum()/len(df)*100:>5.2f}%)\")\n",
    "\n",
    "# Consensus analysis\n",
    "df['outlier_any'] = df['outlier_iqr'] | df['outlier_zscore'] | df['outlier_forest']\n",
    "df['outlier_2plus'] = (df['outlier_iqr'].astype(int) + \n",
    "                        df['outlier_zscore'].astype(int) + \n",
    "                        df['outlier_forest'].astype(int)) >= 2\n",
    "df['outlier_all3'] = df['outlier_iqr'] & df['outlier_zscore'] & df['outlier_forest']\n",
    "\n",
    "print(\"\\nConsensus analysis:\")\n",
    "print(f\"  Flagged by any method:        {df['outlier_any'].sum():>7,} ({df['outlier_any'].sum()/len(df)*100:>5.2f}%)\")\n",
    "print(f\"  Flagged by 2+ methods:        {df['outlier_2plus'].sum():>7,} ({df['outlier_2plus'].sum()/len(df)*100:>5.2f}%)\")\n",
    "print(f\"  Flagged by all 3 (consensus): {df['outlier_all3'].sum():>7,} ({df['outlier_all3'].sum()/len(df)*100:>5.2f}%)\")\n",
    "\n",
    "print(\"\\nMethod overlap:\")\n",
    "print(f\"  IQR only:        {(df['outlier_iqr'] & ~df['outlier_zscore'] & ~df['outlier_forest']).sum():>7,}\")\n",
    "print(f\"  Z-score only:    {(df['outlier_zscore'] & ~df['outlier_iqr'] & ~df['outlier_forest']).sum():>7,}\")\n",
    "print(f\"  Forest only:     {(df['outlier_forest'] & ~df['outlier_iqr'] & ~df['outlier_zscore']).sum():>7,}\")\n",
    "print(f\"  IQR + Z-score:   {(df['outlier_iqr'] & df['outlier_zscore'] & ~df['outlier_forest']).sum():>7,}\")\n",
    "print(f\"  IQR + Forest:    {(df['outlier_iqr'] & df['outlier_forest'] & ~df['outlier_zscore']).sum():>7,}\")\n",
    "print(f\"  Z-score + Forest:{(df['outlier_zscore'] & df['outlier_forest'] & ~df['outlier_iqr']).sum():>7,}\")\n",
    "print(f\"  All 3 methods:   {df['outlier_all3'].sum():>7,}\")\n",
    "\n",
    "print(\"\\nHigh-confidence outliers (all 3 methods agree):\")\n",
    "if df['outlier_all3'].sum() > 0:\n",
    "    print(df[df['outlier_all3']][['date', 'store_nbr', 'item_nbr', 'unit_sales', 'family']].head(20))\n",
    "else:\n",
    "    print(\"  No outliers flagged by all 3 methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79149c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize method comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Outlier counts by method\n",
    "ax1 = axes[0]\n",
    "methods = ['IQR', 'Z-score', 'Forest', 'Any', '2+ methods', 'All 3']\n",
    "counts = [\n",
    "    df['outlier_iqr'].sum(),\n",
    "    df['outlier_zscore'].sum(),\n",
    "    df['outlier_forest'].sum(),\n",
    "    df['outlier_any'].sum(),\n",
    "    df['outlier_2plus'].sum(),\n",
    "    df['outlier_all3'].sum()\n",
    "]\n",
    "colors = ['#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2']\n",
    "bars = ax1.bar(methods, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_ylabel('Outlier Count', fontsize=12)\n",
    "ax1.set_title('Outlier Detection - Method Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height):,}\\n({height/len(df)*100:.2f}%)',\n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 2: Distribution comparison (outliers vs non-outliers)\n",
    "ax2 = axes[1]\n",
    "ax2.boxplot([\n",
    "    df[~df['outlier_any']]['unit_sales'],\n",
    "    df[df['outlier_2plus']]['unit_sales'],\n",
    "    df[df['outlier_all3']]['unit_sales']\n",
    "], labels=['Normal', '2+ methods', 'All 3 methods'], showfliers=False)\n",
    "ax2.set_ylabel('Unit Sales', fontsize=12)\n",
    "ax2.set_title('Sales Distribution by Outlier Consensus', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / '01_outlier_detection_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOK - Visualization saved to outputs/figures/eda/01_outlier_detection_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235bb4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and decision documentation\n",
    "print(\"=\" * 70)\n",
    "print(\"OUTLIER DETECTION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nThree-Method Triangulation Results:\")\n",
    "print(f\"  Total samples: {len(df):,}\")\n",
    "print(f\"  High-confidence outliers (all 3 agree): {df['outlier_all3'].sum():,} ({df['outlier_all3'].sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"  Moderate confidence (2+ methods): {df['outlier_2plus'].sum():,} ({df['outlier_2plus'].sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"  Low confidence (1 method only): {(df['outlier_any'].sum() - df['outlier_2plus'].sum()):,}\")\n",
    "\n",
    "print(\"\\nBusiness Interpretation:\")\n",
    "print(\"  - 0.28% extreme outliers (all 3 methods) likely represent:\")\n",
    "print(\"    • Promotional spikes (holiday periods)\")\n",
    "print(\"    • Bulk purchases (institutional buyers)\")\n",
    "print(\"    • Data entry errors (requires investigation)\")\n",
    "print(\"  - 1.65% moderate outliers (2+ methods) represent:\")\n",
    "print(\"    • Strong promotional effects\")\n",
    "print(\"    • Seasonal demand peaks\")\n",
    "print(\"  - Decision: FLAG but do NOT remove outliers\")\n",
    "print(\"    • Sales spikes are real business events\")\n",
    "print(\"    • Model should learn these patterns\")\n",
    "print(\"    • Flag for separate analysis if needed\")\n",
    "\n",
    "print(\"\\nOutlier flags added to dataset:\")\n",
    "print(\"  - outlier_iqr: IQR method\")\n",
    "print(\"  - outlier_zscore: Z-score method\")\n",
    "print(\"  - outlier_forest: Isolation Forest\")\n",
    "print(\"  - outlier_all3: High-confidence consensus\")\n",
    "\n",
    "print(\"\\nDECISION (DEC-004): Three-Method Outlier Detection\")\n",
    "print(\"  Context: Need robust outlier identification for sales data\")\n",
    "print(\"  Methods: IQR (robust) + Z-score (statistical) + Isolation Forest (ML)\")\n",
    "print(\"  Decision: Flag outliers but retain in dataset\")\n",
    "print(\"  Rationale: Sales spikes are legitimate business events (promotions, holidays)\")\n",
    "print(\"  Impact: 846 high-confidence outliers (0.28%) flagged for investigation\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SECTION 3 COMPLETE - Outlier Detection\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38db920",
   "metadata": {},
   "source": [
    "## 4. Store-Level Performance Analysis\n",
    "\n",
    "**Objective:** Compare sales performance across 11 Guayas stores, analyze by type, city, and cluster\n",
    "\n",
    "**Activities:**\n",
    "- Total sales by store (identify top/bottom performers)\n",
    "- Sales by store type (A/B/C/D/E comparison)\n",
    "- Sales by city (Guayaquil vs Daule/Playas/Libertad)\n",
    "- Sales by cluster (1, 3, 6, 10, 17)\n",
    "- Visualize performance patterns\n",
    "\n",
    "**Expected output:** \n",
    "- Store performance ranking\n",
    "- Type/city/cluster analysis\n",
    "- Visualizations for stakeholder communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b04897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load store metadata for analysis\n",
    "print(\"Loading store metadata...\")\n",
    "df_stores = pd.read_csv(DATA_RAW / 'stores.csv')\n",
    "\n",
    "# Merge with main dataset\n",
    "df = df.merge(df_stores[['store_nbr', 'city', 'state', 'type', 'cluster']], \n",
    "              on='store_nbr', how='left')\n",
    "\n",
    "print(f\"OK - Merged with stores.csv\")\n",
    "print(f\"New columns: {['city', 'state', 'type', 'cluster']}\")\n",
    "print(f\"\\nStore distribution in sample:\")\n",
    "print(df['store_nbr'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d195e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze total sales by store\n",
    "print(\"Store Performance Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "store_performance = df.groupby('store_nbr').agg({\n",
    "    'unit_sales': ['sum', 'mean', 'std', 'count']\n",
    "}).round(2)\n",
    "store_performance.columns = ['Total Sales', 'Avg Sales', 'Std Dev', 'Sample Count']\n",
    "store_performance = store_performance.sort_values('Total Sales', ascending=False)\n",
    "\n",
    "print(\"\\nTop/Bottom 5 Stores by Total Sales:\")\n",
    "print(store_performance)\n",
    "\n",
    "# Add store metadata\n",
    "store_performance = store_performance.merge(\n",
    "    df_stores[['store_nbr', 'city', 'type', 'cluster']],\n",
    "    left_index=True,\n",
    "    right_on='store_nbr'\n",
    ").set_index('store_nbr')\n",
    "\n",
    "print(\"\\nStore Performance with Metadata:\")\n",
    "print(store_performance)\n",
    "\n",
    "print(f\"\\nPerformance spread:\")\n",
    "print(f\"  Top store (#{store_performance.index[0]}): {store_performance.iloc[0]['Total Sales']:,.0f} total sales\")\n",
    "print(f\"  Bottom store (#{store_performance.index[-1]}): {store_performance.iloc[-1]['Total Sales']:,.0f} total sales\")\n",
    "print(f\"  Ratio (Top/Bottom): {store_performance.iloc[0]['Total Sales'] / store_performance.iloc[-1]['Total Sales']:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7108f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze by store type\n",
    "print(\"\\nPerformance by Store Type:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "type_performance = df.groupby('type').agg({\n",
    "    'unit_sales': ['sum', 'mean', 'count'],\n",
    "    'store_nbr': 'nunique'\n",
    "}).round(2)\n",
    "type_performance.columns = ['Total Sales', 'Avg Sales per Transaction', 'Transaction Count', 'Store Count']\n",
    "type_performance = type_performance.sort_values('Total Sales', ascending=False)\n",
    "\n",
    "print(type_performance)\n",
    "\n",
    "print(\"\\nStore Type Characteristics:\")\n",
    "print(f\"  Type A (Premium): 1 store, highest avg sales ({type_performance.loc['A', 'Avg Sales per Transaction']:.2f})\")\n",
    "print(f\"  Type B (Large):   1 store\")\n",
    "print(f\"  Type C (Medium):  3 stores, lowest performance\")\n",
    "print(f\"  Type D (Small):   3 stores, good performance\")\n",
    "print(f\"  Type E (Micro):   3 stores\")\n",
    "\n",
    "# Analyze by city\n",
    "print(\"\\nPerformance by City:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "city_performance = df.groupby('city').agg({\n",
    "    'unit_sales': ['sum', 'mean', 'count'],\n",
    "    'store_nbr': 'nunique'\n",
    "}).round(2)\n",
    "city_performance.columns = ['Total Sales', 'Avg Sales', 'Transaction Count', 'Store Count']\n",
    "city_performance = city_performance.sort_values('Total Sales', ascending=False)\n",
    "\n",
    "print(city_performance)\n",
    "\n",
    "print(f\"\\nGuayaquil dominates: {city_performance.loc['Guayaquil', 'Total Sales'] / city_performance['Total Sales'].sum() * 100:.1f}% of total sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze by cluster\n",
    "print(\"\\nPerformance by Cluster:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cluster_performance = df.groupby('cluster').agg({\n",
    "    'unit_sales': ['sum', 'mean', 'count'],\n",
    "    'store_nbr': 'nunique'\n",
    "}).round(2)\n",
    "cluster_performance.columns = ['Total Sales', 'Avg Sales', 'Transaction Count', 'Store Count']\n",
    "cluster_performance = cluster_performance.sort_values('Total Sales', ascending=False)\n",
    "\n",
    "print(cluster_performance)\n",
    "\n",
    "print(\"\\nCluster Insights:\")\n",
    "for cluster_id in cluster_performance.index:\n",
    "    stores_in_cluster = df[df['cluster'] == cluster_id]['store_nbr'].unique()\n",
    "    print(f\"  Cluster {cluster_id}: {len(stores_in_cluster)} stores (#{', #'.join(map(str, sorted(stores_in_cluster)))})\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STORE PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total stores analyzed: 11\")\n",
    "print(f\"Total sales in sample: {df['unit_sales'].sum():,.0f} units\")\n",
    "print(f\"Performance variation: 4.25x (top vs bottom store)\")\n",
    "print(f\"City concentration: 73.8% in Guayaquil\")\n",
    "print(f\"Top store: #51 (Type A, Cluster 17)\")\n",
    "print(f\"Bottom store: #32 (Type C, Cluster 3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize store performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Total sales by store\n",
    "ax1 = axes[0, 0]\n",
    "store_sales = df.groupby('store_nbr')['unit_sales'].sum().sort_values(ascending=False)\n",
    "bars1 = ax1.bar(range(len(store_sales)), store_sales.values, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax1.set_xticks(range(len(store_sales)))\n",
    "ax1.set_xticklabels([f'#{s}' for s in store_sales.index], rotation=45, ha='right')\n",
    "ax1.set_ylabel('Total Sales (units)', fontsize=11)\n",
    "ax1.set_title('Total Sales by Store (11 Guayas Stores)', fontsize=13, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Highlight top/bottom\n",
    "bars1[0].set_color('#2ca02c')  # Top - green\n",
    "bars1[-1].set_color('#d62728')  # Bottom - red\n",
    "\n",
    "# Plot 2: Average sales by store type\n",
    "ax2 = axes[0, 1]\n",
    "type_avg = df.groupby('type')['unit_sales'].mean().sort_values(ascending=False)\n",
    "colors_type = ['#2ca02c', '#ff7f0e', '#9467bd', '#8c564b', '#e377c2']\n",
    "bars2 = ax2.bar(type_avg.index, type_avg.values, color=colors_type, alpha=0.7, edgecolor='black')\n",
    "ax2.set_ylabel('Average Sales per Transaction', fontsize=11)\n",
    "ax2.set_title('Average Sales by Store Type', fontsize=13, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Plot 3: Sales by city\n",
    "ax3 = axes[1, 0]\n",
    "city_sales = df.groupby('city')['unit_sales'].sum().sort_values(ascending=False)\n",
    "wedges, texts, autotexts = ax3.pie(city_sales.values, labels=city_sales.index, autopct='%1.1f%%',\n",
    "                                     startangle=90, colors=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "ax3.set_title('Sales Distribution by City', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Make percentage text more readable\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontsize(10)\n",
    "    autotext.set_weight('bold')\n",
    "\n",
    "# Plot 4: Sales by cluster\n",
    "ax4 = axes[1, 1]\n",
    "cluster_sales = df.groupby('cluster')['unit_sales'].sum().sort_values(ascending=False)\n",
    "bars4 = ax4.bar([str(c) for c in cluster_sales.index], cluster_sales.values, \n",
    "                color='coral', alpha=0.7, edgecolor='black')\n",
    "ax4.set_xlabel('Cluster', fontsize=11)\n",
    "ax4.set_ylabel('Total Sales (units)', fontsize=11)\n",
    "ax4.set_title('Sales by Store Cluster', fontsize=13, fontweight='bold')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add store count labels\n",
    "for i, (cluster_id, sales) in enumerate(cluster_sales.items()):\n",
    "    store_count = df[df['cluster'] == cluster_id]['store_nbr'].nunique()\n",
    "    ax4.text(i, sales, f'{store_count} stores', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / '02_store_performance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOK - Visualization saved to outputs/figures/eda/02_store_performance_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9b237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4 summary\n",
    "print(\"=\" * 70)\n",
    "print(\"SECTION 4 COMPLETE - Store-Level Performance Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"\\n1. Store Performance Hierarchy:\")\n",
    "print(f\"   • Top performer: Store #51 (Type A, Cluster 17) - 356,659 units\")\n",
    "print(f\"   • Bottom performer: Store #32 (Type C, Cluster 3) - 83,947 units\")\n",
    "print(f\"   • Performance gap: 4.25x\")\n",
    "\n",
    "print(\"\\n2. Store Type Patterns:\")\n",
    "print(f\"   • Type A (Premium): Highest avg sales (9.63 units/transaction)\")\n",
    "print(f\"   • Type C (Medium): Lowest performance (4.78 units/transaction)\")\n",
    "print(f\"   • Types D & E: Similar performance (~6.5-6.9 avg)\")\n",
    "\n",
    "print(\"\\n3. Geographic Concentration:\")\n",
    "print(f\"   • Guayaquil: 73.8% of total sales (8 stores)\")\n",
    "print(f\"   • Other cities: 26.2% combined (Daule, Libertad, Playas)\")\n",
    "\n",
    "print(\"\\n4. Cluster Analysis:\")\n",
    "print(f\"   • Cluster 10: Largest volume (4 stores, 664K units)\")\n",
    "print(f\"   • Cluster 17: Highest efficiency (1 store, 9.63 avg)\")\n",
    "print(f\"   • Cluster 3: Needs investigation (3 stores, lowest avg)\")\n",
    "\n",
    "print(\"\\nBusiness Implications:\")\n",
    "print(\"   → Store #51 best practices should be studied and replicated\")\n",
    "print(\"   → Cluster 3 stores (30, 32, 35) require performance improvement plans\")\n",
    "print(\"   → Type A stores justify premium positioning with 2x avg sales vs Type C\")\n",
    "\n",
    "print(\"\\nTime elapsed: ~1.5 hours / 5.5 hours allocated\")\n",
    "print(\"Status: Ahead of schedule ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09203383",
   "metadata": {},
   "source": [
    "## 5. Item Coverage Analysis\n",
    "\n",
    "**Objective:** Understand product availability patterns - which items sell in which stores\n",
    "\n",
    "**Activities:**\n",
    "- Create store-item availability matrix\n",
    "- Calculate coverage per store (% of 2,296 items sold)\n",
    "- Identify universal items (sold in all stores) vs niche items (few stores)\n",
    "- Analyze coverage by product family\n",
    "- Zero-sales preliminary analysis\n",
    "\n",
    "**Expected output:** \n",
    "- Item coverage matrix\n",
    "- Universal vs niche items classification\n",
    "- Coverage report by store and family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a980a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create store-item coverage matrix\n",
    "print(\"Item Coverage Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Count unique items per store\n",
    "items_per_store = df.groupby('store_nbr')['item_nbr'].nunique().sort_values(ascending=False)\n",
    "\n",
    "print(f\"Total unique items in sample: {df['item_nbr'].nunique():,}\")\n",
    "print(f\"\\nUnique items sold per store:\")\n",
    "for store_nbr, item_count in items_per_store.items():\n",
    "    coverage_pct = item_count / df['item_nbr'].nunique() * 100\n",
    "    store_info = df[df['store_nbr'] == store_nbr][['type', 'city']].iloc[0]\n",
    "    print(f\"  Store #{store_nbr:>2} (Type {store_info['type']}, {store_info['city']:<12}): {item_count:>4,} items ({coverage_pct:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nCoverage statistics:\")\n",
    "print(f\"  Max coverage: {items_per_store.max():,} items ({items_per_store.max()/df['item_nbr'].nunique()*100:.1f}%)\")\n",
    "print(f\"  Min coverage: {items_per_store.min():,} items ({items_per_store.min()/df['item_nbr'].nunique()*100:.1f}%)\")\n",
    "print(f\"  Avg coverage: {items_per_store.mean():.0f} items ({items_per_store.mean()/df['item_nbr'].nunique()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9332b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze item distribution across stores\n",
    "print(\"\\nItem Distribution Across Stores:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Count how many stores each item appears in\n",
    "stores_per_item = df.groupby('item_nbr')['store_nbr'].nunique()\n",
    "\n",
    "print(f\"\\nItems by store count:\")\n",
    "print(f\"  Sold in all 11 stores (universal):  {(stores_per_item == 11).sum():>5,} items ({(stores_per_item == 11).sum()/len(stores_per_item)*100:>5.1f}%)\")\n",
    "print(f\"  Sold in 8-10 stores (widespread):   {((stores_per_item >= 8) & (stores_per_item < 11)).sum():>5,} items ({((stores_per_item >= 8) & (stores_per_item < 11)).sum()/len(stores_per_item)*100:>5.1f}%)\")\n",
    "print(f\"  Sold in 5-7 stores (common):        {((stores_per_item >= 5) & (stores_per_item < 8)).sum():>5,} items ({((stores_per_item >= 5) & (stores_per_item < 8)).sum()/len(stores_per_item)*100:>5.1f}%)\")\n",
    "print(f\"  Sold in 2-4 stores (selective):     {((stores_per_item >= 2) & (stores_per_item < 5)).sum():>5,} items ({((stores_per_item >= 2) & (stores_per_item < 5)).sum()/len(stores_per_item)*100:>5.1f}%)\")\n",
    "print(f\"  Sold in only 1 store (niche):       {(stores_per_item == 1).sum():>5,} items ({(stores_per_item == 1).sum()/len(stores_per_item)*100:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nDistribution statistics:\")\n",
    "print(f\"  Mean stores per item: {stores_per_item.mean():.1f}\")\n",
    "print(f\"  Median stores per item: {stores_per_item.median():.0f}\")\n",
    "\n",
    "# Identify universal and niche items\n",
    "universal_items = stores_per_item[stores_per_item == 11].index.tolist()\n",
    "niche_items = stores_per_item[stores_per_item == 1].index.tolist()\n",
    "\n",
    "print(f\"\\nUniversal items (sold in all 11 stores): {len(universal_items):,}\")\n",
    "print(f\"Niche items (sold in only 1 store): {len(niche_items):,}\")\n",
    "\n",
    "# Check which families have universal items\n",
    "if len(universal_items) > 0:\n",
    "    universal_family_dist = df[df['item_nbr'].isin(universal_items)]['family'].value_counts()\n",
    "    print(f\"\\nUniversal items by family:\")\n",
    "    print(universal_family_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865621f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze coverage by product family\n",
    "print(\"\\nCoverage by Product Family:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "family_coverage = df.groupby(['family', 'store_nbr'])['item_nbr'].nunique().unstack(fill_value=0)\n",
    "\n",
    "print(\"\\nItems per family per store:\")\n",
    "print(family_coverage)\n",
    "\n",
    "# Summary by family\n",
    "family_summary = pd.DataFrame({\n",
    "    'Total Items': df.groupby('family')['item_nbr'].nunique(),\n",
    "    'Avg Items per Store': family_coverage.mean(axis=1).round(0),\n",
    "    'Min Coverage': family_coverage.min(axis=1),\n",
    "    'Max Coverage': family_coverage.max(axis=1),\n",
    "    'Coverage Range': family_coverage.max(axis=1) - family_coverage.min(axis=1)\n",
    "})\n",
    "\n",
    "print(\"\\nFamily Coverage Summary:\")\n",
    "print(family_summary)\n",
    "\n",
    "print(\"\\nInsights:\")\n",
    "for family in family_summary.index:\n",
    "    total = family_summary.loc[family, 'Total Items']\n",
    "    avg = family_summary.loc[family, 'Avg Items per Store']\n",
    "    coverage_pct = (avg / total * 100)\n",
    "    print(f\"  {family}: {avg:.0f}/{total:.0f} items per store ({coverage_pct:.1f}% avg coverage)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36509a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize item coverage patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Items per store\n",
    "ax1 = axes[0, 0]\n",
    "bars1 = ax1.bar(range(len(items_per_store)), items_per_store.values, \n",
    "                color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax1.set_xticks(range(len(items_per_store)))\n",
    "ax1.set_xticklabels([f'#{s}' for s in items_per_store.index], rotation=45, ha='right')\n",
    "ax1.set_ylabel('Unique Items Sold', fontsize=11)\n",
    "ax1.set_title('Item Coverage by Store', fontsize=13, fontweight='bold')\n",
    "ax1.axhline(df['item_nbr'].nunique(), color='red', linestyle='--', alpha=0.5, label='Total Items (2,296)')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Highlight top/bottom\n",
    "bars1[0].set_color('#2ca02c')  # Top\n",
    "bars1[-1].set_color('#d62728')  # Bottom\n",
    "\n",
    "# Plot 2: Distribution of items by store count\n",
    "ax2 = axes[0, 1]\n",
    "store_count_bins = [1, 2, 5, 8, 11, 12]\n",
    "store_count_labels = ['1 store\\n(niche)', '2-4 stores\\n(selective)', \n",
    "                      '5-7 stores\\n(common)', '8-10 stores\\n(widespread)', \n",
    "                      '11 stores\\n(universal)']\n",
    "store_counts = pd.cut(stores_per_item, bins=store_count_bins, labels=store_count_labels, right=False)\n",
    "store_counts_dist = store_counts.value_counts().sort_index()\n",
    "\n",
    "colors2 = ['#d62728', '#ff7f0e', '#ffbb78', '#aec7e8', '#2ca02c']\n",
    "bars2 = ax2.bar(range(len(store_counts_dist)), store_counts_dist.values, \n",
    "                color=colors2, alpha=0.7, edgecolor='black')\n",
    "ax2.set_xticks(range(len(store_counts_dist)))\n",
    "ax2.set_xticklabels(store_counts_dist.index, rotation=0, ha='center')\n",
    "ax2.set_ylabel('Number of Items', fontsize=11)\n",
    "ax2.set_title('Item Distribution by Store Count', fontsize=13, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height):,}\\n({height/len(stores_per_item)*100:.1f}%)',\n",
    "             ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 3: Coverage by family and store\n",
    "ax3 = axes[1, 0]\n",
    "family_coverage_pct = (family_coverage.T / family_summary['Total Items'] * 100).T\n",
    "sns.heatmap(family_coverage_pct, annot=True, fmt='.0f', cmap='YlGnBu', \n",
    "            cbar_kws={'label': 'Coverage %'}, ax=ax3, vmin=60, vmax=95)\n",
    "ax3.set_xlabel('Store Number', fontsize=11)\n",
    "ax3.set_ylabel('Product Family', fontsize=11)\n",
    "ax3.set_title('Item Coverage % by Family and Store', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Plot 4: Coverage summary by family\n",
    "ax4 = axes[1, 1]\n",
    "x = np.arange(len(family_summary))\n",
    "width = 0.25\n",
    "\n",
    "bars_min = ax4.bar(x - width, family_summary['Min Coverage'], width, \n",
    "                   label='Min', color='#d62728', alpha=0.7)\n",
    "bars_avg = ax4.bar(x, family_summary['Avg Items per Store'], width, \n",
    "                   label='Avg', color='#2ca02c', alpha=0.7)\n",
    "bars_max = ax4.bar(x + width, family_summary['Max Coverage'], width, \n",
    "                   label='Max', color='#1f77b4', alpha=0.7)\n",
    "\n",
    "ax4.set_ylabel('Number of Items', fontsize=11)\n",
    "ax4.set_title('Coverage Range by Product Family', fontsize=13, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(family_summary.index, rotation=0)\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / '03_item_coverage_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOK - Visualization saved to outputs/figures/eda/03_item_coverage_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5 summary\n",
    "print(\"=\" * 70)\n",
    "print(\"SECTION 5 COMPLETE - Item Coverage Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"\\n1. Overall Coverage:\")\n",
    "print(f\"   • Total unique items in sample: 2,296\")\n",
    "print(f\"   • Average coverage per store: 1,790 items (78.0%)\")\n",
    "print(f\"   • Coverage range: 1,486 to 2,064 items (64.7% to 89.9%)\")\n",
    "\n",
    "print(\"\\n2. Item Distribution Patterns:\")\n",
    "print(f\"   • Universal items (all 11 stores): 1,124 (49.0%)\")\n",
    "print(f\"   • Widespread items (8-10 stores): 481 (20.9%)\")\n",
    "print(f\"   • Common items (5-7 stores): 260 (11.3%)\")\n",
    "print(f\"   • Selective items (2-4 stores): 390 (17.0%)\")\n",
    "print(f\"   • Niche items (1 store only): 41 (1.8%)\")\n",
    "\n",
    "print(\"\\n3. Store Type Correlation:\")\n",
    "print(f\"   • Type A (Store #51): Best coverage (89.9% - 2,064 items)\")\n",
    "print(f\"   • Type D stores: Good coverage (81-88%)\")\n",
    "print(f\"   • Type C stores: Lowest coverage (65-66%)\")\n",
    "\n",
    "print(\"\\n4. Family Coverage:\")\n",
    "print(f\"   • BEVERAGES: 81.0% avg coverage (485/599 items per store)\")\n",
    "print(f\"   • CLEANING: 77.1% avg coverage (340/441 items per store)\")\n",
    "print(f\"   • GROCERY I: 76.9% avg coverage (966/1,256 items per store)\")\n",
    "\n",
    "print(\"\\nBusiness Implications:\")\n",
    "print(\"   → Nearly half of items (49%) are universal - strong core assortment\")\n",
    "print(\"   → Type C stores need assortment expansion (15-25% gap vs top stores)\")\n",
    "print(\"   → Only 1.8% niche items - minimal fragmentation\")\n",
    "print(\"   → Mean 8.6 stores per item indicates good distribution efficiency\")\n",
    "\n",
    "print(\"\\nTime elapsed: ~2.0 hours / 5.5 hours allocated\")\n",
    "print(\"Status: Ahead of schedule ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee7a6a8",
   "metadata": {},
   "source": [
    "## 6. Calendar Gap Filling\n",
    "\n",
    "**Objective:** Create complete daily time series for each store-item pair\n",
    "\n",
    "**Activities:**\n",
    "- Convert date to datetime format\n",
    "- Identify missing dates per store-item group\n",
    "- Fill calendar gaps with zero sales (complete daily index)\n",
    "- Validate completeness\n",
    "- Compare row count before/after\n",
    "\n",
    "**Expected output:** \n",
    "- Complete daily calendar (no gaps)\n",
    "- Expanded dataset with zero-filled missing dates\n",
    "- Validation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43db03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime\n",
    "print(\"Calendar Gap Filling\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"Step 1: Convert date to datetime format...\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"  Date dtype: {df['date'].dtype}\")\n",
    "print(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"  Total days: {(df['date'].max() - df['date'].min()).days + 1} days\")\n",
    "\n",
    "# Check current temporal coverage\n",
    "print(\"\\nStep 2: Analyze temporal coverage...\")\n",
    "print(f\"  Current rows: {len(df):,}\")\n",
    "print(f\"  Unique dates in data: {df['date'].nunique():,}\")\n",
    "print(f\"  Unique store-item pairs: {df.groupby(['store_nbr', 'item_nbr']).ngroups:,}\")\n",
    "\n",
    "# Calculate expected rows (if complete)\n",
    "expected_rows = (df['date'].max() - df['date'].min()).days + 1\n",
    "expected_rows *= df.groupby(['store_nbr', 'item_nbr']).ngroups\n",
    "\n",
    "print(f\"\\nExpected rows (complete calendar): {expected_rows:,}\")\n",
    "print(f\"Current rows: {len(df):,}\")\n",
    "print(f\"Gap: {expected_rows - len(df):,} missing date-store-item combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f25f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sparsity and make filling decision\n",
    "print(\"\\nStep 3: Sparsity Analysis...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sparsity = (1 - len(df) / expected_rows) * 100\n",
    "print(f\"  Data sparsity: {sparsity:.1f}%\")\n",
    "print(f\"  Coverage: {(len(df) / expected_rows) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nBusiness Reality:\")\n",
    "print(\"  → Most store-item-date combinations have ZERO sales\")\n",
    "print(\"  → Items not sold on specific dates (not stocked/no demand)\")\n",
    "print(\"  → Filling ALL gaps would create 33.2M row dataset (110x expansion)\")\n",
    "\n",
    "print(\"\\nDecision Point:\")\n",
    "print(\"  Option A: Fill ALL gaps → 33M rows (unmanageable for development)\")\n",
    "print(\"  Option B: Fill gaps only for 'active' items → Still very large\")\n",
    "print(\"  Option C: Keep sparse format → 300K rows (manageable)\")\n",
    "\n",
    "print(\"\\nRECOMMENDATION: Option C - Keep Sparse Format\")\n",
    "print(\"  Rationale:\")\n",
    "print(\"    • Retail data is naturally sparse (items not sold every day)\")\n",
    "print(\"    • 33M rows exceeds development budget (memory/time)\")\n",
    "print(\"    • Most forecasting models handle missing dates internally\")\n",
    "print(\"    • Zero-filling appropriate only for true stockouts (needs investigation)\")\n",
    "\n",
    "print(\"\\nAlternative for modeling phase:\")\n",
    "print(\"  → Create subset of 'active items' (sold ≥10% of days)\")\n",
    "print(\"  → Use time series models that handle irregular intervals\")\n",
    "print(\"  → Document as known limitation in project\")\n",
    "\n",
    "print(\"\\nDECISION (DEC-005): Sparse Data Handling\")\n",
    "print(\"  Context: 99.1% of store-item-date combinations have no sales\")\n",
    "print(\"  Decision: Keep sparse format (300K rows), do NOT fill all calendar gaps\")\n",
    "print(\"  Rationale: Retail reality, memory constraints, modeling flexibility\")\n",
    "print(\"  Impact: Models must handle irregular time intervals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate sparse format decision\n",
    "print(\"\\nStep 4: Validate Sparse Format...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check date completeness per item (sample analysis)\n",
    "sample_items = df.groupby('item_nbr')['date'].count().nlargest(10)\n",
    "\n",
    "print(\"Top 10 most frequently sold items (days with sales):\")\n",
    "for item_nbr, days_sold in sample_items.items():\n",
    "    total_days = (df['date'].max() - df['date'].min()).days + 1\n",
    "    frequency_pct = days_sold / total_days * 100\n",
    "    family = df[df['item_nbr'] == item_nbr]['family'].iloc[0]\n",
    "    print(f\"  Item #{item_nbr}: {days_sold:>4} days ({frequency_pct:>5.1f}%) - {family}\")\n",
    "\n",
    "print(f\"\\nMedian sales frequency across all items:\")\n",
    "median_freq = df.groupby('item_nbr')['date'].count().median()\n",
    "total_days = (df['date'].max() - df['date'].min()).days + 1\n",
    "print(f\"  {median_freq:.0f} days sold out of {total_days} total days ({median_freq/total_days*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"  → Even top items sold only ~25% of days\")\n",
    "print(\"  → Median item sold <10% of days\")\n",
    "print(\"  → Sparse format is appropriate\")\n",
    "print(\"  → Calendar completeness NOT required for this project\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SECTION 6 COMPLETE - Calendar Analysis (Sparse Format Retained)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9609d86",
   "metadata": {},
   "source": [
    "## 7. Date Feature Extraction\n",
    "\n",
    "**Objective:** Create temporal features for time series analysis\n",
    "\n",
    "**Activities:**\n",
    "- Extract year, month, day, day_of_week\n",
    "- Add is_weekend flag\n",
    "- Add day_of_month, week_of_year\n",
    "- Validate feature distributions\n",
    "- Final dataset summary\n",
    "\n",
    "**Expected output:** \n",
    "- 6+ date-based features\n",
    "- Clean, analysis-ready dataset\n",
    "- Day 3 completion summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date features\n",
    "print(\"Date Feature Extraction\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"Creating temporal features...\")\n",
    "\n",
    "# Basic date components (some already exist from outlier detection)\n",
    "if 'year' not in df.columns:\n",
    "    df['year'] = df['date'].dt.year\n",
    "if 'month' not in df.columns:\n",
    "    df['month'] = df['date'].dt.month\n",
    "if 'day_of_week' not in df.columns:\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "\n",
    "# Additional features\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_of_month'] = df['date'].dt.day\n",
    "df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)  # Saturday=5, Sunday=6\n",
    "df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "\n",
    "print(f\"OK - Created {10} temporal features\")\n",
    "\n",
    "print(\"\\nTemporal features created:\")\n",
    "temporal_features = ['year', 'month', 'day', 'day_of_week', 'day_of_month', \n",
    "                     'week_of_year', 'quarter', 'is_weekend', 'is_month_start', 'is_month_end']\n",
    "for feat in temporal_features:\n",
    "    print(f\"  • {feat}\")\n",
    "\n",
    "print(f\"\\nCurrent dataset shape: {df.shape}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77581833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate temporal feature distributions\n",
    "print(\"\\nTemporal Feature Validation:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nYear distribution:\")\n",
    "print(df['year'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nMonth distribution:\")\n",
    "print(df['month'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nDay of week distribution (0=Monday, 6=Sunday):\")\n",
    "dow_counts = df['day_of_week'].value_counts().sort_index()\n",
    "dow_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "for dow, count in dow_counts.items():\n",
    "    print(f\"  {dow} ({dow_names[dow]}): {count:>7,} ({count/len(df)*100:>5.2f}%)\")\n",
    "\n",
    "print(\"\\nQuarter distribution:\")\n",
    "print(df['quarter'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nWeekend vs Weekday:\")\n",
    "print(f\"  Weekday: {(df['is_weekend'] == 0).sum():>7,} ({(df['is_weekend'] == 0).sum()/len(df)*100:>5.2f}%)\")\n",
    "print(f\"  Weekend: {(df['is_weekend'] == 1).sum():>7,} ({(df['is_weekend'] == 1).sum()/len(df)*100:>5.2f}%)\")\n",
    "\n",
    "print(\"\\nMonth start/end:\")\n",
    "print(f\"  Month start: {df['is_month_start'].sum():>6,} ({df['is_month_start'].sum()/len(df)*100:>5.2f}%)\")\n",
    "print(f\"  Month end:   {df['is_month_end'].sum():>6,} ({df['is_month_end'].sum()/len(df)*100:>5.2f}%)\")\n",
    "\n",
    "print(\"\\nOK - All temporal features validated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f5547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset summary\n",
    "print(\"=\" * 70)\n",
    "print(\"DAY 3 COMPLETE - Data Quality & Store Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nFinal Dataset Summary:\")\n",
    "print(f\"  Rows: {len(df):,}\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "print(\"\\nColumn Categories:\")\n",
    "print(f\"  Core data: id, date, store_nbr, item_nbr, unit_sales, onpromotion\")\n",
    "print(f\"  Product metadata: family, class, perishable\")\n",
    "print(f\"  Store metadata: city, state, type, cluster\")\n",
    "print(f\"  Outlier flags: outlier_iqr, outlier_zscore, outlier_forest, outlier_all3, outlier_2plus, outlier_any\")\n",
    "print(f\"  Temporal features: year, month, day, day_of_week, quarter, is_weekend, etc. (10 features)\")\n",
    "\n",
    "print(\"\\nData Quality Status:\")\n",
    "print(f\"  Missing values: {df.isnull().sum().sum()} (0%)\")\n",
    "print(f\"  Negative sales: 0 (already handled)\")\n",
    "print(f\"  Outliers flagged: {df['outlier_all3'].sum():,} high-confidence (0.28%)\")\n",
    "print(f\"  Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "print(f\"  Sparse format: 0.9% coverage (99.1% sparsity - normal for retail)\")\n",
    "\n",
    "print(\"\\nKey Accomplishments:\")\n",
    "print(\"  ✓ Missing values handled (onpromotion filled)\")\n",
    "print(\"  ✓ Three-method outlier detection (IQR + Z-score + Isolation Forest)\")\n",
    "print(\"  ✓ Store performance analyzed (11 stores, 4.25x variation)\")\n",
    "print(\"  ✓ Item coverage mapped (49% universal items)\")\n",
    "print(\"  ✓ Sparsity documented (retail reality)\")\n",
    "print(\"  ✓ Temporal features created (10 features)\")\n",
    "\n",
    "print(\"\\nDecisions Logged:\")\n",
    "print(\"  DEC-003: Fill onpromotion NaN with False\")\n",
    "print(\"  DEC-004: Three-method outlier detection, flag but retain\")\n",
    "print(\"  DEC-005: Keep sparse format (not fill all calendar gaps)\")\n",
    "\n",
    "print(\"\\nReady for Day 4:\")\n",
    "print(\"  → Feature engineering (rolling averages, lags)\")\n",
    "print(\"  → Temporal pattern analysis (seasonality, trends)\")\n",
    "print(\"  → Product dynamics (fast/slow movers)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Time spent: ~4.5 hours / 5.5 hours allocated\")\n",
    "print(f\"Status: 1 hour under budget! ✓\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(\"\\nNext: Save notebook, commit to Git, start Day 4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
