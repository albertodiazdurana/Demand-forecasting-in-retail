{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "994d2425",
   "metadata": {},
   "source": [
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "**w03_d01_MODEL_baseline.ipynb**\n",
    "\n",
    "**Author:** Alberto Diaz Durana  \n",
    "**Date:** November 2025  \n",
    "**Purpose:** Establish XGBoost baseline model with comprehensive evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook accomplishes the following:\n",
    "\n",
    "- Load final feature-engineered dataset from Week 2 (w02_d05_FE_final.pkl)\n",
    "- Create chronological train/test split (Jan-Feb train, March 2014 test)\n",
    "- Train XGBoost baseline model with default parameters\n",
    "- Evaluate with 6 comprehensive metrics (MAE, RMSE, Bias, MAD, rMAD, MAPE)\n",
    "- Visualize predictions and residuals\n",
    "- Document baseline performance for Week 3 comparison\n",
    "\n",
    "---\n",
    "\n",
    "## Business Context\n",
    "\n",
    "**Why baseline modeling matters:**\n",
    "\n",
    "- Establishes performance benchmark for hyperparameter tuning\n",
    "- Validates feature engineering efforts from Week 2\n",
    "- Identifies model strengths and weaknesses before optimization\n",
    "- Provides interpretable metrics for stakeholder communication\n",
    "\n",
    "**Expected outcomes:**\n",
    "- RMSE baseline documented\n",
    "- Feature importance from default XGBoost\n",
    "- Clear under/over-forecasting patterns identified\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e1adfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library versions:\n",
      "pandas: 2.1.4\n",
      "numpy: 1.26.4\n",
      "xgboost: 2.0.3\n",
      "matplotlib: 3.10.7\n",
      "seaborn: 0.13.2\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Project Setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# XGBoost and evaluation\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Library versions:\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"xgboost: {xgb.__version__}\")\n",
    "print(f\"matplotlib: {plt.matplotlib.__version__}\")\n",
    "print(f\"seaborn: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45bb8cd",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Verification\n",
    "\n",
    "**Objective:** Load final feature-engineered dataset from Week 2 Day 5\n",
    "\n",
    "**Activities:**\n",
    "- Load w02_d05_FE_final.pkl (300,896 rows × 57 columns expected)\n",
    "- Verify dataset structure and shape\n",
    "- Check feature count (29 engineered + 28 base = 57 total)\n",
    "- Confirm temporal range (2013-01-02 to 2017-08-15)\n",
    "- Verify no data quality issues before modeling\n",
    "\n",
    "**Expected output:** \n",
    "- Dataset loaded successfully\n",
    "- Shape: (300896, 57)\n",
    "- Date range confirmed\n",
    "- No missing values in critical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaef6525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Project root: D:\\Demand-forecasting-in-retail\n",
      "Data processed: D:\\Demand-forecasting-in-retail\\data\\processed\n",
      "Results output: D:\\Demand-forecasting-in-retail\\data\\results\\features\n",
      "Figures output: D:\\Demand-forecasting-in-retail\\outputs\\figures\\features\n"
     ]
    }
   ],
   "source": [
    "# Determine paths (works from notebooks/ or project root)\n",
    "current_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "PROJECT_ROOT = current_dir.parent if current_dir.name == 'notebooks' else current_dir\n",
    "\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed'\n",
    "DATA_RESULTS = PROJECT_ROOT / 'data' / 'results' / 'features'\n",
    "OUTPUTS_FIGURES = PROJECT_ROOT / 'outputs' / 'figures' / 'features'\n",
    "\n",
    "print(f\"\\nProject root: {PROJECT_ROOT.resolve()}\")\n",
    "print(f\"Data processed: {DATA_PROCESSED.resolve()}\")\n",
    "print(f\"Results output: {DATA_RESULTS.resolve()}\")\n",
    "print(f\"Figures output: {OUTPUTS_FIGURES.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ab19d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully\n",
      "Shape: (300896, 57)\n",
      "\n",
      "Date range: 2013-01-02 00:00:00 to 2017-08-15 00:00:00\n",
      "\n",
      "Column count: 57\n",
      "\n",
      "First few columns:\n",
      "['id', 'date', 'store_nbr', 'item_nbr', 'unit_sales', 'onpromotion', 'family', 'class', 'perishable', 'city']\n",
      "\n",
      "Last few columns:\n",
      "['cluster_avg_sales', 'cluster_median_sales', 'cluster_std_sales', 'item_avg_sales', 'item_median_sales', 'item_std_sales', 'item_count', 'item_total_sales', 'promo_item_avg_interaction', 'promo_cluster_interaction']\n",
      "\n",
      "Data types summary:\n",
      "float64           30\n",
      "int64              9\n",
      "int32              9\n",
      "object             8\n",
      "datetime64[ns]     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Memory usage: 219.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Final Feature-Engineered Dataset\n",
    "\n",
    "# Load the final dataset from Week 2 Day 5\n",
    "file_path = DATA_PROCESSED / 'w02_d05_FE_final.pkl'  \n",
    "df = pd.read_pickle(file_path)\n",
    "\n",
    "print(\"Dataset loaded successfully\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"\\nColumn count: {len(df.columns)}\")\n",
    "print(f\"\\nFirst few columns:\")\n",
    "print(df.columns.tolist()[:10])\n",
    "print(f\"\\nLast few columns:\")\n",
    "print(df.columns.tolist()[-10:])\n",
    "print(f\"\\nData types summary:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a4d55",
   "metadata": {},
   "source": [
    "### 1.1 Data Quality Verification\n",
    "\n",
    "**Objective:** Verify dataset quality before modeling\n",
    "\n",
    "**Checks:**\n",
    "- Temporal order preserved (critical for time series)\n",
    "- Missing values assessment\n",
    "- Feature types validation\n",
    "- Basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b17ea456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA QUALITY CHECK ===\n",
      "\n",
      "1. Temporal Order:\n",
      "   Dataset sorted by (store_nbr, item_nbr, date): True\n",
      "\n",
      "2. Missing Values:\n",
      "   Features with missing values:\n",
      "   - unit_sales_lag30: 291,884 (97.00%)\n",
      "   - holiday_type: 273,698 (90.96%)\n",
      "   - holiday_name: 273,698 (90.96%)\n",
      "   - unit_sales_lag14: 204,230 (67.87%)\n",
      "   - unit_sales_lag7: 119,961 (39.87%)\n",
      "   - unit_sales_lag1: 19,692 (6.54%)\n",
      "   - unit_sales_7d_std: 19,692 (6.54%)\n",
      "   - unit_sales_14d_std: 19,692 (6.54%)\n",
      "   - unit_sales_30d_std: 19,692 (6.54%)\n",
      "   - oil_price_lag30: 30 (0.01%)\n",
      "\n",
      "3. Target Variable (unit_sales):\n",
      "   Mean: 6.79\n",
      "   Median: 3.00\n",
      "   Std: 15.63\n",
      "   Min: -92.00\n",
      "   Max: 2534.00\n",
      "   Zeros: 0 (0.0%)\n",
      "\n",
      "4. Feature Categories:\n",
      "   Total features: 57\n",
      "   - Lag features: 7\n",
      "   - Rolling features: 12\n",
      "   - Oil features: 6\n",
      "   - Aggregation features: 4\n",
      "   - Promotion features: 4\n",
      "\n",
      "5. Date Range for Modeling:\n",
      "   Full range: 2013-01-02 to 2017-08-15\n",
      "   Total days: 1686\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Quality Verification\n",
    "\n",
    "print(\"=== DATA QUALITY CHECK ===\\n\")\n",
    "\n",
    "# Check temporal order\n",
    "print(\"1. Temporal Order:\")\n",
    "is_sorted = df.sort_values(['store_nbr', 'item_nbr', 'date']).equals(df)\n",
    "print(f\"   Dataset sorted by (store_nbr, item_nbr, date): {is_sorted}\")\n",
    "\n",
    "# Missing values\n",
    "print(f\"\\n2. Missing Values:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_features = missing[missing > 0].sort_values(ascending=False)\n",
    "if len(missing_features) > 0:\n",
    "    print(f\"   Features with missing values:\")\n",
    "    for col, count in missing_features.head(10).items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"   - {col}: {count:,} ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"   No missing values found\")\n",
    "\n",
    "# Target variable statistics\n",
    "print(f\"\\n3. Target Variable (unit_sales):\")\n",
    "print(f\"   Mean: {df['unit_sales'].mean():.2f}\")\n",
    "print(f\"   Median: {df['unit_sales'].median():.2f}\")\n",
    "print(f\"   Std: {df['unit_sales'].std():.2f}\")\n",
    "print(f\"   Min: {df['unit_sales'].min():.2f}\")\n",
    "print(f\"   Max: {df['unit_sales'].max():.2f}\")\n",
    "print(f\"   Zeros: {(df['unit_sales'] == 0).sum():,} ({(df['unit_sales'] == 0).sum() / len(df) * 100:.1f}%)\")\n",
    "\n",
    "# Feature categories\n",
    "print(f\"\\n4. Feature Categories:\")\n",
    "print(f\"   Total features: {len(df.columns)}\")\n",
    "lag_features = [col for col in df.columns if 'lag' in col.lower()]\n",
    "rolling_features = [col for col in df.columns if ('avg' in col or 'std' in col) and 'sales' in col]\n",
    "oil_features = [col for col in df.columns if 'oil' in col.lower()]\n",
    "agg_features = [col for col in df.columns if any(x in col for x in ['store_avg', 'cluster_avg', 'item_avg'])]\n",
    "promo_features = [col for col in df.columns if 'promo' in col.lower()]\n",
    "\n",
    "print(f\"   - Lag features: {len(lag_features)}\")\n",
    "print(f\"   - Rolling features: {len(rolling_features)}\")\n",
    "print(f\"   - Oil features: {len(oil_features)}\")\n",
    "print(f\"   - Aggregation features: {len(agg_features)}\")\n",
    "print(f\"   - Promotion features: {len(promo_features)}\")\n",
    "\n",
    "print(f\"\\n5. Date Range for Modeling:\")\n",
    "print(f\"   Full range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "print(f\"   Total days: {(df['date'].max() - df['date'].min()).days}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a35ac39",
   "metadata": {},
   "source": [
    "Key observations:\n",
    "\n",
    "Temporal order preserved \n",
    "High NaN in lag features expected (sparse retail data) \n",
    "No zeros in dataset (sparse format - only recorded sales) \n",
    "Feature categories align with Week 2 engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d232a140",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split (Chronological)\n",
    "\n",
    "**Objective:** Create time-based train/test split for realistic forecasting evaluation\n",
    "\n",
    "**Strategy:**\n",
    "- **Training period:** January-February 2014 (2 months)\n",
    "- **Test period:** March 2014 (1 month)\n",
    "- **No shuffling:** Preserve temporal order (critical for time series)\n",
    "- **Rationale:** Simulate realistic forecast scenario (predict next month using past 2 months)\n",
    "\n",
    "**Expected split:**\n",
    "- Train: ~60-70% of 2014 Q1 data\n",
    "- Test: ~30-40% of 2014 Q1 data\n",
    "- Course requirement: Model Jan-Mar 2014 only (Guayas, top-3 families)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc59c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN/TEST SPLIT (CHRONOLOGICAL) ===\n",
      "\n",
      "1. Filter to Q1 2014 (Jan-Mar):\n",
      "   Original dataset: 300,896 rows\n",
      "   Q1 2014 subset: 12,668 rows (4.2%)\n",
      "   Date range: 2014-01-01 to 2014-03-31\n",
      "\n",
      "2. Chronological Split:\n",
      "   Train (Jan-Feb 2014): 7,982 rows\n",
      "     Date range: 2014-01-01 to 2014-02-28\n",
      "     Days: 59\n",
      "   Test (March 2014): 4,686 rows\n",
      "     Date range: 2014-03-01 to 2014-03-31\n",
      "     Days: 31\n",
      "\n",
      "3. Split Ratio:\n",
      "   Train: 63.0%\n",
      "   Test: 37.0%\n",
      "\n",
      "4. Data Leakage Check:\n",
      "   Latest train date: 2014-02-28\n",
      "   Earliest test date: 2014-03-01\n",
      "   No overlap: True\n",
      "\n",
      "5. Target Distribution:\n",
      "   Train unit_sales - Mean: 7.21, Median: 3.00\n",
      "   Test unit_sales - Mean: 7.26, Median: 3.00\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Chronological Train/Test Split\n",
    "\n",
    "print(\"=== TRAIN/TEST SPLIT (CHRONOLOGICAL) ===\\n\")\n",
    "\n",
    "# Filter to Jan-Mar 2014 only (course requirement)\n",
    "print(\"1. Filter to Q1 2014 (Jan-Mar):\")\n",
    "df_2014q1 = df[(df['date'] >= '2014-01-01') & (df['date'] <= '2014-03-31')].copy()\n",
    "print(f\"   Original dataset: {len(df):,} rows\")\n",
    "print(f\"   Q1 2014 subset: {len(df_2014q1):,} rows ({len(df_2014q1)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Date range: {df_2014q1['date'].min().date()} to {df_2014q1['date'].max().date()}\")\n",
    "\n",
    "# Split: Jan-Feb (train), March (test)\n",
    "print(f\"\\n2. Chronological Split:\")\n",
    "train = df_2014q1[df_2014q1['date'] < '2014-03-01'].copy()\n",
    "test = df_2014q1[df_2014q1['date'] >= '2014-03-01'].copy()\n",
    "\n",
    "print(f\"   Train (Jan-Feb 2014): {len(train):,} rows\")\n",
    "print(f\"     Date range: {train['date'].min().date()} to {train['date'].max().date()}\")\n",
    "print(f\"     Days: {(train['date'].max() - train['date'].min()).days + 1}\")\n",
    "\n",
    "print(f\"   Test (March 2014): {len(test):,} rows\")\n",
    "print(f\"     Date range: {test['date'].min().date()} to {test['date'].max().date()}\")\n",
    "print(f\"     Days: {(test['date'].max() - test['date'].min()).days + 1}\")\n",
    "\n",
    "print(f\"\\n3. Split Ratio:\")\n",
    "print(f\"   Train: {len(train)/len(df_2014q1)*100:.1f}%\")\n",
    "print(f\"   Test: {len(test)/len(df_2014q1)*100:.1f}%\")\n",
    "\n",
    "# Verify no data leakage (test dates > train dates)\n",
    "print(f\"\\n4. Data Leakage Check:\")\n",
    "latest_train_date = train['date'].max()\n",
    "earliest_test_date = test['date'].min()\n",
    "print(f\"   Latest train date: {latest_train_date.date()}\")\n",
    "print(f\"   Earliest test date: {earliest_test_date.date()}\")\n",
    "print(f\"   No overlap: {latest_train_date < earliest_test_date}\")\n",
    "\n",
    "# Target variable distribution\n",
    "print(f\"\\n5. Target Distribution:\")\n",
    "print(f\"   Train unit_sales - Mean: {train['unit_sales'].mean():.2f}, Median: {train['unit_sales'].median():.2f}\")\n",
    "print(f\"   Test unit_sales - Mean: {test['unit_sales'].mean():.2f}, Median: {test['unit_sales'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62c1c1",
   "metadata": {},
   "source": [
    "Gap Period to Prevent Lag Feature Leakage\n",
    "The Issue:\n",
    "\n",
    "Our max lag is 30 days (unit_sales_lag30)\n",
    "If we predict March 1 without a gap:\n",
    "\n",
    "lag1 uses Feb 28 (training data)\n",
    "lag7 uses Feb 22 (training data)\n",
    "lag30 uses Jan 30 (training data)\n",
    "\n",
    "\n",
    "This creates subtle information leakage from training into test predictions\n",
    "\n",
    "Standard Solution:\n",
    "Leave a gap period equal to maximum lag (30 days) between train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd03c5",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split (Chronological with Gap Period)\n",
    "\n",
    "**Objective:** Create time-based train/test split with gap period to prevent lag feature leakage\n",
    "\n",
    "**Strategy:**\n",
    "- **Training period:** January 1 - February 21, 2014 (52 days)\n",
    "- **Gap period:** February 22 - February 28, 2014 (7 days) - EXCLUDED from both sets\n",
    "- **Test period:** March 1 - March 31, 2014 (31 days)\n",
    "- **No shuffling:** Preserve temporal order (critical for time series)\n",
    "\n",
    "**Rationale for 7-day gap:**\n",
    "- Prevents leakage for lag7 feature (strongest autocorrelation: r=0.40)\n",
    "- Balances training data availability (52 days) vs leak prevention\n",
    "- lag1 predictions use gap period data (acceptable trade-off)\n",
    "- lag14 and lag30 still use some training period (documented limitation)\n",
    "\n",
    "**Trade-offs documented:**\n",
    "- Strict 30-day gap would leave only 30 days training (insufficient)\n",
    "- 7-day gap pragmatically prevents most critical leakage\n",
    "- Acceptable for academic project scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0586e502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN/TEST SPLIT WITH GAP PERIOD ===\n",
      "\n",
      "1. Filter to Q1 2014 (Jan-Mar):\n",
      "   Original dataset: 300,896 rows\n",
      "   Q1 2014 subset: 12,668 rows (4.2%)\n",
      "   Date range: 2014-01-01 to 2014-03-31\n",
      "\n",
      "2. Chronological Split with Gap:\n",
      "   Train (Jan 1 - Feb 21, 2014): 7,050 rows\n",
      "     Date range: 2014-01-01 to 2014-02-21\n",
      "     Days: 52\n",
      "   Gap (Feb 22 - Feb 28, 2014): 932 rows [EXCLUDED]\n",
      "     Date range: 2014-02-22 to 2014-02-28\n",
      "     Days: 7\n",
      "     Purpose: Prevent lag7 feature leakage\n",
      "   Test (March 1 - 31, 2014): 4,686 rows\n",
      "     Date range: 2014-03-01 to 2014-03-31\n",
      "     Days: 31\n",
      "\n",
      "3. Split Ratio (excluding gap):\n",
      "   Train: 60.1%\n",
      "   Test: 39.9%\n",
      "   Gap excluded: 932 rows (7.4% of Q1)\n",
      "\n",
      "4. Gap Period Verification:\n",
      "   Latest train date: 2014-02-21\n",
      "   Earliest test date: 2014-03-01\n",
      "   Gap period: 7 days\n",
      "   Prevents lag7 leakage: True\n",
      "\n",
      "5. Lag Feature Leakage Assessment:\n",
      "   lag1 (yesterday): Uses gap period data (acceptable)\n",
      "   lag7 (last week): No leakage (gap prevents)\n",
      "   lag14 (2 weeks ago): Partial overlap with training period\n",
      "   lag30 (1 month ago): Uses training period data\n",
      "   Decision: Pragmatic 7-day gap balances data availability vs leakage\n",
      "\n",
      "6. Target Distribution:\n",
      "   Train unit_sales - Mean: 7.19, Median: 3.00\n",
      "   Test unit_sales - Mean: 7.26, Median: 3.00\n",
      "   Gap unit_sales - Mean: 7.32, Median: 3.00\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Chronological Train/Test Split with Gap Period\n",
    "\n",
    "print(\"=== TRAIN/TEST SPLIT WITH GAP PERIOD ===\\n\")\n",
    "\n",
    "# Filter to Jan-Mar 2014 only (course requirement)\n",
    "print(\"1. Filter to Q1 2014 (Jan-Mar):\")\n",
    "df_2014q1 = df[(df['date'] >= '2014-01-01') & (df['date'] <= '2014-03-31')].copy()\n",
    "print(f\"   Original dataset: {len(df):,} rows\")\n",
    "print(f\"   Q1 2014 subset: {len(df_2014q1):,} rows ({len(df_2014q1)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Date range: {df_2014q1['date'].min().date()} to {df_2014q1['date'].max().date()}\")\n",
    "\n",
    "# Split with 7-day gap: Train (Jan 1 - Feb 21), Gap (Feb 22-28), Test (Mar 1-31)\n",
    "print(f\"\\n2. Chronological Split with Gap:\")\n",
    "train = df_2014q1[df_2014q1['date'] <= '2014-02-21'].copy()\n",
    "gap = df_2014q1[(df_2014q1['date'] > '2014-02-21') & (df_2014q1['date'] < '2014-03-01')].copy()\n",
    "test = df_2014q1[df_2014q1['date'] >= '2014-03-01'].copy()\n",
    "\n",
    "print(f\"   Train (Jan 1 - Feb 21, 2014): {len(train):,} rows\")\n",
    "print(f\"     Date range: {train['date'].min().date()} to {train['date'].max().date()}\")\n",
    "print(f\"     Days: {(train['date'].max() - train['date'].min()).days + 1}\")\n",
    "\n",
    "print(f\"   Gap (Feb 22 - Feb 28, 2014): {len(gap):,} rows [EXCLUDED]\")\n",
    "print(f\"     Date range: {gap['date'].min().date()} to {gap['date'].max().date()}\")\n",
    "print(f\"     Days: {(gap['date'].max() - gap['date'].min()).days + 1}\")\n",
    "print(f\"     Purpose: Prevent lag7 feature leakage\")\n",
    "\n",
    "print(f\"   Test (March 1 - 31, 2014): {len(test):,} rows\")\n",
    "print(f\"     Date range: {test['date'].min().date()} to {test['date'].max().date()}\")\n",
    "print(f\"     Days: {(test['date'].max() - test['date'].min()).days + 1}\")\n",
    "\n",
    "print(f\"\\n3. Split Ratio (excluding gap):\")\n",
    "print(f\"   Train: {len(train)/(len(train)+len(test))*100:.1f}%\")\n",
    "print(f\"   Test: {len(test)/(len(train)+len(test))*100:.1f}%\")\n",
    "print(f\"   Gap excluded: {len(gap):,} rows ({len(gap)/len(df_2014q1)*100:.1f}% of Q1)\")\n",
    "\n",
    "# Verify gap period\n",
    "print(f\"\\n4. Gap Period Verification:\")\n",
    "latest_train_date = train['date'].max()\n",
    "earliest_test_date = test['date'].min()\n",
    "gap_days = (earliest_test_date - latest_train_date).days - 1\n",
    "print(f\"   Latest train date: {latest_train_date.date()}\")\n",
    "print(f\"   Earliest test date: {earliest_test_date.date()}\")\n",
    "print(f\"   Gap period: {gap_days} days\")\n",
    "print(f\"   Prevents lag7 leakage: {gap_days >= 7}\")\n",
    "\n",
    "# Lag feature leakage analysis\n",
    "print(f\"\\n5. Lag Feature Leakage Assessment:\")\n",
    "print(f\"   lag1 (yesterday): Uses gap period data (acceptable)\")\n",
    "print(f\"   lag7 (last week): No leakage (gap prevents)\")\n",
    "print(f\"   lag14 (2 weeks ago): Partial overlap with training period\")\n",
    "print(f\"   lag30 (1 month ago): Uses training period data\")\n",
    "print(f\"   Decision: Pragmatic 7-day gap balances data availability vs leakage\")\n",
    "\n",
    "# Target variable distribution\n",
    "print(f\"\\n6. Target Distribution:\")\n",
    "print(f\"   Train unit_sales - Mean: {train['unit_sales'].mean():.2f}, Median: {train['unit_sales'].median():.2f}\")\n",
    "print(f\"   Test unit_sales - Mean: {test['unit_sales'].mean():.2f}, Median: {test['unit_sales'].median():.2f}\")\n",
    "print(f\"   Gap unit_sales - Mean: {gap['unit_sales'].mean():.2f}, Median: {gap['unit_sales'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1fa1f0",
   "metadata": {},
   "source": [
    "## 3. Feature & Target Separation\n",
    "\n",
    "**Objective:** Prepare X (features) and y (target) for modeling\n",
    "\n",
    "**Strategy:**\n",
    "- **Target variable:** unit_sales (what we predict)\n",
    "- **Features to EXCLUDE:** \n",
    "  - id (identifier, not predictive)\n",
    "  - date (used for split, not as feature)\n",
    "  - store_nbr, item_nbr (identifiers - aggregations already capture patterns)\n",
    "  - unit_sales (target variable)\n",
    "  - Categorical text features (city, state, type, family, class) - use engineered features instead\n",
    "  \n",
    "- **Features to INCLUDE:** All 29 engineered features + relevant base features\n",
    "- **Handle NaN:** XGBoost handles natively (no imputation needed per DEC-011)\n",
    "\n",
    "**Expected result:**\n",
    "- X_train: (7,050 rows × ~40-45 features)\n",
    "- X_test: (4,686 rows × ~40-45 features)\n",
    "- y_train: (7,050 values)\n",
    "- y_test: (4,686 values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a67df601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE & TARGET SEPARATION ===\n",
      "\n",
      "1. Define Features:\n",
      "   Total columns in train: 57\n",
      "   Excluded columns: 12\n",
      "   Feature columns: 45\n",
      "\n",
      "2. Feature Categories:\n",
      "   - Base features: 3\n",
      "   - Lag features: 7\n",
      "   - Rolling features: 12\n",
      "   - Oil features: 6\n",
      "   - Aggregation features: 12\n",
      "   - Promotion interaction features: 3\n",
      "\n",
      "3. Create X and y:\n",
      "   X_train shape: (7050, 45)\n",
      "   y_train shape: (7050,)\n",
      "   X_test shape: (4686, 45)\n",
      "   y_test shape: (4686,)\n",
      "\n",
      "4. Missing Values in Features:\n",
      "   Features with NaN in training set:\n",
      "   - unit_sales_lag30: 7,050 (100.00%)\n",
      "   - unit_sales_lag14: 7,005 (99.36%)\n",
      "   - unit_sales_lag7: 4,842 (68.68%)\n",
      "   - unit_sales_30d_std: 696 (9.87%)\n",
      "   - unit_sales_14d_std: 696 (9.87%)\n",
      "   - unit_sales_7d_std: 696 (9.87%)\n",
      "   - unit_sales_lag1: 696 (9.87%)\n",
      "\n",
      "5. Data Types:\n",
      "   {dtype('float64'): 29, dtype('int32'): 9, dtype('int64'): 5, dtype('O'): 2}\n",
      "\n",
      "6. Feature List (first 15):\n",
      "   1. onpromotion\n",
      "   2. perishable\n",
      "   3. cluster\n",
      "   4. year\n",
      "   5. month\n",
      "   6. day\n",
      "   7. day_of_week\n",
      "   8. day_of_month\n",
      "   9. is_weekend\n",
      "   10. is_holiday\n",
      "   11. days_to_holiday\n",
      "   12. is_pre_holiday\n",
      "   13. is_post_holiday\n",
      "   14. holiday_proximity\n",
      "   15. holiday_period\n",
      "   ... and 30 more features\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Feature & Target Separation\n",
    "\n",
    "print(\"=== FEATURE & TARGET SEPARATION ===\\n\")\n",
    "\n",
    "# Define columns to exclude\n",
    "exclude_cols = [\n",
    "    'id',           # Identifier\n",
    "    'date',         # Used for split, not feature\n",
    "    'store_nbr',    # Identifier (aggregations capture store patterns)\n",
    "    'item_nbr',     # Identifier (aggregations capture item patterns)\n",
    "    'unit_sales',   # Target variable\n",
    "    # Categorical text features (already encoded via aggregations)\n",
    "    'city', 'state', 'type', 'family', 'class',\n",
    "    # Holiday text features (sparse, mostly NaN)\n",
    "    'holiday_name', 'holiday_type'\n",
    "]\n",
    "\n",
    "print(\"1. Define Features:\")\n",
    "feature_cols = [col for col in train.columns if col not in exclude_cols]\n",
    "print(f\"   Total columns in train: {len(train.columns)}\")\n",
    "print(f\"   Excluded columns: {len(exclude_cols)}\")\n",
    "print(f\"   Feature columns: {len(feature_cols)}\")\n",
    "\n",
    "print(f\"\\n2. Feature Categories:\")\n",
    "# Categorize features for documentation\n",
    "base_features = [col for col in feature_cols if col in ['onpromotion', 'perishable', 'cluster']]\n",
    "lag_features = [col for col in feature_cols if 'lag' in col.lower()]\n",
    "rolling_features = [col for col in feature_cols if ('avg' in col or 'std' in col) and 'sales' in col]\n",
    "oil_features = [col for col in feature_cols if 'oil' in col.lower()]\n",
    "agg_features = [col for col in feature_cols if any(x in col for x in ['store_avg', 'cluster_avg', 'item_avg', 'store_median', 'cluster_median', 'item_median', 'store_std', 'cluster_std', 'item_std', 'item_count', 'item_total'])]\n",
    "promo_features = [col for col in feature_cols if 'promo' in col.lower() and col != 'onpromotion']\n",
    "\n",
    "print(f\"   - Base features: {len(base_features)}\")\n",
    "print(f\"   - Lag features: {len(lag_features)}\")\n",
    "print(f\"   - Rolling features: {len(rolling_features)}\")\n",
    "print(f\"   - Oil features: {len(oil_features)}\")\n",
    "print(f\"   - Aggregation features: {len(agg_features)}\")\n",
    "print(f\"   - Promotion interaction features: {len(promo_features)}\")\n",
    "\n",
    "print(f\"\\n3. Create X and y:\")\n",
    "X_train = train[feature_cols].copy()\n",
    "y_train = train['unit_sales'].copy()\n",
    "X_test = test[feature_cols].copy()\n",
    "y_test = test['unit_sales'].copy()\n",
    "\n",
    "print(f\"   X_train shape: {X_train.shape}\")\n",
    "print(f\"   y_train shape: {y_train.shape}\")\n",
    "print(f\"   X_test shape: {X_test.shape}\")\n",
    "print(f\"   y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\n4. Missing Values in Features:\")\n",
    "train_missing = X_train.isnull().sum().sort_values(ascending=False)\n",
    "train_missing_pct = (train_missing / len(X_train) * 100).round(2)\n",
    "features_with_nan = train_missing[train_missing > 0]\n",
    "if len(features_with_nan) > 0:\n",
    "    print(f\"   Features with NaN in training set:\")\n",
    "    for feat, count in features_with_nan.head(10).items():\n",
    "        print(f\"   - {feat}: {count:,} ({train_missing_pct[feat]:.2f}%)\")\n",
    "else:\n",
    "    print(\"   No missing values in training features\")\n",
    "\n",
    "print(f\"\\n5. Data Types:\")\n",
    "print(f\"   {X_train.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\n6. Feature List (first 15):\")\n",
    "for i, col in enumerate(feature_cols[:15], 1):\n",
    "    print(f\"   {i}. {col}\")\n",
    "print(f\"   ... and {len(feature_cols) - 15} more features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
