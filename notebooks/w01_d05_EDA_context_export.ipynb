{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6b8cd9",
   "metadata": {},
   "source": [
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "## w01_d05_EDA_context_export.ipynb\n",
    "\n",
    "**Author:** Alberto Diaz Durana  \n",
    "**Date:** November 2025  \n",
    "**Purpose:** Analyze holidays, promotions, perishables; export final analysis-ready dataset\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook accomplishes the following:\n",
    "\n",
    "- Analyze holiday impact on sales (by holiday type, pre/post effects)\n",
    "- Measure promotion effectiveness (sales lift, frequency)\n",
    "- Investigate promotion × holiday interactions\n",
    "- Compare perishable vs non-perishable patterns\n",
    "- Identify high-volatility items (waste risk)\n",
    "- Analyze oil price correlation\n",
    "- Export final cleaned dataset with all features\n",
    "- Create Week 1 summary report\n",
    "\n",
    "---\n",
    "\n",
    "## Business Context\n",
    "\n",
    "**Why external factors matter:**\n",
    "\n",
    "Understanding holidays, promotions, and product characteristics enables:\n",
    "- Optimal promotional timing (avoid/leverage holidays)\n",
    "- Inventory risk management (perishables require higher accuracy)\n",
    "- Resource planning (holiday staffing, promotional support)\n",
    "- Waste reduction (identify high-volatility perishables)\n",
    "\n",
    "**Deliverables:**\n",
    "- Holiday analysis report with sales lift by type\n",
    "- Promotion effectiveness metrics (ROI quantification)\n",
    "- Perishable waste indicators\n",
    "- Final dataset: guayas_prepared.csv (analysis-ready)\n",
    "- Week 1 summary report\n",
    "\n",
    "---\n",
    "\n",
    "## Input Dependencies\n",
    "\n",
    "From Day 4:\n",
    "- Clean dataset with temporal + rolling features (300K rows, 26 columns)\n",
    "- Store and item metadata merged\n",
    "\n",
    "From raw data:\n",
    "- holidays_events.csv (350 holiday records)\n",
    "- oil.csv (1,218 oil price records)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d556c",
   "metadata": {},
   "source": [
    "## 1. Holiday Effects Analysis\n",
    "\n",
    "**Objective:** Quantify impact of holidays on sales patterns\n",
    "\n",
    "**Activities:**\n",
    "- Merge holidays_events.csv with main dataset\n",
    "- Calculate proximity to holidays (days before/after)\n",
    "- Compare sales on holidays vs normal days\n",
    "- Identify holiday types with strongest impact\n",
    "- Analyze transferred holidays behavior\n",
    "- Create holiday visualization (bar chart by type)\n",
    "\n",
    "**Expected output:** \n",
    "- Holiday proximity features (days_to_holiday, days_after_holiday)\n",
    "- Sales lift metrics by holiday type\n",
    "- Visualization showing holiday vs non-holiday sales\n",
    "- Business recommendations for holiday inventory\n",
    "\n",
    "⚠️ **Note:** Computation time ~4-5 minutes due to date proximity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef05d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "# Configure environment\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"Package Versions:\")\n",
    "print(f\"  pandas: {pd.__version__}\")\n",
    "print(f\"  numpy: {np.__version__}\")\n",
    "print(f\"  matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"  seaborn: {sns.__version__}\")\n",
    "print(f\"  scipy: {scipy.__version__}\")\n",
    "print(\"\\nOK - Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637960ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine paths\n",
    "current_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "project_root = current_dir.parent if current_dir.name == 'notebooks' else current_dir\n",
    "\n",
    "# Define path constants\n",
    "DATA_RAW = project_root / 'data' / 'raw'\n",
    "DATA_PROCESSED = project_root / 'data' / 'processed'\n",
    "OUTPUTS = project_root / 'outputs' / 'figures' / 'eda'\n",
    "\n",
    "# Verify paths\n",
    "assert DATA_RAW.exists(), f\"ERROR - Path not found: {DATA_RAW}\"\n",
    "assert DATA_PROCESSED.exists(), f\"ERROR - Path not found: {DATA_PROCESSED}\"\n",
    "assert OUTPUTS.exists(), f\"ERROR - Path not found: {OUTPUTS}\"\n",
    "\n",
    "print(\"OK - Paths validated:\")\n",
    "print(f\"  Project root: {project_root.resolve()}\")\n",
    "print(f\"  DATA_RAW: {DATA_RAW.resolve()}\")\n",
    "print(f\"  DATA_PROCESSED: {DATA_PROCESSED.resolve()}\")\n",
    "print(f\"  OUTPUTS: {OUTPUTS.resolve()}\")\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"\\nRandom seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Day 4 (need to reapply transformations)\n",
    "print(\"Loading dataset and reapplying Day 3-4 transformations...\")\n",
    "\n",
    "df = pd.read_pickle(DATA_PROCESSED / 'guayas_sample_300k.pkl')\n",
    "\n",
    "# Quick transformations from Days 3-4\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['onpromotion'] = df['onpromotion'].fillna(0.0)\n",
    "\n",
    "# Merge store metadata\n",
    "df_stores = pd.read_csv(DATA_RAW / 'stores.csv')\n",
    "df = df.merge(df_stores[['store_nbr', 'city', 'state', 'type', 'cluster']], \n",
    "              on='store_nbr', how='left')\n",
    "\n",
    "# Create temporal features\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['day_of_month'] = df['date'].dt.day\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "print(f\"OK - Dataset ready\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "print(f\"  Missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff3a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore holidays data\n",
    "print(\"Holiday Impact Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nLoading holidays_events.csv...\")\n",
    "df_holidays = pd.read_csv(DATA_RAW / 'holidays_events.csv')\n",
    "\n",
    "print(f\"OK - Holidays loaded\")\n",
    "print(f\"  Total holiday records: {len(df_holidays)}\")\n",
    "print(f\"  Columns: {list(df_holidays.columns)}\")\n",
    "\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(df_holidays.head(10))\n",
    "\n",
    "print(\"\\nHoliday types:\")\n",
    "print(df_holidays['type'].value_counts())\n",
    "\n",
    "print(\"\\nLocale distribution:\")\n",
    "print(df_holidays['locale'].value_counts())\n",
    "\n",
    "print(\"\\nLocale names (sample):\")\n",
    "print(df_holidays['locale_name'].value_counts().head(10))\n",
    "\n",
    "# Filter to National and Guayas holidays\n",
    "print(\"\\nFiltering to National and Guayas (regional/local) holidays...\")\n",
    "df_holidays_filtered = df_holidays[\n",
    "    (df_holidays['locale'] == 'National') | \n",
    "    (df_holidays['locale_name'] == 'Guayas')\n",
    "].copy()\n",
    "\n",
    "print(f\"  Filtered holidays: {len(df_holidays_filtered)} (from {len(df_holidays)})\")\n",
    "\n",
    "# Convert date to datetime\n",
    "df_holidays_filtered['date'] = pd.to_datetime(df_holidays_filtered['date'])\n",
    "\n",
    "print(f\"\\nDate range: {df_holidays_filtered['date'].min().date()} to {df_holidays_filtered['date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge holidays with sales data (CORRECTED)\n",
    "print(\"\\nMerging holidays with sales data (CORRECTED)...\")\n",
    "\n",
    "# Start fresh - reload to avoid duplicate columns\n",
    "df = pd.read_pickle(DATA_PROCESSED / 'guayas_sample_300k.pkl')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['onpromotion'] = df['onpromotion'].fillna(0.0)\n",
    "\n",
    "# Merge store metadata\n",
    "df_stores = pd.read_csv(DATA_RAW / 'stores.csv')\n",
    "df = df.merge(df_stores[['store_nbr', 'city', 'state', 'type', 'cluster']], \n",
    "              on='store_nbr', how='left')\n",
    "\n",
    "# Create temporal features\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['day_of_month'] = df['date'].dt.day\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Create holiday flag\n",
    "df['is_holiday'] = df['date'].isin(df_holidays_filtered['date']).astype(int)\n",
    "\n",
    "# Merge holiday details (avoid column name conflicts)\n",
    "df_holidays_merge = df_holidays_filtered[['date', 'type', 'description']].rename(\n",
    "    columns={'type': 'holiday_type', 'description': 'holiday_name'}\n",
    ")\n",
    "\n",
    "df = df.merge(df_holidays_merge, on='date', how='left')\n",
    "\n",
    "print(f\"OK - Merge complete\")\n",
    "print(f\"  Dataset shape: {df.shape}\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "\n",
    "# CORRECTED CALCULATION: Compare daily averages\n",
    "print(\"\\nNon-holiday vs Holiday comparison (CORRECTED - Daily Averages):\")\n",
    "non_holiday_sales = df[df['is_holiday'] == 0]['unit_sales'].sum()\n",
    "holiday_sales = df[df['is_holiday'] == 1]['unit_sales'].sum()\n",
    "non_holiday_days = df[df['is_holiday'] == 0]['date'].nunique()\n",
    "holiday_days = df[df['is_holiday'] == 1]['date'].nunique()\n",
    "\n",
    "non_holiday_avg = non_holiday_sales / non_holiday_days\n",
    "holiday_avg = holiday_sales / holiday_days\n",
    "\n",
    "print(f\"  Non-holiday: {non_holiday_sales:,.0f} units / {non_holiday_days} days = {non_holiday_avg:,.1f} units/day\")\n",
    "print(f\"  Holiday: {holiday_sales:,.0f} units / {holiday_days} days = {holiday_avg:,.1f} units/day\")\n",
    "print(f\"  Holiday lift: {((holiday_avg / non_holiday_avg) - 1) * 100:+.1f}%\")\n",
    "\n",
    "print(\"\\nHoliday type distribution:\")\n",
    "print(df[df['is_holiday'] == 1]['holiday_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756fc6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sales by holiday type\n",
    "print(\"\\nSales Analysis by Holiday Type:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate average daily sales by holiday type\n",
    "holiday_type_analysis = df[df['is_holiday'] == 1].groupby('holiday_type').agg({\n",
    "    'unit_sales': 'sum',\n",
    "    'date': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "holiday_type_analysis.columns = ['holiday_type', 'total_sales', 'days']\n",
    "holiday_type_analysis['avg_daily_sales'] = holiday_type_analysis['total_sales'] / holiday_type_analysis['days']\n",
    "\n",
    "# Calculate lift vs non-holiday baseline\n",
    "non_holiday_baseline = df[df['is_holiday'] == 0]['unit_sales'].sum() / df[df['is_holiday'] == 0]['date'].nunique()\n",
    "\n",
    "holiday_type_analysis['lift_vs_baseline'] = ((holiday_type_analysis['avg_daily_sales'] / non_holiday_baseline) - 1) * 100\n",
    "\n",
    "# Sort by lift\n",
    "holiday_type_analysis = holiday_type_analysis.sort_values('lift_vs_baseline', ascending=False)\n",
    "\n",
    "print(\"\\nSales by Holiday Type:\")\n",
    "print(holiday_type_analysis.to_string(index=False))\n",
    "\n",
    "print(f\"\\nBaseline (non-holiday): {non_holiday_baseline:,.1f} units/day\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "for _, row in holiday_type_analysis.iterrows():\n",
    "    if row['lift_vs_baseline'] > 20:\n",
    "        impact = \"STRONG POSITIVE\"\n",
    "    elif row['lift_vs_baseline'] > 0:\n",
    "        impact = \"Moderate positive\"\n",
    "    elif row['lift_vs_baseline'] > -20:\n",
    "        impact = \"Slight negative\"\n",
    "    else:\n",
    "        impact = \"STRONG NEGATIVE\"\n",
    "    print(f\"  {row['holiday_type']:<15} {row['lift_vs_baseline']:>+6.1f}%  ({impact})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze pre/post holiday effects\n",
    "print(\"\\nPre/Post Holiday Effects Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create pre/post holiday flags (±3 days around holidays)\n",
    "holiday_dates = df[df['is_holiday'] == 1]['date'].unique()\n",
    "\n",
    "df['days_to_holiday'] = df['date'].apply(\n",
    "    lambda x: min([abs((x - hol).days) for hol in holiday_dates]) if len(holiday_dates) > 0 else 999\n",
    ")\n",
    "\n",
    "df['is_pre_holiday'] = ((df['days_to_holiday'] >= 1) & (df['days_to_holiday'] <= 3) & (df['is_holiday'] == 0)).astype(int)\n",
    "df['is_post_holiday'] = ((df['days_to_holiday'] >= -3) & (df['days_to_holiday'] <= -1) & (df['is_holiday'] == 0)).astype(int)\n",
    "\n",
    "# Note: This is a simplified approach - we're using absolute distance\n",
    "# For better results, we'd need to track direction (before/after)\n",
    "\n",
    "# Recalculate with proper direction\n",
    "def get_holiday_proximity(date, holiday_dates):\n",
    "    \"\"\"\n",
    "    Calculate distance and period classification for a date relative to holidays.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (distance_in_days, period_label)\n",
    "            - distance: 0 for holiday, positive for pre-holiday, negative for post-holiday, 999 for normal\n",
    "            - period: 'holiday', 'pre' (1-3 days before), 'post' (1-3 days after), or 'normal'\n",
    "    \"\"\"\n",
    "    if date in holiday_dates:\n",
    "        return 0, 'holiday'\n",
    "    \n",
    "    future_holidays = [h for h in holiday_dates if h > date]\n",
    "    past_holidays = [h for h in holiday_dates if h < date]\n",
    "    \n",
    "    days_to_next = min([(h - date).days for h in future_holidays]) if future_holidays else 999\n",
    "    days_from_prev = min([(date - h).days for h in past_holidays]) if past_holidays else 999\n",
    "    \n",
    "    if days_to_next <= 3:\n",
    "        return days_to_next, 'pre'\n",
    "    elif days_from_prev <= 3:\n",
    "        return -days_from_prev, 'post'\n",
    "    else:\n",
    "        return 999, 'normal'\n",
    "\n",
    "print(\"Calculating holiday proximity (this may take 5 minutes)...\")\n",
    "df['holiday_proximity'], df['holiday_period'] = zip(*df['date'].apply(lambda x: get_holiday_proximity(x, holiday_dates)))\n",
    "\n",
    "# Update flags\n",
    "df['is_pre_holiday'] = (df['holiday_period'] == 'pre').astype(int)\n",
    "df['is_post_holiday'] = (df['holiday_period'] == 'post').astype(int)\n",
    "\n",
    "print(\"OK - Proximity calculated\")\n",
    "\n",
    "# Analyze by period\n",
    "period_analysis = df.groupby('holiday_period').agg({\n",
    "    'unit_sales': 'sum',\n",
    "    'date': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "period_analysis.columns = ['period', 'total_sales', 'days']\n",
    "period_analysis['avg_daily_sales'] = period_analysis['total_sales'] / period_analysis['days']\n",
    "period_analysis['lift_vs_normal'] = ((period_analysis['avg_daily_sales'] / \n",
    "                                       period_analysis[period_analysis['period'] == 'normal']['avg_daily_sales'].values[0]) - 1) * 100\n",
    "\n",
    "print(\"\\nSales by Holiday Period:\")\n",
    "period_order = ['pre', 'holiday', 'post', 'normal']\n",
    "period_analysis['period'] = pd.Categorical(period_analysis['period'], categories=period_order, ordered=True)\n",
    "period_analysis = period_analysis.sort_values('period')\n",
    "print(period_analysis.to_string(index=False))\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"  → Pre-holiday period shows shopping preparation behavior\")\n",
    "print(\"  → Holiday day itself may have lower sales (store closures)\")\n",
    "print(\"  → Post-holiday period returns to baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f598898",
   "metadata": {},
   "source": [
    "## 2. Promotion Effectiveness Analysis\n",
    "\n",
    "**Objective:** Measure sales lift from promotions and interactions with holidays\n",
    "\n",
    "**Activities:**\n",
    "- Compare sales: promoted vs non-promoted items\n",
    "- Calculate promotion lift (% increase)\n",
    "- Analyze promotion frequency by family and store\n",
    "- Investigate promotion × holiday interaction\n",
    "- Assess non-promoted baseline trends\n",
    "\n",
    "**Expected output:** \n",
    "- Promotion effectiveness report (sales lift %)\n",
    "- Promotion frequency analysis\n",
    "- Promotion × holiday interaction insights\n",
    "- ROI implications for promotional planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea34a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promotion effectiveness analysis\n",
    "print(\"Promotion Effectiveness Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nPromotion distribution in dataset:\")\n",
    "print(f\"  Total transactions: {len(df):,}\")\n",
    "print(f\"  On promotion: {(df['onpromotion'] == 1).sum():,} ({(df['onpromotion'] == 1).sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"  Not on promotion: {(df['onpromotion'] == 0).sum():,} ({(df['onpromotion'] == 0).sum()/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nPromotion by family:\")\n",
    "promo_by_family = df.groupby('family')['onpromotion'].agg(['sum', 'count'])\n",
    "promo_by_family['promo_rate'] = (promo_by_family['sum'] / promo_by_family['count'] * 100)\n",
    "promo_by_family.columns = ['Promoted Items', 'Total Items', 'Promotion Rate %']\n",
    "print(promo_by_family)\n",
    "\n",
    "print(\"\\nPromotion by store type:\")\n",
    "promo_by_type = df.groupby('type')['onpromotion'].agg(['sum', 'count'])\n",
    "promo_by_type['promo_rate'] = (promo_by_type['sum'] / promo_by_type['count'] * 100)\n",
    "promo_by_type.columns = ['Promoted Items', 'Total Items', 'Promotion Rate %']\n",
    "print(promo_by_type.sort_values('Promotion Rate %', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e408799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate promotion lift\n",
    "print(\"\\nPromotion Sales Lift Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Overall lift\n",
    "promo_sales_avg = df[df['onpromotion'] == 1]['unit_sales'].mean()\n",
    "non_promo_sales_avg = df[df['onpromotion'] == 0]['unit_sales'].mean()\n",
    "overall_lift = ((promo_sales_avg / non_promo_sales_avg) - 1) * 100\n",
    "\n",
    "print(f\"\\nOverall promotion effectiveness:\")\n",
    "print(f\"  Non-promoted avg: {non_promo_sales_avg:.2f} units\")\n",
    "print(f\"  Promoted avg: {promo_sales_avg:.2f} units\")\n",
    "print(f\"  Sales lift: {overall_lift:+.1f}%\")\n",
    "\n",
    "# By family\n",
    "print(\"\\nPromotion lift by family:\")\n",
    "lift_by_family = []\n",
    "for family in df['family'].unique():\n",
    "    family_df = df[df['family'] == family]\n",
    "    promo_avg = family_df[family_df['onpromotion'] == 1]['unit_sales'].mean()\n",
    "    non_promo_avg = family_df[family_df['onpromotion'] == 0]['unit_sales'].mean()\n",
    "    lift = ((promo_avg / non_promo_avg) - 1) * 100\n",
    "    lift_by_family.append({\n",
    "        'family': family,\n",
    "        'non_promo_avg': non_promo_avg,\n",
    "        'promo_avg': promo_avg,\n",
    "        'lift_%': lift,\n",
    "        'promo_count': (family_df['onpromotion'] == 1).sum()\n",
    "    })\n",
    "\n",
    "lift_df = pd.DataFrame(lift_by_family).sort_values('lift_%', ascending=False)\n",
    "print(lift_df.to_string(index=False))\n",
    "\n",
    "# By store type\n",
    "print(\"\\nPromotion lift by store type:\")\n",
    "lift_by_type = []\n",
    "for store_type in df['type'].unique():\n",
    "    type_df = df[df['type'] == store_type]\n",
    "    promo_avg = type_df[type_df['onpromotion'] == 1]['unit_sales'].mean()\n",
    "    non_promo_avg = type_df[type_df['onpromotion'] == 0]['unit_sales'].mean()\n",
    "    lift = ((promo_avg / non_promo_avg) - 1) * 100\n",
    "    lift_by_type.append({\n",
    "        'type': store_type,\n",
    "        'non_promo_avg': non_promo_avg,\n",
    "        'promo_avg': promo_avg,\n",
    "        'lift_%': lift,\n",
    "        'promo_count': (type_df['onpromotion'] == 1).sum()\n",
    "    })\n",
    "\n",
    "lift_type_df = pd.DataFrame(lift_by_type).sort_values('lift_%', ascending=False)\n",
    "print(lift_type_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "if overall_lift > 50:\n",
    "    print(f\"  → Promotions are HIGHLY effective ({overall_lift:+.0f}% lift)\")\n",
    "elif overall_lift > 20:\n",
    "    print(f\"  → Promotions are effective ({overall_lift:+.0f}% lift)\")\n",
    "elif overall_lift > 0:\n",
    "    print(f\"  → Promotions have modest impact ({overall_lift:+.0f}% lift)\")\n",
    "else:\n",
    "    print(f\"  → WARNING: Promotions appear ineffective ({overall_lift:+.0f}% lift)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e63f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promotion × Holiday interaction\n",
    "print(\"\\nPromotion × Holiday Interaction Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create interaction categories\n",
    "df['promo_holiday_category'] = 'Normal (no promo)'\n",
    "df.loc[(df['onpromotion'] == 1) & (df['is_holiday'] == 0), 'promo_holiday_category'] = 'Promoted (no holiday)'\n",
    "df.loc[(df['onpromotion'] == 0) & (df['is_holiday'] == 1), 'promo_holiday_category'] = 'Holiday (no promo)'\n",
    "df.loc[(df['onpromotion'] == 1) & (df['is_holiday'] == 1), 'promo_holiday_category'] = 'Promoted + Holiday'\n",
    "\n",
    "# Analyze each combination\n",
    "interaction_analysis = df.groupby('promo_holiday_category').agg({\n",
    "    'unit_sales': ['sum', 'mean', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "interaction_analysis.columns = ['category', 'total_sales', 'avg_sales', 'count']\n",
    "interaction_analysis = interaction_analysis.sort_values('avg_sales', ascending=False)\n",
    "\n",
    "print(\"\\nSales by Promotion × Holiday Combination:\")\n",
    "print(interaction_analysis.to_string(index=False))\n",
    "\n",
    "# Calculate lifts\n",
    "baseline = interaction_analysis[interaction_analysis['category'] == 'Normal (no promo)']['avg_sales'].values[0]\n",
    "\n",
    "print(f\"\\nBaseline (normal days, no promo): {baseline:.2f} units\")\n",
    "print(\"\\nLifts vs baseline:\")\n",
    "for _, row in interaction_analysis.iterrows():\n",
    "    if row['category'] != 'Normal (no promo)':\n",
    "        lift = ((row['avg_sales'] / baseline) - 1) * 100\n",
    "        print(f\"  {row['category']:<30} {lift:>+6.1f}%\")\n",
    "\n",
    "# Check for synergy (is Promo+Holiday > Promo + Holiday separately?)\n",
    "promo_only_lift = ((interaction_analysis[interaction_analysis['category'] == 'Promoted (no holiday)']['avg_sales'].values[0] / baseline) - 1) * 100\n",
    "holiday_only_lift = ((interaction_analysis[interaction_analysis['category'] == 'Holiday (no promo)']['avg_sales'].values[0] / baseline) - 1) * 100\n",
    "\n",
    "if 'Promoted + Holiday' in interaction_analysis['category'].values:\n",
    "    combined_lift = ((interaction_analysis[interaction_analysis['category'] == 'Promoted + Holiday']['avg_sales'].values[0] / baseline) - 1) * 100\n",
    "    expected_additive = promo_only_lift + holiday_only_lift\n",
    "    synergy = combined_lift - expected_additive\n",
    "    \n",
    "    print(f\"\\nSynergy Analysis:\")\n",
    "    print(f\"  Promotion only: {promo_only_lift:>+6.1f}%\")\n",
    "    print(f\"  Holiday only: {holiday_only_lift:>+6.1f}%\")\n",
    "    print(f\"  Expected (additive): {expected_additive:>+6.1f}%\")\n",
    "    print(f\"  Actual (Promo+Holiday): {combined_lift:>+6.1f}%\")\n",
    "    print(f\"  Synergy effect: {synergy:>+6.1f}%\")\n",
    "    \n",
    "    if synergy > 10:\n",
    "        print(\"  → STRONG positive synergy (promotions + holidays amplify each other)\")\n",
    "    elif synergy > 0:\n",
    "        print(\"  → Slight positive synergy\")\n",
    "    elif synergy > -10:\n",
    "        print(\"  → Slight negative synergy (diminishing returns)\")\n",
    "    else:\n",
    "        print(\"  → STRONG negative synergy (avoid combining promotions with holidays)\")\n",
    "else:\n",
    "    print(\"\\nNote: No 'Promoted + Holiday' combinations in sample (rare event)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c143b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize promotion effectiveness\n",
    "print(\"\\nVisualizing promotion effectiveness...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Promotion lift by family\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(lift_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, lift_df['non_promo_avg'], width, \n",
    "                label='Non-promoted', color='#d62728', alpha=0.7)\n",
    "bars2 = ax1.bar(x + width/2, lift_df['promo_avg'], width, \n",
    "                label='Promoted', color='#2ca02c', alpha=0.7)\n",
    "\n",
    "ax1.set_ylabel('Average Unit Sales', fontsize=11)\n",
    "ax1.set_title('Promotion Effectiveness by Family', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(lift_df['family'], rotation=0)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add lift percentage labels\n",
    "# Before the visualization, rename the column\n",
    "lift_df = lift_df.rename(columns={'lift_%': 'lift_pct'})\n",
    "\n",
    "# Then use the clean name\n",
    "for i, row in enumerate(lift_df.itertuples()):\n",
    "    ax1.text(i, max(row.non_promo_avg, row.promo_avg) + 0.5,\n",
    "             f'+{row.lift_pct:.0f}%', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: Promotion × Holiday interaction\n",
    "ax2 = axes[1]\n",
    "interaction_sorted = interaction_analysis.sort_values('avg_sales')\n",
    "colors_interaction = ['#7f7f7f', '#ff7f0e', '#1f77b4', '#d62728']\n",
    "\n",
    "bars = ax2.barh(range(len(interaction_sorted)), interaction_sorted['avg_sales'],\n",
    "                color=colors_interaction, alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax2.set_yticks(range(len(interaction_sorted)))\n",
    "ax2.set_yticklabels(interaction_sorted['category'])\n",
    "ax2.set_xlabel('Average Unit Sales', fontsize=11)\n",
    "ax2.set_title('Promotion × Holiday Interaction', fontsize=13, fontweight='bold')\n",
    "ax2.axvline(x=baseline, color='black', linestyle='--', alpha=0.5, \n",
    "            label=f'Baseline ({baseline:.2f})')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, row in enumerate(interaction_sorted.itertuples()):\n",
    "    lift = ((row.avg_sales / baseline) - 1) * 100\n",
    "    ax2.text(row.avg_sales, i, f' {row.avg_sales:.2f} ({lift:+.0f}%)',\n",
    "             va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / '12_promotion_effectiveness.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOK - Promotion visualization saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SECTION 2 COMPLETE - Promotion Effectiveness Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"  • Overall promotion lift: +74.0% (HIGHLY effective)\")\n",
    "print(\"  • BEVERAGES: +63% lift (lowest but still strong)\")\n",
    "print(\"  • CLEANING: +72% lift\")\n",
    "print(\"  • GROCERY I: +71% lift\")\n",
    "print(\"  • Type C stores: +101% lift (promotions most effective in underperforming stores)\")\n",
    "print(\"  • Negative synergy: -16.1% when combining promos with holidays\")\n",
    "print(\"  • Recommendation: Run promotions on NORMAL days, not holidays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69eba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perishable analysis\n",
    "print(\"Perishable Deep Dive\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nPerishable distribution in dataset:\")\n",
    "print(df['perishable'].value_counts())\n",
    "perishable_pct = (df['perishable'] == 1).sum() / len(df) * 100\n",
    "print(f\"\\nPerishable transactions: {perishable_pct:.2f}%\")\n",
    "\n",
    "print(\"\\nPerishable percentage by family:\")\n",
    "perishable_by_family = df.groupby('family')['perishable'].agg(['sum', 'count'])\n",
    "perishable_by_family['perishable_%'] = (perishable_by_family['sum'] / perishable_by_family['count'] * 100)\n",
    "perishable_by_family.columns = ['Perishable Count', 'Total', 'Perishable %']\n",
    "print(perishable_by_family.sort_values('Perishable %', ascending=False))\n",
    "\n",
    "# Sales comparison\n",
    "print(\"\\nSales comparison: Perishable vs Non-perishable:\")\n",
    "perishable_sales = df[df['perishable'] == 1]['unit_sales'].sum()\n",
    "non_perishable_sales = df[df['perishable'] == 0]['unit_sales'].sum()\n",
    "perishable_avg = df[df['perishable'] == 1]['unit_sales'].mean()\n",
    "non_perishable_avg = df[df['perishable'] == 0]['unit_sales'].mean()\n",
    "\n",
    "print(f\"  Non-perishable: {non_perishable_sales:,.0f} total, {non_perishable_avg:.2f} avg\")\n",
    "print(f\"  Perishable: {perishable_sales:,.0f} total, {perishable_avg:.2f} avg\")\n",
    "print(f\"  Perishable avg sales: {((perishable_avg / non_perishable_avg) - 1) * 100:+.1f}% vs non-perishable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512cd9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document the perishable limitation\n",
    "print(\"\\nPerishable Analysis Limitation:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nFINDING: Our sample contains 0% perishable items\")\n",
    "print(\"\\nExplanation:\")\n",
    "print(\"  • Top-3 families selected (GROCERY I, BEVERAGES, CLEANING) are non-perishable\")\n",
    "print(\"  • Perishable items concentrated in other families:\")\n",
    "print(\"    - PRODUCE (fresh fruits, vegetables)\")\n",
    "print(\"    - DAIRY (milk, cheese, yogurt)\")\n",
    "print(\"    - MEATS (fresh meat, seafood)\")\n",
    "print(\"    - BREAD/BAKERY (fresh baked goods)\")\n",
    "print(\"    - DELI (prepared foods)\")\n",
    "\n",
    "print(\"\\nImplication for forecasting:\")\n",
    "print(\"  → Our models will focus on non-perishable inventory\")\n",
    "print(\"  → Lower forecasting accuracy requirements (less waste risk)\")\n",
    "print(\"  → Longer shelf life = more forecast horizon flexibility\")\n",
    "print(\"  → Safety stock strategies different from perishables\")\n",
    "\n",
    "print(\"\\nFor complete perishable analysis:\")\n",
    "print(\"  → Would need to include PRODUCE, DAIRY, MEATS in sample\")\n",
    "print(\"  → Noted as limitation in project documentation\")\n",
    "\n",
    "# Check actual items.csv to confirm perishable distribution\n",
    "print(\"\\nVerifying against full items.csv...\")\n",
    "df_items_full = pd.read_csv(DATA_RAW / 'items.csv')\n",
    "print(f\"\\nFull item catalog:\")\n",
    "print(f\"  Total items: {len(df_items_full):,}\")\n",
    "print(f\"  Perishable items: {(df_items_full['perishable'] == 1).sum():,} ({(df_items_full['perishable'] == 1).sum()/len(df_items_full)*100:.1f}%)\")\n",
    "print(f\"  Non-perishable items: {(df_items_full['perishable'] == 0).sum():,} ({(df_items_full['perishable'] == 0).sum()/len(df_items_full)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nPerishable items by family (full catalog):\")\n",
    "perishable_full = df_items_full.groupby('family')['perishable'].agg(['sum', 'count'])\n",
    "perishable_full['perishable_%'] = (perishable_full['sum'] / perishable_full['count'] * 100)\n",
    "perishable_full.columns = ['Perishable Count', 'Total Items', 'Perishable %']\n",
    "perishable_full = perishable_full[perishable_full['Perishable Count'] > 0].sort_values('Perishable %', ascending=False)\n",
    "print(perishable_full.head(10))\n",
    "\n",
    "print(\"\\nOur top-3 families in full catalog:\")\n",
    "our_families = df_items_full[df_items_full['family'].isin(['GROCERY I', 'BEVERAGES', 'CLEANING'])]\n",
    "print(f\"  Total items in top-3: {len(our_families):,}\")\n",
    "print(f\"  Perishable in top-3: {(our_families['perishable'] == 1).sum()} ({(our_families['perishable'] == 1).sum()/len(our_families)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SECTION 3 COMPLETE - Perishable Analysis (Scope Limitation Noted)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nKey Finding:\")\n",
    "print(\"  • 0% perishable items in our sample (by design - top-3 families are non-perishable)\")\n",
    "print(\"  • Project focus: Non-perishable forecasting (longer shelf life, lower waste risk)\")\n",
    "print(\"  • Limitation documented for stakeholder communication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2869d7f4",
   "metadata": {},
   "source": [
    "## 4. External Factors & Zero-Sales Analysis\n",
    "\n",
    "**Objective:** Investigate oil price correlation and zero-sales patterns\n",
    "\n",
    "**Activities:**\n",
    "- Load oil.csv and merge with aggregated daily sales\n",
    "- Plot dual-axis chart (oil price + sales over time)\n",
    "- Calculate correlation coefficient\n",
    "- Analyze zero-sales patterns (true zeros vs stockouts)\n",
    "- Document findings for feature selection\n",
    "\n",
    "**Expected output:** \n",
    "- Oil price vs sales correlation analysis\n",
    "- Zero-sales pattern report\n",
    "- Decision on oil feature inclusion (Week 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e93c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oil price analysis\n",
    "print(\"External Factors Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. Oil Price Correlation:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nLoading oil.csv...\")\n",
    "df_oil = pd.read_csv(DATA_RAW / 'oil.csv')\n",
    "df_oil['date'] = pd.to_datetime(df_oil['date'])\n",
    "\n",
    "print(f\"OK - Oil data loaded\")\n",
    "print(f\"  Records: {len(df_oil):,}\")\n",
    "print(f\"  Date range: {df_oil['date'].min().date()} to {df_oil['date'].max().date()}\")\n",
    "print(f\"  Missing values: {df_oil['dcoilwtico'].isnull().sum()} ({df_oil['dcoilwtico'].isnull().sum()/len(df_oil)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nOil price statistics:\")\n",
    "print(df_oil['dcoilwtico'].describe())\n",
    "\n",
    "# Aggregate daily sales for correlation\n",
    "print(\"\\nAggregating daily sales...\")\n",
    "daily_sales = df.groupby('date')['unit_sales'].sum().reset_index()\n",
    "daily_sales.columns = ['date', 'total_sales']\n",
    "\n",
    "print(f\"  Daily sales records: {len(daily_sales):,}\")\n",
    "\n",
    "# Merge oil with sales\n",
    "print(\"\\nMerging oil prices with daily sales...\")\n",
    "df_oil_sales = daily_sales.merge(df_oil, on='date', how='left')\n",
    "\n",
    "print(f\"  Merged records: {len(df_oil_sales):,}\")\n",
    "print(f\"  Missing oil prices after merge: {df_oil_sales['dcoilwtico'].isnull().sum()}\")\n",
    "\n",
    "# Forward fill missing oil prices\n",
    "df_oil_sales['dcoilwtico'] = df_oil_sales['dcoilwtico'].fillna(method='ffill')\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = df_oil_sales[['total_sales', 'dcoilwtico']].corr().iloc[0, 1]\n",
    "\n",
    "print(f\"\\nCorrelation Analysis:\")\n",
    "print(f\"  Pearson correlation: {correlation:.4f}\")\n",
    "\n",
    "if abs(correlation) > 0.6:\n",
    "    strength = \"Strong\"\n",
    "elif abs(correlation) > 0.3:\n",
    "    strength = \"Moderate\"\n",
    "else:\n",
    "    strength = \"Weak\"\n",
    "\n",
    "print(f\"  Interpretation: {strength} {'positive' if correlation > 0 else 'negative'} correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dbeedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize oil price vs sales\n",
    "print(\"\\nVisualizing oil price vs sales relationship...\")\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Plot sales on left axis\n",
    "color = 'steelblue'\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_ylabel('Total Daily Sales (units)', color=color, fontsize=12)\n",
    "ax1.plot(df_oil_sales['date'], df_oil_sales['total_sales'], \n",
    "         color=color, linewidth=1, alpha=0.6, label='Daily Sales')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot oil price on right axis\n",
    "ax2 = ax1.twinx()\n",
    "color = 'darkred'\n",
    "ax2.set_ylabel('Oil Price (WTI, USD)', color=color, fontsize=12)\n",
    "ax2.plot(df_oil_sales['date'], df_oil_sales['dcoilwtico'], \n",
    "         color=color, linewidth=2, alpha=0.7, label='Oil Price')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Title and legend\n",
    "plt.title(f'Oil Price vs Sales (Correlation: {correlation:.3f})', \n",
    "          fontsize=14, fontweight='bold')\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / '13_oil_price_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOK - Oil price visualization saved\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  • Moderate negative correlation (-0.55)\")\n",
    "print(\"  • When oil prices rise → sales tend to fall\")\n",
    "print(\"  • Ecuador is oil-dependent: high oil prices → inflation → reduced purchasing power\")\n",
    "print(\"  • This correlation is STRONG ENOUGH to include as a predictive feature\")\n",
    "print(\"  • Notable periods:\")\n",
    "print(\"    - 2014-2015: Oil price crash (~$110 → $26) coincides with sales increase\")\n",
    "print(\"    - 2016-2017: Oil price recovery (~$26 → $50) shows sales stabilization\")\n",
    "\n",
    "print(\"\\nDecision for Week 2 (Feature Engineering):\")\n",
    "if abs(correlation) > 0.3:\n",
    "    print(f\"  → INCLUDE oil price as feature (strong/moderate correlation: {correlation:.3f})\")\n",
    "    print(\"  → Create lagged oil price features (7/14/30-day)\")\n",
    "    print(\"  → Consider oil price change (derivative) as feature\")\n",
    "else:\n",
    "    print(f\"  → EXCLUDE oil price as feature (weak correlation: {correlation:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a468ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-sales analysis\n",
    "print(\"\\n2. Zero-Sales Pattern Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nAnalyzing zero-sales transactions...\")\n",
    "zero_sales = df[df['unit_sales'] == 0]\n",
    "non_zero_sales = df[df['unit_sales'] > 0]\n",
    "\n",
    "print(f\"  Total transactions: {len(df):,}\")\n",
    "print(f\"  Zero sales: {len(zero_sales):,} ({len(zero_sales)/len(df)*100:.2f}%)\")\n",
    "print(f\"  Non-zero sales: {len(non_zero_sales):,} ({len(non_zero_sales)/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nZero-sales by family:\")\n",
    "zero_by_family = df.groupby('family')['unit_sales'].apply(lambda x: (x == 0).sum()).sort_values(ascending=False)\n",
    "total_by_family = df.groupby('family').size()\n",
    "zero_pct_family = (zero_by_family / total_by_family * 100).round(2)\n",
    "\n",
    "for family in zero_by_family.index:\n",
    "    print(f\"  {family:<15} {zero_by_family[family]:>6,} ({zero_pct_family[family]:>5.2f}%)\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  • Sparse data contains implicit zeros (store-item-date combinations not in dataset)\")\n",
    "print(\"  • Explicit zeros (0.30%): Likely data recording or returns processing\")\n",
    "print(\"  • Our 99.1% sparsity (from Day 3) represents true zero demand days\")\n",
    "print(\"  • Zero-sales distinction:\")\n",
    "print(\"    - Explicit zeros: In dataset with unit_sales=0\")\n",
    "print(\"    - Implicit zeros: Missing from dataset (store-item didn't sell that day)\")\n",
    "\n",
    "print(\"\\nImplication for forecasting:\")\n",
    "print(\"  → Models must handle sparse time series\")\n",
    "print(\"  → Zero-inflation models may be appropriate\")\n",
    "print(\"  → Feature: 'days_since_last_sale' could be valuable\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SECTION 4 COMPLETE - External Factors & Zero-Sales\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"  • Oil price: -0.55 correlation (INCLUDE as feature)\")\n",
    "print(\"  • Zero-sales: 0.30% explicit, 99.1% sparsity (implicit zeros)\")\n",
    "print(\"  • Recommendation: Use sparse time series models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8389125",
   "metadata": {},
   "source": [
    "## 5. Export & Documentation\n",
    "\n",
    "**Objective:** Save final analysis-ready dataset and create Week 1 summary\n",
    "\n",
    "**Activities:**\n",
    "- Export final dataset to CSV and pickle (guayas_prepared.csv/.pkl)\n",
    "- Create feature dictionary documenting all columns\n",
    "- Update decision log with Week 1 decisions\n",
    "- Generate Week 1 summary report\n",
    "- Final notebook completion summary\n",
    "\n",
    "**Expected output:** \n",
    "- guayas_prepared.csv (analysis-ready dataset)\n",
    "- guayas_prepared.pkl (fast-loading format)\n",
    "- Feature dictionary\n",
    "- Week 1 summary report\n",
    "- Decision log updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export final dataset\n",
    "print(\"Final Dataset Export\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nPreparing final dataset for export...\")\n",
    "\n",
    "# Final dataset characteristics\n",
    "print(f\"\\nFinal dataset summary:\")\n",
    "print(f\"  Rows: {len(df):,}\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"  Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nColumn list:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i:>2}. {col}\")\n",
    "\n",
    "# Export to CSV\n",
    "csv_path = DATA_PROCESSED / 'guayas_prepared.csv'\n",
    "print(f\"\\nExporting to CSV: {csv_path.name}\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "csv_size = csv_path.stat().st_size / 1024**2\n",
    "print(f\"  ✓ CSV saved ({csv_size:.1f} MB)\")\n",
    "\n",
    "# Export to pickle\n",
    "pkl_path = DATA_PROCESSED / 'guayas_prepared.pkl'\n",
    "print(f\"\\nExporting to pickle: {pkl_path.name}\")\n",
    "df.to_pickle(pkl_path)\n",
    "pkl_size = pkl_path.stat().st_size / 1024**2\n",
    "print(f\"  ✓ Pickle saved ({pkl_size:.1f} MB)\")\n",
    "\n",
    "print(\"\\nOK - Final dataset exported\")\n",
    "print(f\"  CSV: {csv_path.resolve()}\")\n",
    "print(f\"  Pickle: {pkl_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature dictionary\n",
    "print(\"\\nCreating Feature Dictionary...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "feature_dict = {\n",
    "    # Original features\n",
    "    'id': 'Unique transaction ID from Kaggle dataset',\n",
    "    'date': 'Transaction date (YYYY-MM-DD)',\n",
    "    'store_nbr': 'Store number (24-51 in Guayas region)',\n",
    "    'item_nbr': 'Item/product number (unique SKU identifier)',\n",
    "    'unit_sales': 'Number of units sold (target variable)',\n",
    "    'onpromotion': 'Promotion flag (1=promoted, 0=not promoted)',\n",
    "    'family': 'Product family (GROCERY I, BEVERAGES, CLEANING)',\n",
    "    'class': 'Product class (subcategory within family)',\n",
    "    'perishable': 'Perishable flag (0=non-perishable, 1=perishable) - All 0 in sample',\n",
    "    \n",
    "    # Store metadata\n",
    "    'city': 'Store city (Guayaquil, Daule, Libertad)',\n",
    "    'state': 'Store state (Guayas)',\n",
    "    'type': 'Store type (A=premium, B=good, C=medium, D=basic, E=entry)',\n",
    "    'cluster': 'Store cluster (1-17, grouping similar stores)',\n",
    "    \n",
    "    # Temporal features (Day 3)\n",
    "    'year': 'Year (2013-2017)',\n",
    "    'month': 'Month (1-12)',\n",
    "    'day': 'Day of month (1-31)',\n",
    "    'day_of_week': 'Day of week (0=Monday, 6=Sunday)',\n",
    "    'day_of_month': 'Day within month (1-31, duplicate of \"day\")',\n",
    "    'is_weekend': 'Weekend flag (1=Sat/Sun, 0=weekday)',\n",
    "    \n",
    "    # Holiday features (Day 5)\n",
    "    'is_holiday': 'Holiday flag (1=national holiday, 0=normal day)',\n",
    "    'holiday_type': 'Holiday type (Holiday, Event, Additional, Transfer, Work Day, Bridge)',\n",
    "    'holiday_name': 'Holiday description (e.g., Christmas, Independence Day)',\n",
    "    'days_to_holiday': 'Days to nearest holiday (absolute distance)',\n",
    "    'is_pre_holiday': 'Pre-holiday flag (1-3 days before holiday)',\n",
    "    'is_post_holiday': 'Post-holiday flag (1-3 days after holiday)',\n",
    "    'holiday_proximity': 'Signed days to holiday (negative=after, positive=before)',\n",
    "    'holiday_period': 'Holiday period label (pre, holiday, post, normal)',\n",
    "    'promo_holiday_category': 'Promotion × Holiday interaction category'\n",
    "}\n",
    "\n",
    "print(\"\\nFeature Dictionary (28 features):\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "for i, (feature, description) in enumerate(feature_dict.items(), 1):\n",
    "    print(f\"{i:>2}. {feature:<25} {description}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Feature Categories:\")\n",
    "print(\"  • Original (9): id, date, store_nbr, item_nbr, unit_sales, onpromotion, family, class, perishable\")\n",
    "print(\"  • Store metadata (4): city, state, type, cluster\")\n",
    "print(\"  • Temporal (6): year, month, day, day_of_week, day_of_month, is_weekend\")\n",
    "print(\"  • Holiday (9): is_holiday, holiday_type, holiday_name, days_to_holiday, is_pre_holiday,\")\n",
    "print(\"                 is_post_holiday, holiday_proximity, holiday_period, promo_holiday_category\")\n",
    "\n",
    "print(\"\\nFeatures NOT YET created (Week 2):\")\n",
    "print(\"  • Rolling statistics (7/14/30-day moving averages)\")\n",
    "print(\"  • Lag features (1/7/14/30-day lags)\")\n",
    "print(\"  • Oil price features (daily price, 7/14/30-day lags)\")\n",
    "print(\"  • Store/item aggregations (store avg, item avg)\")\n",
    "print(\"  • Promotion history (promotion frequency, days since last promo)\")\n",
    "\n",
    "# Save feature dictionary\n",
    "dict_path = project_root / 'docs' / 'feature_dictionary.txt'\n",
    "with open(dict_path, 'w') as f:\n",
    "    f.write(\"Feature Dictionary - Guayas Prepared Dataset\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "    for feature, description in feature_dict.items():\n",
    "        f.write(f\"{feature:<25} {description}\\n\")\n",
    "\n",
    "print(f\"\\n✓ Feature dictionary saved: {dict_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb68da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 1 completion summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DAY 5 COMPLETE - Context Analysis & Export\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nDay 5 Summary:\")\n",
    "print(\"  ✓ Holiday impact analysis (+24.2% overall, +49.6% Additional days)\")\n",
    "print(\"  ✓ Promotion effectiveness (+74% lift, Type C stores +101%)\")\n",
    "print(\"  ✓ Promotion × Holiday synergy (-16.1% negative, avoid combining)\")\n",
    "print(\"  ✓ Perishable analysis (0% in sample, scope limitation documented)\")\n",
    "print(\"  ✓ Oil price correlation (-0.55 moderate, INCLUDE as feature)\")\n",
    "print(\"  ✓ Zero-sales patterns (99.1% sparsity, implicit zeros)\")\n",
    "print(\"  ✓ Final dataset exported (guayas_prepared.csv, 300K rows, 28 cols)\")\n",
    "print(\"  ✓ Feature dictionary created\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"WEEK 1 COMPLETE - Exploration & Understanding\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n WEEK 1 ACCOMPLISHMENTS:\")\n",
    "print(\"\\n1. Data Scope & Quality:\")\n",
    "print(\"   • 11 Guayas stores analyzed (73.8% in Guayaquil)\")\n",
    "print(\"   • Top-3 families: GROCERY I, BEVERAGES, CLEANING\")\n",
    "print(\"   • 300K sample (representative, manageable)\")\n",
    "print(\"   • 0% missing values after cleaning\")\n",
    "print(\"   • 0.28% high-confidence outliers (retained)\")\n",
    "print(\"   • 99.1% sparsity (retail reality documented)\")\n",
    "\n",
    "print(\"\\n2. Store Performance Insights:\")\n",
    "print(\"   • 4.25x performance gap (Store #51: 356K vs Store #32: 84K)\")\n",
    "print(\"   • Type A stores: 2x higher avg sales vs Type C\")\n",
    "print(\"   • 49% universal items (sold in all stores)\")\n",
    "print(\"   • Item coverage: 64.7% to 89.9% range\")\n",
    "\n",
    "print(\"\\n3. Temporal Patterns:\")\n",
    "print(\"   • Weekend lift: +33.9% (BEVERAGES +40.2%)\")\n",
    "print(\"   • Payday effect: +10.7% (Day 1 peak at +21.9%)\")\n",
    "print(\"   • December seasonality: +30.4%\")\n",
    "print(\"   • Thursday lowest day (78-84% of average)\")\n",
    "print(\"   • Strong autocorrelation: 0.32-0.63 at lags 1-90\")\n",
    "\n",
    "print(\"\\n4. Product Dynamics:\")\n",
    "print(\"   • Pareto: 34% items = 80% sales\")\n",
    "print(\"   • Fast movers (20%): 58.4% of sales\")\n",
    "print(\"   • Slow movers (20%): 2.2% of sales\")\n",
    "print(\"   • Top item velocity: 70.21 units/day\")\n",
    "\n",
    "print(\"\\n5. Context Factors:\")\n",
    "print(\"   • Holiday lift: +24.2% overall\")\n",
    "print(\"   • Additional days: +49.6% (highest)\")\n",
    "print(\"   • Event days: +24.7%\")\n",
    "print(\"   • Regular holidays: -0.4% (closures)\")\n",
    "print(\"   • Promotion lift: +74% (HIGHLY effective)\")\n",
    "print(\"   • Type C stores: +101% promo lift\")\n",
    "print(\"   • Promo × Holiday: -16.1% synergy (avoid)\")\n",
    "print(\"   • Oil correlation: -0.55 (moderate negative)\")\n",
    "\n",
    "print(\"\\n6. Decisions Logged (7 total):\")\n",
    "print(\"   • DEC-001: Top-3 families by item count\")\n",
    "print(\"   • DEC-002: 300K sample for development speed\")\n",
    "print(\"   • DEC-003: Fill onpromotion NaN with False\")\n",
    "print(\"   • DEC-004: 3-method outlier detection (retain)\")\n",
    "print(\"   • DEC-005: Keep sparse format (no gap filling)\")\n",
    "print(\"   • DEC-006: Rolling stats with min_periods=1\")\n",
    "print(\"   • DEC-007: Fast/slow velocity classification\")\n",
    "\n",
    "print(\"\\n7. Deliverables:\")\n",
    "print(\"   • 5 notebooks: d01-d05 (setup, sampling, quality, temporal, context)\")\n",
    "print(\"   • 13 visualizations: store perf, outliers, time series, holidays, promos\")\n",
    "print(\"   • Final dataset: guayas_prepared.csv (300K × 28)\")\n",
    "print(\"   • Feature dictionary: 28 features documented\")\n",
    "print(\"   • 2 checkpoints: Day 3, Day 4\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"WEEK 2 PREVIEW - Feature Development\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nWeek 2 Priorities:\")\n",
    "print(\"  1. Lag features (1/7/14/30-day)\")\n",
    "print(\"  2. Rolling statistics (already started: 7/14/30-day)\")\n",
    "print(\"  3. Oil price features (daily, 7/14/30-day lags)\")\n",
    "print(\"  4. Store/item aggregations (historical averages)\")\n",
    "print(\"  5. Promotion history (frequency, days since last)\")\n",
    "print(\"  6. Holiday proximity features (optimize window)\")\n",
    "print(\"  7. Feature importance analysis (Week 3)\")\n",
    "\n",
    "print(\"\\nKey Recommendations:\")\n",
    "print(\"  → Run promotions on NORMAL days (not holidays)\")\n",
    "print(\"  → Elevate weekend inventory 30-40%\")\n",
    "print(\"  → Focus forecasting on fast movers (34% items)\")\n",
    "print(\"  → Include oil price as macro indicator\")\n",
    "print(\"  → Use sparse time series models\")\n",
    "print(\"  → Type C stores need promotional support\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TIME SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "time_summary = {\n",
    "    'Day 1': {'allocated': 4.0, 'actual': 3.0, 'status': '✓'},\n",
    "    'Day 2': {'allocated': 4.0, 'actual': 1.5, 'status': '✓'},\n",
    "    'Day 3': {'allocated': 5.5, 'actual': 4.5, 'status': '✓'},\n",
    "    'Day 4': {'allocated': 5.0, 'actual': 3.5, 'status': '✓'},\n",
    "    'Day 5': {'allocated': 5.0, 'actual': 2.5, 'status': '✓'}\n",
    "}\n",
    "\n",
    "print(\"\\nWeek 1 Time Tracking:\")\n",
    "total_allocated = sum(d['allocated'] for d in time_summary.values())\n",
    "total_actual = sum(d['actual'] for d in time_summary.values())\n",
    "buffer = total_allocated - total_actual\n",
    "\n",
    "for day, times in time_summary.items():\n",
    "    variance = times['allocated'] - times['actual']\n",
    "    print(f\"  {day}: {times['actual']:.1f}h / {times['allocated']:.1f}h ({variance:+.1f}h) {times['status']}\")\n",
    "\n",
    "print(f\"\\n  Total: {total_actual:.1f}h / {total_allocated:.1f}h\")\n",
    "print(f\"  Buffer remaining: {buffer:.1f}h\")\n",
    "print(f\"  Efficiency: {(total_actual/total_allocated)*100:.0f}%\")\n",
    "\n",
    "print(\"\\n WEEK 1 COMPLETE - EXCELLENT PROGRESS!\")\n",
    "print(f\"\\nCompleted {buffer:.1f} hours ahead of schedule!\")\n",
    "print(\"Ready for Week 2: Feature Development\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
