{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86f7d696",
   "metadata": {},
   "source": [
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "**w01_d01_SETUP_data_inventory.ipynb**\n",
    "\n",
    "**Author:** Alberto Diaz Durana  \n",
    "**Date:** November 2025  \n",
    "**Purpose:** Project setup, environment configuration, and comprehensive data inventory\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook accomplishes the following:\n",
    "\n",
    "- Configure Python virtual environment and install base packages\n",
    "- Download Kaggle dataset files (8 CSVs)\n",
    "- Create project directory structure\n",
    "- Generate comprehensive data inventory (schema, size, coverage)\n",
    "- Document data quality baseline (missing values, duplicates)\n",
    "- Establish data filtering criteria (Guayas region, top-3 families)\n",
    "\n",
    "---\n",
    "\n",
    "## Business Context\n",
    "\n",
    "**Why proper setup matters:**\n",
    "\n",
    "A solid foundation ensures:\n",
    "- Reproducible environment across sessions\n",
    "- Complete understanding of available data\n",
    "- Clear scope definition (300K sample from 125M rows)\n",
    "- Quality baseline for downstream analysis\n",
    "- Efficient storage and access patterns\n",
    "\n",
    "**Key decisions made:**\n",
    "- Geographic scope: Guayas region only (11 stores)\n",
    "- Product scope: Top-3 families (GROCERY I, BEVERAGES, CLEANING)\n",
    "- Sample size: 300K rows (manageable for 4-week timeline)\n",
    "- Date range: 2013-2017 (full training period available)\n",
    "\n",
    "**Deliverables:**\n",
    "- Virtual environment with pinned packages\n",
    "- Project directory structure\n",
    "- Data inventory report (8 files documented)\n",
    "- Filtering criteria defined\n",
    "- README and setup documentation\n",
    "\n",
    "---\n",
    "\n",
    "## Input Dependencies\n",
    "\n",
    "External:\n",
    "- Kaggle API credentials configured\n",
    "- Internet connection for dataset download\n",
    "- Python 3.11+ installed\n",
    "\n",
    "Dataset:\n",
    "- Kaggle competition: \"favorita-grocery-sales-forecasting\"\n",
    "- 8 CSV files (train.csv ~479 MB, others <50 MB each)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4de99",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment Configuration\n",
    "\n",
    "**Objective:** Import required libraries, configure paths, validate environment\n",
    "\n",
    "**Activities:**\n",
    "- Import pandas, numpy, dask for data manipulation\n",
    "- Define path constants for data/raw/ and docs/\n",
    "- Test imports and display versions\n",
    "- Configure warnings and display settings\n",
    "\n",
    "**Expected output:** Confirmation that environment is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d15cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Package versions\n",
    "print(\"Package Versions:\")\n",
    "print(f\"  pandas: {pd.__version__}\")\n",
    "print(f\"  numpy: {np.__version__}\")\n",
    "print(f\"  dask: {dask.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e365ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"OK - Display settings configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f19275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine current directory (works in both scripts and notebooks)\n",
    "current_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "project_root = current_dir.parent if current_dir.name == 'notebooks' else current_dir\n",
    "print(f\"Project root: {project_root.resolve()}\")\n",
    "\n",
    "# Define path constants relative to project root\n",
    "DATA_RAW = project_root / 'data' / 'raw'\n",
    "DOCS = project_root / 'docs'\n",
    "\n",
    "# Verify paths exist\n",
    "assert DATA_RAW.exists(), f\"ERROR - Path not found: {DATA_RAW}\"\n",
    "assert DOCS.exists(), f\"ERROR - Path not found: {DOCS}\"\n",
    "\n",
    "print(\"OK - Paths validated:\")\n",
    "print(f\"  DATA_RAW: {DATA_RAW.resolve()}\")\n",
    "print(f\"  DOCS: {DOCS.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70d7ef9",
   "metadata": {},
   "source": [
    "## 2. Load Support Files (Small CSVs)\n",
    "\n",
    "**Objective:** Load and validate all small CSV files (stores, items, oil, holidays, transactions)\n",
    "\n",
    "**Activities:**\n",
    "- Load 5 support CSV files into pandas DataFrames\n",
    "- Display shape, columns, and data types for each\n",
    "- Check for missing values\n",
    "- Convert date columns to datetime format\n",
    "- Display first few rows for validation\n",
    "\n",
    "**Expected output:** \n",
    "- 5 DataFrames loaded successfully\n",
    "- Schema validation report\n",
    "- Missing value counts per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e46b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_RAW / 'stores.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702dff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stores.csv\n",
    "df_stores = pd.read_csv(DATA_RAW / 'stores.csv')\n",
    "\n",
    "print(\"stores.csv loaded:\")\n",
    "print(f\"  Shape: {df_stores.shape}\")\n",
    "print(f\"  Columns: {list(df_stores.columns)}\")\n",
    "print(f\"  Missing values: {df_stores.isnull().sum().sum()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_stores.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load items.csv\n",
    "df_items = pd.read_csv(DATA_RAW / 'items.csv')\n",
    "\n",
    "print(\"items.csv loaded:\")\n",
    "print(f\"  Shape: {df_items.shape}\")\n",
    "print(f\"  Columns: {list(df_items.columns)}\")\n",
    "print(f\"  Missing values: {df_items.isnull().sum().sum()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_items.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db037bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load oil.csv\n",
    "df_oil = pd.read_csv(DATA_RAW / 'oil.csv')\n",
    "\n",
    "print(\"oil.csv loaded:\")\n",
    "print(f\"  Shape: {df_oil.shape}\")\n",
    "print(f\"  Columns: {list(df_oil.columns)}\")\n",
    "print(f\"  Missing values: {df_oil.isnull().sum().sum()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_oil.dtypes)\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_oil.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdbce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load holidays_events.csv\n",
    "df_holidays = pd.read_csv(DATA_RAW / 'holidays_events.csv')\n",
    "\n",
    "print(\"holidays_events.csv loaded:\")\n",
    "print(f\"  Shape: {df_holidays.shape}\")\n",
    "print(f\"  Columns: {list(df_holidays.columns)}\")\n",
    "print(f\"  Missing values: {df_holidays.isnull().sum().sum()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_holidays.dtypes)\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_holidays.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ebd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transactions.csv\n",
    "df_transactions = pd.read_csv(DATA_RAW / 'transactions.csv')\n",
    "\n",
    "print(\"transactions.csv loaded:\")\n",
    "print(f\"  Shape: {df_transactions.shape}\")\n",
    "print(f\"  Columns: {list(df_transactions.columns)}\")\n",
    "print(f\"  Missing values: {df_transactions.isnull().sum().sum()}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_transactions.dtypes)\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(df_transactions.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0a7016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dictionary\n",
    "support_files_summary = {\n",
    "    'File': ['stores.csv', 'items.csv', 'oil.csv', 'holidays_events.csv', 'transactions.csv'],\n",
    "    'Rows': [len(df_stores), len(df_items), len(df_oil), len(df_holidays), len(df_transactions)],\n",
    "    'Columns': [df_stores.shape[1], df_items.shape[1], df_oil.shape[1], df_holidays.shape[1], df_transactions.shape[1]],\n",
    "    'Missing': [0, 0, 43, 0, 0]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(support_files_summary)\n",
    "print(\"Support Files Summary:\")\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"\\nOK - All 5 support files loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb81d4",
   "metadata": {},
   "source": [
    "## 3. Inspect train.csv with Dask\n",
    "\n",
    "**Objective:** Load large train.csv file using Dask and inspect structure without loading full dataset into memory\n",
    "\n",
    "**Activities:**\n",
    "- Use Dask to read train.csv (479 MB file)\n",
    "- Display schema and estimated row count\n",
    "- Check for missing values\n",
    "- Sample first rows for validation\n",
    "- Document file characteristics\n",
    "\n",
    "**Expected output:** \n",
    "- Train data structure confirmed\n",
    "- Missing value percentages calculated\n",
    "- Memory-efficient inspection complete\n",
    "\n",
    "**Note:** train.csv is too large for pandas (125M rows). Dask enables lazy evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc743af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train.csv with Dask (lazy evaluation)\n",
    "print(\"Loading train.csv with Dask (this may take a moment)...\")\n",
    "df_train = dd.read_csv(DATA_RAW / 'train.csv')\n",
    "\n",
    "print(\"OK - train.csv loaded (Dask DataFrame)\")\n",
    "print(f\"\\nColumns: {list(df_train.columns)}\")\n",
    "print(f\"Data types:\")\n",
    "print(df_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dee56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute actual row count (triggers computation)\n",
    "print(\"Computing row count (this will take time - processing 125M rows)...\")\n",
    "train_length = len(df_train)\n",
    "print(f\"OK - Total rows: {train_length:,}\")\n",
    "\n",
    "# Check missing values per column\n",
    "print(\"\\nComputing missing values per column...\")\n",
    "missing_counts = df_train.isnull().sum().compute()\n",
    "missing_pct = (missing_counts / train_length * 100).round(2)\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "for col in df_train.columns:\n",
    "    count = missing_counts[col]\n",
    "    pct = missing_pct[col]\n",
    "    print(f\"  {col:<15} {count:>12,} ({pct:>6}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b61e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample first 1000 rows to inspect data\n",
    "print(\"Sampling first 1000 rows...\")\n",
    "df_train_sample = df_train.head(1000, npartitions=-1)\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_train_sample.head())\n",
    "\n",
    "print(\"\\nBasic statistics for unit_sales:\")\n",
    "print(df_train_sample['unit_sales'].describe())\n",
    "\n",
    "# Check for negative values\n",
    "negative_count = (df_train_sample['unit_sales'] < 0).sum()\n",
    "print(f\"\\nNegative unit_sales in sample: {negative_count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb391ec",
   "metadata": {},
   "source": [
    "## 4. Identify Guayas Stores\n",
    "\n",
    "**Objective:** Filter stores to Guayas region for project scope\n",
    "\n",
    "**Activities:**\n",
    "- Query stores.csv WHERE state = 'Guayas'\n",
    "- Count Guayas stores\n",
    "- Display store types and clusters in Guayas\n",
    "- Export Guayas store_nbr list for train filtering\n",
    "\n",
    "**Expected output:** \n",
    "- List of Guayas store identifiers\n",
    "- Guayas store characteristics (types, clusters, cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a34975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter stores to Guayas region\n",
    "guayas_stores = df_stores[df_stores['state'] == 'Guayas'].copy()\n",
    "\n",
    "print(f\"Total stores in dataset: {len(df_stores)}\")\n",
    "print(f\"Stores in Guayas: {len(guayas_stores)}\")\n",
    "print(f\"Percentage: {len(guayas_stores)/len(df_stores)*100:.1f}%\")\n",
    "\n",
    "print(\"\\nGuayas stores:\")\n",
    "print(guayas_stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze store types in Guayas\n",
    "print(\"Store types in Guayas:\")\n",
    "print(guayas_stores['type'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nStore clusters in Guayas:\")\n",
    "print(guayas_stores['cluster'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nCities in Guayas:\")\n",
    "print(guayas_stores['city'].value_counts())\n",
    "\n",
    "# Extract store_nbr list for filtering\n",
    "guayas_store_nbrs = guayas_stores['store_nbr'].tolist()\n",
    "print(f\"\\nGuayas store_nbr list ({len(guayas_store_nbrs)} stores):\")\n",
    "print(guayas_store_nbrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45917d0d",
   "metadata": {},
   "source": [
    "## 5. Identify Top-3 Product Families\n",
    "\n",
    "**Objective:** Determine top-3 product families by item count for scope reduction\n",
    "\n",
    "**Activities:**\n",
    "- Count unique items per product family\n",
    "- Rank families by item count\n",
    "- Select top-3 families\n",
    "- Display family characteristics\n",
    "\n",
    "**Expected output:** \n",
    "- Top-3 families list with item counts\n",
    "- Percentage of total items covered\n",
    "- Family names for train filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96334e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count items per family\n",
    "items_per_family = df_items['family'].value_counts().reset_index()\n",
    "items_per_family.columns = ['family', 'item_count']\n",
    "items_per_family = items_per_family.sort_values('item_count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f\"Total product families: {len(items_per_family)}\")\n",
    "print(f\"Total items: {len(df_items)}\")\n",
    "\n",
    "print(\"\\nTop-10 families by item count:\")\n",
    "print(items_per_family.head(10).to_string(index=False))\n",
    "\n",
    "# Select top-3\n",
    "top_3_families = items_per_family.head(3)\n",
    "top_3_family_names = top_3_families['family'].tolist()\n",
    "\n",
    "print(f\"\\nTop-3 families selected:\")\n",
    "print(top_3_families.to_string(index=False))\n",
    "\n",
    "print(f\"\\nTop-3 families cover {top_3_families['item_count'].sum():,} items ({top_3_families['item_count'].sum()/len(df_items)*100:.1f}% of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183cc0d",
   "metadata": {},
   "source": [
    "## 6. Summary & Export Findings\n",
    "\n",
    "**Objective:** Consolidate inventory findings and export for documentation\n",
    "\n",
    "**Activities:**\n",
    "- Create comprehensive inventory summary\n",
    "- Document Guayas scope (11 stores)\n",
    "- Document top-3 families scope (2,393 items)\n",
    "- Export summary to CSV for data_inventory.md update\n",
    "- Calculate expected filtering impact on train.csv\n",
    "\n",
    "**Expected output:** \n",
    "- Complete inventory summary dictionary\n",
    "- Summary CSV exported to docs/\n",
    "- Filtering estimates documented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inventory summary with the assistance of AI\n",
    "inventory_summary = {\n",
    "    # File characteristics\n",
    "    'stores_total': len(df_stores),\n",
    "    'items_total': len(df_items),\n",
    "    'families_total': df_items['family'].nunique(),\n",
    "    'oil_records': len(df_oil),\n",
    "    'oil_missing': 43,\n",
    "    'holidays_records': len(df_holidays),\n",
    "    'transactions_records': len(df_transactions),\n",
    "    'train_rows': train_length,\n",
    "    'train_columns': len(df_train.columns),\n",
    "    \n",
    "    # Guayas scope\n",
    "    'guayas_stores': len(guayas_stores),\n",
    "    'guayas_store_list': str(guayas_store_nbrs),\n",
    "    'guayas_pct_of_stores': f\"{len(guayas_stores)/len(df_stores)*100:.1f}%\",\n",
    "    \n",
    "    # Top-3 families scope\n",
    "    'top_3_families': str(top_3_family_names),\n",
    "    'top_3_items': top_3_families['item_count'].sum(),\n",
    "    'top_3_pct_of_items': f\"{top_3_families['item_count'].sum()/len(df_items)*100:.1f}%\",\n",
    "    \n",
    "    # Data quality\n",
    "    'onpromotion_missing_count': int(missing_counts['onpromotion']),\n",
    "    'onpromotion_missing_pct': f\"{missing_pct['onpromotion']:.2f}%\",\n",
    "}\n",
    "\n",
    "print(\"Inventory Summary:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in inventory_summary.items():\n",
    "    print(f\"{key:<30} {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08125b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary to CSV for documentation\n",
    "df_summary_export = pd.DataFrame([inventory_summary])\n",
    "output_path = DOCS / 'inventory_summary.csv'\n",
    "df_summary_export.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"OK - Summary exported to: {output_path.resolve()}\")\n",
    "\n",
    "# Also create a more readable text summary\n",
    "summary_text = f\"\"\"\n",
    "DATA INVENTORY SUMMARY\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\n",
    "\n",
    "FILE CHARACTERISTICS:\n",
    "- stores.csv: {len(df_stores)} stores\n",
    "- items.csv: {len(df_items)} items across {df_items['family'].nunique()} families\n",
    "- oil.csv: {len(df_oil)} records ({43} missing values)\n",
    "- holidays_events.csv: {len(df_holidays)} holiday records\n",
    "- transactions.csv: {len(df_transactions):,} transaction records\n",
    "- train.csv: {train_length:,} rows, 6 columns\n",
    "\n",
    "PROJECT SCOPE (Guayas Region):\n",
    "- Stores selected: {len(guayas_stores)} of {len(df_stores)} ({len(guayas_stores)/len(df_stores)*100:.1f}%)\n",
    "- Store IDs: {guayas_store_nbrs}\n",
    "- Top-3 families: {top_3_family_names}\n",
    "- Items covered: {top_3_families['item_count'].sum():,} of {len(df_items)} ({top_3_families['item_count'].sum()/len(df_items)*100:.1f}%)\n",
    "\n",
    "DATA QUALITY NOTES:\n",
    "- onpromotion missing: {int(missing_counts['onpromotion']):,} rows ({missing_pct['onpromotion']:.2f}%)\n",
    "- Decision: Fill with False (assume no promotion)\n",
    "- train.csv size: 479 MB (requires Dask)\n",
    "- Negative unit_sales: To be investigated in full dataset\n",
    "\n",
    "NEXT STEPS (Day 2):\n",
    "1. Filter train.csv to Guayas stores only\n",
    "2. Filter to top-3 families only\n",
    "3. Random sample 300K rows for development\n",
    "4. Begin EDA on filtered dataset\n",
    "\"\"\"\n",
    "\n",
    "print(summary_text)\n",
    "\n",
    "# Save text summary\n",
    "with open(DOCS / 'inventory_summary.txt', 'w') as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(f\"\\nOK - Text summary also saved to: {(DOCS / 'inventory_summary.txt').resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c66421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook completion summary\n",
    "print(\"=\" * 70)\n",
    "print(\"NOTEBOOK COMPLETE: d01_w01_SETUP_data_inventory.ipynb\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nACCOMPLISHMENTS:\")\n",
    "print(\"✓ All 5 support files loaded and validated\")\n",
    "print(\"✓ train.csv inspected with Dask (125,497,040 rows)\")\n",
    "print(\"✓ Guayas region scope defined (11 stores, 20.4% of total)\")\n",
    "print(\"✓ Top-3 families identified (2,393 items, 58.4% of total)\")\n",
    "print(\"✓ Inventory summary exported to docs/\")\n",
    "\n",
    "print(\"\\nFILES CREATED:\")\n",
    "print(f\"  - {(DOCS / 'inventory_summary.csv').resolve()}\")\n",
    "print(f\"  - {(DOCS / 'inventory_summary.txt').resolve()}\")\n",
    "\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(f\"  - Dataset: 125M rows, 6 columns, 479 MB\")\n",
    "print(f\"  - Scope filter: 11 stores × 2,393 items = ~2.3M potential rows\")\n",
    "print(f\"  - Missing data: onpromotion 17.26% (will fill with False)\")\n",
    "print(f\"  - Oil data: 43 missing values (3.5%)\")\n",
    "\n",
    "print(\"\\nNEXT STEPS (Day 2):\")\n",
    "print(\"  1. Filter train.csv to Guayas stores\")\n",
    "print(\"  2. Filter to top-3 families\")\n",
    "print(\"  3. Random sample 300K rows\")\n",
    "print(\"  4. Export guayas_sample_300k.csv\")\n",
    "\n",
    "print(\"\\nREADY FOR DAY 2 ✓\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
