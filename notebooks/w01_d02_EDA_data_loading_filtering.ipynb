{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5a481c",
   "metadata": {},
   "source": [
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "**w01_d02_EDA_data_loading_filtering.ipynb**\n",
    "\n",
    "**Author:** Alberto Diaz Durana  \n",
    "**Date:** November 2025  \n",
    "**Purpose:** Filter train.csv to Guayas region and top-3 families, create 300K sample for EDA\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook accomplishes the following:\n",
    "\n",
    "- Load train.csv with Dask and filter to Guayas stores (11 stores)\n",
    "- Merge with items.csv to get product family information\n",
    "- Filter to top-3 families: GROCERY I, BEVERAGES, CLEANING\n",
    "- Random sample 300,000 rows for development speed\n",
    "- Export filtered dataset as guayas_sample_300k.csv and .pkl\n",
    "- Validate filtering results and document reduction\n",
    "\n",
    "---\n",
    "\n",
    "## Business Context\n",
    "\n",
    "**Why this filtering matters:**\n",
    "\n",
    "Focusing on Guayas region and top-3 families enables:\n",
    "- Manageable dataset size (300K vs 125M rows)\n",
    "- Representative patterns (58.4% of items, 20.4% of stores)\n",
    "- Faster iteration during EDA and modeling\n",
    "- Regional insights for Guayas market specifically\n",
    "\n",
    "**Filtering criteria:**\n",
    "- Guayas stores: 11 of 54 (store IDs: 24, 26, 27, 28, 29, 30, 32, 34, 35, 36, 51)\n",
    "- Top-3 families: GROCERY I (1,334 items), BEVERAGES (613 items), CLEANING (446 items)\n",
    "- Sample: 300,000 random rows (reproducible with seed=42)\n",
    "\n",
    "**Deliverables:**\n",
    "- guayas_sample_300k.csv (300K rows, filtered dataset)\n",
    "- guayas_sample_300k.pkl (faster loading for Day 3)\n",
    "- Filtering validation report\n",
    "\n",
    "---\n",
    "\n",
    "## Input Dependencies\n",
    "\n",
    "From Day 1:\n",
    "- train.csv (125,497,040 rows)\n",
    "- stores.csv (54 stores)\n",
    "- items.csv (4,100 items)\n",
    "- Guayas store list: [24, 26, 27, 28, 29, 30, 32, 34, 35, 36, 51]\n",
    "- Top-3 families: ['GROCERY I', 'BEVERAGES', 'CLEANING']\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa601ce",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "\n",
    "**Objective:** Import libraries, load support files from Day 1, configure paths\n",
    "\n",
    "**Activities:**\n",
    "- Import pandas, dask, numpy\n",
    "- Load inventory results from Day 1 (stores, items)\n",
    "- Define path constants\n",
    "- Set random seed for reproducibility\n",
    "\n",
    "**Expected output:** Environment ready, support files reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab77bb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Package versions\n",
    "print(\"Package Versions:\")\n",
    "print(f\"  pandas: {pd.__version__}\")\n",
    "print(f\"  numpy: {np.__version__}\")\n",
    "print(f\"  dask: {dask.__version__}\")\n",
    "\n",
    "# Configure environment\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"\\nOK - Environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine paths (works from notebooks/ or project root)\n",
    "current_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "project_root = current_dir.parent if current_dir.name == 'notebooks' else current_dir\n",
    "\n",
    "# Define path constants\n",
    "DATA_RAW = project_root / 'data' / 'raw'\n",
    "DATA_PROCESSED = project_root / 'data' / 'processed'\n",
    "\n",
    "# Verify paths exist\n",
    "assert DATA_RAW.exists(), f\"ERROR - Path not found: {DATA_RAW}\"\n",
    "assert DATA_PROCESSED.exists(), f\"ERROR - Path not found: {DATA_PROCESSED}\"\n",
    "\n",
    "print(\"OK - Paths validated:\")\n",
    "print(f\"  Project root: {project_root.resolve()}\")\n",
    "print(f\"  DATA_RAW: {DATA_RAW.resolve()}\")\n",
    "print(f\"  DATA_PROCESSED: {DATA_PROCESSED.resolve()}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"\\nRandom seed set: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a656e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load support files from Day 1\n",
    "print(\"Loading support files...\")\n",
    "\n",
    "df_stores = pd.read_csv(DATA_RAW / 'stores.csv')\n",
    "df_items = pd.read_csv(DATA_RAW / 'items.csv')\n",
    "\n",
    "print(f\"✓ stores.csv: {len(df_stores)} stores\")\n",
    "print(f\"✓ items.csv: {len(df_items)} items\")\n",
    "\n",
    "# Define Guayas scope (from Day 1 analysis)\n",
    "guayas_store_nbrs = [24, 26, 27, 28, 29, 30, 32, 34, 35, 36, 51]\n",
    "top_3_families = ['GROCERY I', 'BEVERAGES', 'CLEANING']\n",
    "\n",
    "print(f\"\\nGuayas scope defined:\")\n",
    "print(f\"  Stores: {len(guayas_store_nbrs)} stores\")\n",
    "print(f\"  Store IDs: {guayas_store_nbrs}\")\n",
    "print(f\"  Families: {top_3_families}\")\n",
    "\n",
    "# Verify items in top-3 families\n",
    "top_3_items = df_items[df_items['family'].isin(top_3_families)]\n",
    "print(f\"  Items in top-3 families: {len(top_3_items):,}\")\n",
    "\n",
    "print(\"\\nOK - Support files loaded and scope defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e150170",
   "metadata": {},
   "source": [
    "## 2. Load & Filter train.csv to Guayas\n",
    "\n",
    "**Objective:** Load large train.csv with Dask and filter to Guayas stores only\n",
    "\n",
    "**Activities:**\n",
    "- Load train.csv with Dask (125M rows)\n",
    "- Filter to 11 Guayas stores\n",
    "- Convert filtered result to pandas\n",
    "- Validate row count reduction\n",
    "- Check memory usage\n",
    "\n",
    "**Expected output:** \n",
    "- Filtered dataset with ~25M rows (20% of original)\n",
    "- Pandas DataFrame ready for further filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train.csv with Dask\n",
    "print(\"Loading train.csv with Dask (125M rows)...\")\n",
    "print(\"This may take 1-2 minutes...\\n\")\n",
    "\n",
    "df_train = dd.read_csv(DATA_RAW / 'train.csv')\n",
    "\n",
    "print(f\"OK - train.csv loaded (Dask DataFrame)\")\n",
    "print(f\"  Columns: {list(df_train.columns)}\")\n",
    "print(f\"  Estimated rows: 125,497,040\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23031069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to Guayas stores only\n",
    "print(\"Filtering to Guayas stores (11 stores)...\")\n",
    "print(\"This will take 2-3 minutes - processing 125M rows...\\n\")\n",
    "\n",
    "df_train_guayas = df_train[df_train['store_nbr'].isin(guayas_store_nbrs)]\n",
    "\n",
    "print(\"Computing filtered dataset...\")\n",
    "df_train_guayas = df_train_guayas.compute()\n",
    "\n",
    "print(f\"\\nOK - Filtering complete!\")\n",
    "print(f\"  Original rows: 125,497,040\")\n",
    "print(f\"  Filtered rows: {len(df_train_guayas):,}\")\n",
    "print(f\"  Reduction: {(1 - len(df_train_guayas)/125497040)*100:.1f}%\")\n",
    "print(f\"  Memory usage: {df_train_guayas.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea76089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first rows and basic info\n",
    "print(\"First 5 rows of Guayas-filtered data:\")\n",
    "print(df_train_guayas.head())\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(df_train_guayas.dtypes)\n",
    "\n",
    "print(\"\\nUnique stores in filtered data:\")\n",
    "print(sorted(df_train_guayas['store_nbr'].unique()))\n",
    "\n",
    "print(\"\\nUnique items in filtered data:\")\n",
    "print(f\"  Total unique items: {df_train_guayas['item_nbr'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323fa222",
   "metadata": {},
   "source": [
    "## 3. Filter to Top-3 Families & Sample 300K\n",
    "\n",
    "**Objective:** Merge with items, filter to top-3 families, sample 300K rows\n",
    "\n",
    "**Activities:**\n",
    "- Merge train data with items.csv to get family column\n",
    "- Filter to top-3 families: GROCERY I, BEVERAGES, CLEANING\n",
    "- Random sample 300,000 rows (seed=42 for reproducibility)\n",
    "- Validate final dataset characteristics\n",
    "\n",
    "**Expected output:** \n",
    "- Final dataset: 300,000 rows\n",
    "- Guayas stores + top-3 families only\n",
    "- Representative sample of filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d30885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with items to get family information\n",
    "print(\"Merging with items.csv to get product family...\")\n",
    "\n",
    "df_train_merged = df_train_guayas.merge(\n",
    "    df_items[['item_nbr', 'family', 'class', 'perishable']], \n",
    "    on='item_nbr', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"OK - Merge complete\")\n",
    "print(f\"  Rows after merge: {len(df_train_merged):,}\")\n",
    "print(f\"  Columns: {list(df_train_merged.columns)}\")\n",
    "\n",
    "# Check for merge issues\n",
    "null_families = df_train_merged['family'].isnull().sum()\n",
    "print(f\"  Rows with missing family: {null_families}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5066f335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to top-3 families\n",
    "print(\"Filtering to top-3 families...\")\n",
    "print(f\"  Families to keep: {top_3_families}\\n\")\n",
    "\n",
    "df_train_top3 = df_train_merged[df_train_merged['family'].isin(top_3_families)].copy()\n",
    "\n",
    "print(f\"OK - Filtering complete\")\n",
    "print(f\"  Rows before family filter: {len(df_train_merged):,}\")\n",
    "print(f\"  Rows after family filter: {len(df_train_top3):,}\")\n",
    "print(f\"  Reduction: {(1 - len(df_train_top3)/len(df_train_merged))*100:.1f}%\")\n",
    "\n",
    "print(\"\\nFamily distribution in filtered data:\")\n",
    "family_counts = df_train_top3['family'].value_counts()\n",
    "print(family_counts)\n",
    "\n",
    "print(\"\\nPercentage by family:\")\n",
    "for family, count in family_counts.items():\n",
    "    pct = count / len(df_train_top3) * 100\n",
    "    print(f\"  {family:<15} {count:>10,} ({pct:>5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sample 300,000 rows\n",
    "print(\"Sampling 300,000 random rows...\")\n",
    "print(f\"  Random seed: {RANDOM_SEED}\")\n",
    "print(f\"  Sample fraction: {300000/len(df_train_top3)*100:.3f}%\\n\")\n",
    "\n",
    "df_sample = df_train_top3.sample(n=300000, random_state=RANDOM_SEED).copy()\n",
    "\n",
    "# Reset index for clean sequential numbering\n",
    "df_sample = df_sample.reset_index(drop=True)\n",
    "\n",
    "print(f\"OK - Sampling complete\")\n",
    "print(f\"  Sample size: {len(df_sample):,} rows\")\n",
    "print(f\"  Columns: {len(df_sample.columns)}\")\n",
    "print(f\"  Memory usage: {df_sample.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "print(\"\\nFamily distribution in sample:\")\n",
    "sample_family_counts = df_sample['family'].value_counts()\n",
    "for family, count in sample_family_counts.items():\n",
    "    pct = count / len(df_sample) * 100\n",
    "    print(f\"  {family:<15} {count:>7,} ({pct:>5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0075a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample characteristics\n",
    "print(\"Sample dataset overview:\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_sample.head())\n",
    "\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(df_sample.tail())\n",
    "\n",
    "print(\"\\nBasic statistics for unit_sales:\")\n",
    "print(df_sample['unit_sales'].describe())\n",
    "\n",
    "print(\"\\nStores in sample:\")\n",
    "store_counts = df_sample['store_nbr'].value_counts().sort_index()\n",
    "print(f\"  Unique stores: {df_sample['store_nbr'].nunique()}\")\n",
    "print(f\"  All 11 Guayas stores present: {set(df_sample['store_nbr'].unique()) == set(guayas_store_nbrs)}\")\n",
    "\n",
    "print(\"\\nDate range in sample:\")\n",
    "print(f\"  First date: {df_sample['date'].min()}\")\n",
    "print(f\"  Last date: {df_sample['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a5235",
   "metadata": {},
   "source": [
    "## 4. Export & Validate\n",
    "\n",
    "**Objective:** Save filtered sample and create validation summary\n",
    "\n",
    "**Activities:**\n",
    "- Export to CSV (guayas_sample_300k.csv)\n",
    "- Export to pickle (guayas_sample_300k.pkl) for faster loading\n",
    "- Create filtering summary report\n",
    "- Document data quality observations\n",
    "\n",
    "**Expected output:** \n",
    "- Two export files in data/processed/\n",
    "- Validation report confirming scope and quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0948347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "csv_path = DATA_PROCESSED / 'guayas_sample_300k.csv'\n",
    "print(f\"Exporting to CSV: {csv_path.name}\")\n",
    "df_sample.to_csv(csv_path, index=False)\n",
    "print(f\"  ✓ CSV saved ({csv_path.stat().st_size / 1024**2:.1f} MB)\")\n",
    "\n",
    "# Export to pickle (faster loading)\n",
    "pkl_path = DATA_PROCESSED / 'guayas_sample_300k.pkl'\n",
    "print(f\"\\nExporting to pickle: {pkl_path.name}\")\n",
    "df_sample.to_pickle(pkl_path)\n",
    "print(f\"  ✓ Pickle saved ({pkl_path.stat().st_size / 1024**2:.1f} MB)\")\n",
    "\n",
    "print(\"\\nOK - Export complete\")\n",
    "print(f\"  CSV: {csv_path.resolve()}\")\n",
    "print(f\"  Pickle: {pkl_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filtering summary report\n",
    "filtering_summary = {\n",
    "    'original_rows': 125_497_040,\n",
    "    'after_guayas_filter': 22_941_656,\n",
    "    'after_family_filter': 14_745_768,\n",
    "    'final_sample': 300_000,\n",
    "    'guayas_stores': len(guayas_store_nbrs),\n",
    "    'top_3_families': len(top_3_families),\n",
    "    'unique_items_in_sample': df_sample['item_nbr'].nunique(),\n",
    "    'date_range_start': str(df_sample['date'].min()),\n",
    "    'date_range_end': str(df_sample['date'].max()),\n",
    "    'missing_onpromotion': df_sample['onpromotion'].isnull().sum(),\n",
    "    'missing_onpromotion_pct': f\"{df_sample['onpromotion'].isnull().sum() / len(df_sample) * 100:.2f}%\",\n",
    "    'negative_sales_count': (df_sample['unit_sales'] < 0).sum(),\n",
    "    'negative_sales_pct': f\"{(df_sample['unit_sales'] < 0).sum() / len(df_sample) * 100:.2f}%\",\n",
    "}\n",
    "\n",
    "print(\"FILTERING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "for key, value in filtering_summary.items():\n",
    "    print(f\"{key:<30} {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA REDUCTION PIPELINE:\")\n",
    "print(f\"  125,497,040 rows (original)\")\n",
    "print(f\"  → 22,941,656 rows (Guayas filter, -81.7%)\")\n",
    "print(f\"  → 14,745,768 rows (top-3 families, -35.7%)\")\n",
    "print(f\"  → 300,000 rows (random sample, -98.0%)\")\n",
    "print(f\"  Final: 0.24% of original dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality validation checks\n",
    "print(\"DATA QUALITY VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check 1: All Guayas stores present\n",
    "all_stores_present = set(df_sample['store_nbr'].unique()) == set(guayas_store_nbrs)\n",
    "print(f\"✓ All 11 Guayas stores present: {all_stores_present}\")\n",
    "\n",
    "# Check 2: Only top-3 families\n",
    "only_top3 = set(df_sample['family'].unique()) == set(top_3_families)\n",
    "print(f\"✓ Only top-3 families present: {only_top3}\")\n",
    "\n",
    "# Check 3: No duplicate rows\n",
    "no_duplicates = df_sample.duplicated().sum() == 0\n",
    "print(f\"✓ No duplicate rows: {no_duplicates} (duplicates: {df_sample.duplicated().sum()})\")\n",
    "\n",
    "# Check 4: All required columns present\n",
    "required_cols = ['id', 'date', 'store_nbr', 'item_nbr', 'unit_sales', 'onpromotion', 'family', 'class', 'perishable']\n",
    "all_cols_present = all(col in df_sample.columns for col in required_cols)\n",
    "print(f\"✓ All required columns present: {all_cols_present}\")\n",
    "\n",
    "# Check 5: Date range coverage\n",
    "date_range_years = pd.to_datetime(df_sample['date']).dt.year.nunique()\n",
    "print(f\"✓ Date range covers {date_range_years} years (2013-2017)\")\n",
    "\n",
    "# Check 6: Memory efficiency\n",
    "memory_mb = df_sample.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"✓ Memory usage: {memory_mb:.1f} MB (manageable for analysis)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VALIDATION COMPLETE - Dataset ready for EDA (Day 3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e44b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook completion summary\n",
    "print(\"=\" * 70)\n",
    "print(\"NOTEBOOK COMPLETE: d02_w01_EDA_data_loading_filtering.ipynb\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nACCOMPLISHMENTS:\")\n",
    "print(\"✓ Loaded train.csv (125M rows) with Dask\")\n",
    "print(\"✓ Filtered to Guayas region (11 stores, 22.9M rows)\")\n",
    "print(\"✓ Filtered to top-3 families (14.7M rows)\")\n",
    "print(\"✓ Random sampled 300K rows (seed=42, reproducible)\")\n",
    "print(\"✓ Exported CSV and pickle to data/processed/\")\n",
    "\n",
    "print(\"\\nFILES CREATED:\")\n",
    "print(f\"  - guayas_sample_300k.csv (16.0 MB)\")\n",
    "print(f\"  - guayas_sample_300k.pkl (20.6 MB)\")\n",
    "\n",
    "print(\"\\nKEY CHARACTERISTICS:\")\n",
    "print(f\"  Rows: 300,000\")\n",
    "print(f\"  Stores: 11 (Guayas region)\")\n",
    "print(f\"  Families: 3 (GROCERY I 56.9%, BEVERAGES 22.0%, CLEANING 21.1%)\")\n",
    "print(f\"  Items: 2,296 unique\")\n",
    "print(f\"  Date range: 2013-01-02 to 2017-08-15 (5 years)\")\n",
    "print(f\"  Missing onpromotion: 18.57% (55,706 rows)\")\n",
    "print(f\"  Negative sales: 13 rows (0.00%)\")\n",
    "\n",
    "print(\"\\nDATA QUALITY NOTES:\")\n",
    "print(\"  → onpromotion NaN values: Will fill with False in Day 3\")\n",
    "print(\"  → Negative unit_sales: Will clip to 0 in Day 3\")\n",
    "print(\"  → Date column: String type, convert to datetime in Day 3\")\n",
    "print(\"  → Missing dates: Calendar gaps to fill in Day 3\")\n",
    "\n",
    "print(\"\\nNEXT STEPS (Day 3):\")\n",
    "print(\"  1. Load guayas_sample_300k.pkl\")\n",
    "print(\"  2. Handle missing onpromotion values (fill False)\")\n",
    "print(\"  3. Clip negative unit_sales to 0\")\n",
    "print(\"  4. Fill calendar gaps (complete daily index)\")\n",
    "print(\"  5. Outlier detection and analysis\")\n",
    "\n",
    "print(\"\\nREADY FOR DAY 3 - EDA Part 1: Quality & Preprocessing ✓\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
