{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fbf3d44",
   "metadata": {},
   "source": [
    "# FULL_01: Data Loading to Feature Engineering (Production Pipeline)\n",
    "\n",
    "**Purpose:** Process full Guayas dataset (no sampling) with 33 optimized features  \n",
    "**Source:** Consolidates w01_d01 through w02_d05 notebooks  \n",
    "**Output:** `data/processed/full_featured_data.pkl`\n",
    "\n",
    "**Key Decisions Applied:**\n",
    "- DEC-014: 33 features (exclude rolling std, oil, promotion interactions)\n",
    "- Full Guayas region, ALL families (no 300K sampling)\n",
    "\n",
    "**Environment:** WSL2 Ubuntu 22.04, Python 3.11, GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b556d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/berto/Demand-forecasting-in-retail\n",
      "Raw data: /home/berto/Demand-forecasting-in-retail/data/raw\n",
      "Processed data: /home/berto/Demand-forecasting-in-retail/data/processed\n",
      "\n",
      "Pandas version: 2.3.3\n",
      "NumPy version: 2.3.5\n"
     ]
    }
   ],
   "source": [
    "### Section 1: Environment Setup\n",
    "# Source: w01_d01_SETUP_data_inventory.ipynb\n",
    "# Standard libraries and path configuration for WSL2 environment\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Path configuration\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA_RAW = PROJECT_ROOT / 'data' / 'raw'\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed'\n",
    "\n",
    "# Ensure output directory exists\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Raw data: {DATA_RAW}\")\n",
    "print(f\"Processed data: {DATA_PROCESSED}\")\n",
    "print(f\"\\nPandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8780f17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data inventory:\n",
      "--------------------------------------------------\n",
      "  train.csv                   4765.9 MB\n",
      "  stores.csv                     0.0 MB\n",
      "  items.csv                      0.1 MB\n",
      "  oil.csv                        0.0 MB\n",
      "  holidays_events.csv            0.0 MB\n",
      "  transactions.csv               1.5 MB\n"
     ]
    }
   ],
   "source": [
    "### Section 1.1: Data Inventory\n",
    "# Source: w01_d01_SETUP_data_inventory.ipynb\n",
    "# Verify all required Kaggle files are present\n",
    "\n",
    "required_files = [\n",
    "    'train.csv',\n",
    "    'stores.csv', \n",
    "    'items.csv',\n",
    "    'oil.csv',\n",
    "    'holidays_events.csv',\n",
    "    'transactions.csv'\n",
    "]\n",
    "\n",
    "print(\"Raw data inventory:\")\n",
    "print(\"-\" * 50)\n",
    "for f in required_files:\n",
    "    filepath = DATA_RAW / f\n",
    "    if filepath.exists():\n",
    "        size_mb = filepath.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {f:<25} {size_mb:>8.1f} MB\")\n",
    "    else:\n",
    "        print(f\"  {f:<25} MISSING\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f082982",
   "metadata": {},
   "source": [
    "all files present. The train.csv is 4.77 GB which is the full dataset (~125M rows). Now we need to load it with Guayas filtering. Given the file size, we should use chunked loading to filter efficiently.\n",
    "Next cell should load stores.csv first to identify Guayas store numbers, then load train.csv with filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a419a2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stores dataset:\n",
      "  Shape: (54, 5)\n",
      "\n",
      "Stores by state:\n",
      "state\n",
      "Pichincha                         19\n",
      "Guayas                            11\n",
      "Azuay                              3\n",
      "Santo Domingo de los Tsachilas     3\n",
      "Manabi                             3\n",
      "Los Rios                           2\n",
      "Cotopaxi                           2\n",
      "Tungurahua                         2\n",
      "El Oro                             2\n",
      "Bolivar                            1\n",
      "Imbabura                           1\n",
      "Chimborazo                         1\n",
      "Pastaza                            1\n",
      "Santa Elena                        1\n",
      "Loja                               1\n",
      "Esmeraldas                         1\n",
      "\n",
      "Guayas store numbers (11 stores): [24, 26, 27, 28, 29, 30, 32, 34, 35, 36, 51]\n"
     ]
    }
   ],
   "source": [
    "### Section 2: Data Loading\n",
    "# Source: w01_d02_EDA_data_loading_filtering.ipynb\n",
    "# Load stores metadata, filter to Guayas province\n",
    "\n",
    "stores = pd.read_csv(DATA_RAW / 'stores.csv')\n",
    "\n",
    "print(\"Stores dataset:\")\n",
    "print(f\"  Shape: {stores.shape}\")\n",
    "print(f\"\\nStores by state:\")\n",
    "print(stores['state'].value_counts().to_string())\n",
    "\n",
    "# Identify Guayas stores\n",
    "guayas_stores = stores[stores['state'] == 'Guayas']['store_nbr'].tolist()\n",
    "print(f\"\\nGuayas store numbers ({len(guayas_stores)} stores): {guayas_stores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2eaac1",
   "metadata": {},
   "source": [
    "we have 11 Guayas stores identified. Now we need to load train.csv but it's 4.7GB which is large. We should load it with filtering to only Guayas stores to save memory. We can use chunked reading or just load and filter.\n",
    "Let me provide the next cell to load train.csv filtering to Guayas only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b5778b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train.csv (chunked, filtering to 11 Guayas stores)...\n",
      "  Processed 25M rows...\n",
      "  Processed 50M rows...\n",
      "  Processed 75M rows...\n",
      "  Processed 100M rows...\n",
      "  Processed 125M rows...\n",
      "\n",
      "Guayas train data loaded:\n",
      "  Shape: (22941656, 6)\n",
      "  Memory: 3011.4 MB\n",
      "  Load time: 64.0 seconds\n"
     ]
    }
   ],
   "source": [
    "### Section 2.1: Load Train Data with Guayas Filter\n",
    "# Source: w01_d02_EDA_data_loading_filtering.ipynb\n",
    "# Load train.csv in chunks, filter to Guayas stores only\n",
    "# Full dataset: ~125M rows, Guayas subset: ~2M rows expected\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read in chunks to manage memory (4.7GB file)\n",
    "chunks = []\n",
    "chunk_size = 1_000_000\n",
    "\n",
    "print(f\"Loading train.csv (chunked, filtering to {len(guayas_stores)} Guayas stores)...\")\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(DATA_RAW / 'train.csv', chunksize=chunk_size)):\n",
    "    guayas_chunk = chunk[chunk['store_nbr'].isin(guayas_stores)]\n",
    "    chunks.append(guayas_chunk)\n",
    "    if (i + 1) % 25 == 0:\n",
    "        print(f\"  Processed {(i+1) * chunk_size / 1e6:.0f}M rows...\")\n",
    "\n",
    "train = pd.concat(chunks, ignore_index=True)\n",
    "del chunks  # Free memory\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nGuayas train data loaded:\")\n",
    "print(f\"  Shape: {train.shape}\")\n",
    "print(f\"  Memory: {train.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n",
    "print(f\"  Load time: {load_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a28395b",
   "metadata": {},
   "source": [
    "22.9M rows for Guayas - that's much larger than expected (~2M). This is the full Guayas data across all dates and all items. We'll need to filter by date range and potentially check the data structure before proceeding with feature engineering.\n",
    "Let me continue with the next cell to explore the data structure and date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2432e050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data structure:\n",
      "  Columns: ['id', 'date', 'store_nbr', 'item_nbr', 'unit_sales', 'onpromotion']\n",
      "  Date range: 2013-01-02 00:00:00 to 2017-08-15 00:00:00\n",
      "  Unique stores: 11\n",
      "  Unique items: 4008\n",
      "  Unique dates: 1680\n",
      "\n",
      "Date coverage:\n",
      "  Total days in range: 1687\n",
      "\n",
      "Sample rows:\n",
      "   id       date  store_nbr  item_nbr  unit_sales onpromotion\n",
      "18789 2013-01-02         24    103665         6.0         NaN\n",
      "18790 2013-01-02         24    105574        12.0         NaN\n",
      "18791 2013-01-02         24    105575         2.0         NaN\n"
     ]
    }
   ],
   "source": [
    "### Section 2.2: Train Data Structure\n",
    "# Source: w01_d02_EDA_data_loading_filtering.ipynb\n",
    "# Examine date range, stores, items\n",
    "\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "\n",
    "print(\"Train data structure:\")\n",
    "print(f\"  Columns: {train.columns.tolist()}\")\n",
    "print(f\"  Date range: {train['date'].min()} to {train['date'].max()}\")\n",
    "print(f\"  Unique stores: {train['store_nbr'].nunique()}\")\n",
    "print(f\"  Unique items: {train['item_nbr'].nunique()}\")\n",
    "print(f\"  Unique dates: {train['date'].nunique()}\")\n",
    "\n",
    "print(f\"\\nDate coverage:\")\n",
    "print(f\"  Total days in range: {(train['date'].max() - train['date'].min()).days + 1}\")\n",
    "\n",
    "print(f\"\\nSample rows:\")\n",
    "print(train.head(3).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fd7c25",
   "metadata": {},
   "source": [
    "Data 22.9M rows, 11 stores, 4008 items, dates from 2013-01-02 to 2017-08-15. Now we need to filter to our target period for DEC-016 (Q4 2013 + Q1 2014 for training, March 2014 for test). But first, let's load the supporting files (items, holidays) and then filter to the relevant date range.\n",
    "Actually, looking at the handoff document, we need:\n",
    "\n",
    "Training: Q4 2013 + Q1 2014 (Oct 1, 2013 - Feb 21, 2014)\n",
    "Gap: 7 days (Feb 22-28, 2014) per DEC-013\n",
    "Test: March 2014 (Mar 1-31, 2014)\n",
    "\n",
    "So we only need data from roughly Oct 2013 to March 2014 for the model. However, for feature engineering (lags up to 30 days), we need data starting from at least September 2013.\n",
    "Let me filter the date range first to reduce memory, then proceed with feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8a0b9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 2013-09-01 - 2014-03-31:\n",
      "  Shape: (2089152, 6)\n",
      "  Memory: 167.1 MB\n",
      "  Date range: 2013-09-01 to 2014-03-31\n",
      "  Unique dates: 211\n",
      "  Unique items: 2638\n"
     ]
    }
   ],
   "source": [
    "### Section 2.3: Filter to Target Period\n",
    "# Source: w01_d02_EDA_data_loading_filtering.ipynb\n",
    "# DEC-016: Q4 2013 + Q1 2014 training, March 2014 test\n",
    "# Need September 2013 start for 30-day lag buffer\n",
    "\n",
    "DATE_START = '2013-09-01'  # Buffer for 30-day lags\n",
    "DATE_END = '2014-03-31'    # Test period end\n",
    "\n",
    "train = train[(train['date'] >= DATE_START) & (train['date'] <= DATE_END)].copy()\n",
    "\n",
    "print(f\"Filtered to {DATE_START} - {DATE_END}:\")\n",
    "print(f\"  Shape: {train.shape}\")\n",
    "print(f\"  Memory: {train.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n",
    "print(f\"  Date range: {train['date'].min().date()} to {train['date'].max().date()}\")\n",
    "print(f\"  Unique dates: {train['date'].nunique()}\")\n",
    "print(f\"  Unique items: {train['item_nbr'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e822f",
   "metadata": {},
   "source": [
    "Filtered down from 22.9M to 2.1M rows. This is the target period we need. Now we need to load supporting data (items, holidays) and start feature engineering.\n",
    "Next steps:\n",
    "\n",
    "Load items metadata (for family, perishable)\n",
    "Load holidays data\n",
    "Create calendar features\n",
    "Create lag features\n",
    "Create rolling features\n",
    "Create aggregation features\n",
    "Assemble final 33 features\n",
    "\n",
    "Let me continue with items data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce1b9894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items metadata:\n",
      "  Shape: (4100, 4)\n",
      "  Columns: ['item_nbr', 'family', 'class', 'perishable']\n",
      "\n",
      "Product families (33):\n",
      "family\n",
      "GROCERY I        1334\n",
      "BEVERAGES         613\n",
      "CLEANING          446\n",
      "PRODUCE           306\n",
      "DAIRY             242\n",
      "PERSONAL CARE     153\n",
      "BREAD/BAKERY      134\n",
      "HOME CARE         108\n",
      "DELI               91\n",
      "MEATS              84\n",
      "\n",
      "Perishable distribution:\n",
      "perishable\n",
      "0    3114\n",
      "1     986\n",
      "\n",
      "Train after items merge:\n",
      "  Shape: (2089152, 9)\n"
     ]
    }
   ],
   "source": [
    "### Section 3: Load Supporting Data - Items\n",
    "# Source: w01_d02_EDA_data_loading_filtering.ipynb\n",
    "# Items metadata: family, class, perishable flag\n",
    "\n",
    "items = pd.read_csv(DATA_RAW / 'items.csv')\n",
    "\n",
    "print(\"Items metadata:\")\n",
    "print(f\"  Shape: {items.shape}\")\n",
    "print(f\"  Columns: {items.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\nProduct families ({items['family'].nunique()}):\")\n",
    "family_counts = items['family'].value_counts()\n",
    "print(family_counts.head(10).to_string())\n",
    "\n",
    "print(f\"\\nPerishable distribution:\")\n",
    "print(items['perishable'].value_counts().to_string())\n",
    "\n",
    "# Merge items info into train\n",
    "train = train.merge(items[['item_nbr', 'family', 'class', 'perishable']], \n",
    "                    on='item_nbr', how='left')\n",
    "\n",
    "print(f\"\\nTrain after items merge:\")\n",
    "print(f\"  Shape: {train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682d542",
   "metadata": {},
   "source": [
    "We now have items merged. Next we need to load stores data (already have it, but need to merge), holidays, and oil data. However, per DEC-014, oil features are excluded, so we can skip oil processing. Let me load holidays next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "edcea370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holidays data:\n",
      "  Shape: (26, 6)\n",
      "  Columns: ['date', 'type', 'locale', 'locale_name', 'description', 'transferred']\n",
      "\n",
      "Holiday types:\n",
      "type\n",
      "Holiday       18\n",
      "Additional     7\n",
      "Transfer       1\n",
      "\n",
      "Sample holidays in period:\n",
      "      date     type   locale locale_name                         description\n",
      "2013-09-28  Holiday    Local      Ibarra                 Fundacion de Ibarra\n",
      "2013-10-07  Holiday    Local     Quevedo            Cantonizacion de Quevedo\n",
      "2013-10-09  Holiday National     Ecuador          Independencia de Guayaquil\n",
      "2013-10-11 Transfer National     Ecuador Traslado Independencia de Guayaquil\n",
      "2013-11-02  Holiday National     Ecuador                     Dia de Difuntos\n"
     ]
    }
   ],
   "source": [
    "### Section 3.1: Load Supporting Data - Holidays\n",
    "# Source: w01_d02_EDA_data_loading_filtering.ipynb\n",
    "# Holidays and events for Ecuador\n",
    "\n",
    "holidays = pd.read_csv(DATA_RAW / 'holidays_events.csv')\n",
    "holidays['date'] = pd.to_datetime(holidays['date'])\n",
    "\n",
    "# Filter to target period\n",
    "holidays = holidays[(holidays['date'] >= DATE_START) & (holidays['date'] <= DATE_END)].copy()\n",
    "\n",
    "print(\"Holidays data:\")\n",
    "print(f\"  Shape: {holidays.shape}\")\n",
    "print(f\"  Columns: {holidays.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\nHoliday types:\")\n",
    "print(holidays['type'].value_counts().to_string())\n",
    "\n",
    "print(f\"\\nSample holidays in period:\")\n",
    "print(holidays[['date', 'type', 'locale', 'locale_name', 'description']].head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05941b49",
   "metadata": {},
   "source": [
    "We have the holidays data loaded. Now we need to load stores metadata (we already have it) and create a complete calendar. Let me continue with the data loading steps.\n",
    "Next I should:\n",
    "\n",
    "Load transactions (optional, but we have it)\n",
    "Create complete calendar (all dates for all store-item combinations)\n",
    "Then start feature engineering\n",
    "\n",
    "Actually, looking at the Week 2 notebooks structure, I think we should:\n",
    "\n",
    "First create a complete calendar/skeleton\n",
    "Then merge all the supporting data\n",
    "Then do quality checks\n",
    "Then feature engineering\n",
    "\n",
    "Let me continue with loading stores (we already have it but need to merge) and then create the complete calendar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b0c498b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train after stores merge:\n",
      "  Shape: (2089152, 13)\n",
      "  Columns: ['id', 'date', 'store_nbr', 'item_nbr', 'unit_sales', 'onpromotion', 'family', 'class', 'perishable', 'city', 'state', 'type', 'cluster']\n",
      "\n",
      "Store clusters distribution:\n",
      "cluster\n",
      "1     2\n",
      "3     3\n",
      "6     1\n",
      "10    3\n",
      "17    1\n",
      "\n",
      "Store types distribution:\n",
      "type\n",
      "A    1\n",
      "B    1\n",
      "C    3\n",
      "D    3\n",
      "E    2\n"
     ]
    }
   ],
   "source": [
    "### Section 3.2: Merge Stores Metadata\n",
    "# Source: w01_d02_EDA_data_loading_filtering.ipynb\n",
    "# Add store cluster, city, type information\n",
    "\n",
    "train = train.merge(stores[['store_nbr', 'city', 'state', 'type', 'cluster']], \n",
    "                    on='store_nbr', how='left')\n",
    "\n",
    "print(\"Train after stores merge:\")\n",
    "print(f\"  Shape: {train.shape}\")\n",
    "print(f\"  Columns: {train.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\nStore clusters distribution:\")\n",
    "print(train.groupby('cluster')['store_nbr'].nunique().to_string())\n",
    "\n",
    "print(f\"\\nStore types distribution:\")\n",
    "print(train.groupby('type')['store_nbr'].nunique().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa245642",
   "metadata": {},
   "source": [
    " All metadata merged. Now I need to move to data quality checks and preprocessing. This should check for missing values, negatives (returns), outliers, etc. Following the Week 1 pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a79e8e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Assessment:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Missing values:\n",
      "             Missing  Percent\n",
      "onpromotion  2089152    100.0\n",
      "\n",
      "Unit sales statistics:\n",
      "  Min: -120.0\n",
      "  Max: 4851.002\n",
      "  Mean: 7.58\n",
      "  Median: 3.90\n",
      "\n",
      "Negative sales (returns):\n",
      "  Count: 107 (0.01%)\n"
     ]
    }
   ],
   "source": [
    "### Section 4: Data Quality & Preprocessing\n",
    "# Source: w01_d03_EDA_quality_preprocessing.ipynb\n",
    "# Check for missing values, negative sales, outliers\n",
    "\n",
    "print(\"Data Quality Assessment:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "missing = train.isnull().sum()\n",
    "missing_pct = (missing / len(train) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing': missing[missing > 0],\n",
    "    'Percent': missing_pct[missing > 0]\n",
    "})\n",
    "print(missing_df.to_string())\n",
    "\n",
    "print(f\"\\nUnit sales statistics:\")\n",
    "print(f\"  Min: {train['unit_sales'].min()}\")\n",
    "print(f\"  Max: {train['unit_sales'].max()}\")\n",
    "print(f\"  Mean: {train['unit_sales'].mean():.2f}\")\n",
    "print(f\"  Median: {train['unit_sales'].median():.2f}\")\n",
    "\n",
    "print(f\"\\nNegative sales (returns):\")\n",
    "negative_count = (train['unit_sales'] < 0).sum()\n",
    "negative_pct = (negative_count / len(train) * 100)\n",
    "print(f\"  Count: {negative_count} ({negative_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183f9fd1",
   "metadata": {},
   "source": [
    "Interesting - onpromotion is 100% missing for this time period. This is different from the Week 3 development sample where it had ~16% NaN. This could be because:\n",
    "\n",
    "The onpromotion column wasn't being tracked in late 2013/early 2014\n",
    "The data for Guayas stores specifically doesn't have promotion data\n",
    "The filtered period doesn't have promotion data\n",
    "\n",
    "Since DEC-014 excluded promotion interaction features but kept the base onpromotion feature, we need to handle this. We should fill with 0 (no promotion) for now and note this limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f30cb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocessing:\n",
      "  onpromotion filled: 2089152 zeros, 0 ones\n",
      "  Negative sales clipped to >= -100\n",
      "  Current unit_sales range: [-100.0, 4851.002]\n",
      "\n",
      "Final train shape: (2089152, 13)\n",
      "Missing values remaining: 0\n"
     ]
    }
   ],
   "source": [
    "### Section 4.1: Handle Missing Values and Negative Sales\n",
    "# Source: w01_d03_EDA_quality_preprocessing.ipynb\n",
    "# Fill onpromotion NaN with 0 (no promotion)\n",
    "# Keep negative sales (returns are valid business events)\n",
    "\n",
    "# Fill onpromotion - 100% NaN in this period suggests no promotion tracking\n",
    "train['onpromotion'] = train['onpromotion'].fillna(0).astype(int)\n",
    "\n",
    "print(\"After preprocessing:\")\n",
    "print(f\"  onpromotion filled: {(train['onpromotion'] == 0).sum()} zeros, {(train['onpromotion'] == 1).sum()} ones\")\n",
    "\n",
    "# Clip extreme negatives to reasonable return threshold\n",
    "train.loc[train['unit_sales'] < -100, 'unit_sales'] = -100\n",
    "\n",
    "print(f\"  Negative sales clipped to >= -100\")\n",
    "print(f\"  Current unit_sales range: [{train['unit_sales'].min()}, {train['unit_sales'].max()}]\")\n",
    "\n",
    "print(f\"\\nFinal train shape: {train.shape}\")\n",
    "print(f\"Missing values remaining: {train.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d83e8a",
   "metadata": {},
   "source": [
    "Data is clean now. Next we need to create the calendar scaffold (complete date grid) to handle missing store-item-date combinations (which represent zero sales). This is critical for time series.\n",
    "Then we'll move into feature engineering:\n",
    "\n",
    "Temporal features (lags, rolling)\n",
    "Calendar features (day of week, etc.)\n",
    "Holiday features\n",
    "Store/item aggregations\n",
    "\n",
    "Let me create the calendar scaffold next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e8257d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating complete calendar scaffold...\n",
      "  Dates: 212\n",
      "  Stores: 10\n",
      "  Items: 2638\n",
      "  Expected combinations: 5,592,560\n",
      "\n",
      "Calendar scaffold shape: (5592560, 3)\n",
      "  Memory: 134.2 MB\n"
     ]
    }
   ],
   "source": [
    "### Section 5: Create Complete Calendar\n",
    "# Source: w01_d03_EDA_quality_preprocessing.ipynb\n",
    "# Missing store-item-date combinations = zero sales\n",
    "# Create complete date grid for all store-item pairs\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "print(\"Creating complete calendar scaffold...\")\n",
    "\n",
    "# Get unique values\n",
    "unique_dates = pd.date_range(start=DATE_START, end=DATE_END, freq='D')\n",
    "unique_stores = train['store_nbr'].unique()\n",
    "unique_items = train['item_nbr'].unique()\n",
    "\n",
    "print(f\"  Dates: {len(unique_dates)}\")\n",
    "print(f\"  Stores: {len(unique_stores)}\")\n",
    "print(f\"  Items: {len(unique_items)}\")\n",
    "print(f\"  Expected combinations: {len(unique_dates) * len(unique_stores) * len(unique_items):,}\")\n",
    "\n",
    "# Create complete grid\n",
    "calendar = pd.DataFrame(\n",
    "    list(product(unique_dates, unique_stores, unique_items)),\n",
    "    columns=['date', 'store_nbr', 'item_nbr']\n",
    ")\n",
    "\n",
    "print(f\"\\nCalendar scaffold shape: {calendar.shape}\")\n",
    "print(f\"  Memory: {calendar.memory_usage(deep=True).sum() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0fd0c4",
   "metadata": {},
   "source": [
    "We have 5.6M complete combinations. Now we need to merge the actual sales data with this calendar scaffold, filling missing combinations with 0 sales. This will expand from 2.1M rows (sparse) to 5.6M rows (complete calendar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce2f288f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging actual sales with calendar scaffold...\n",
      "\n",
      "Complete dataset:\n",
      "  Shape: (5592560, 5)\n",
      "  Memory: 223.7 MB\n",
      "\n",
      "Data density:\n",
      "  Original rows: 2,089,152\n",
      "  Complete rows: 5,592,560\n",
      "  Density: 37.4% (sparsity: 62.6%)\n"
     ]
    }
   ],
   "source": [
    "### Section 5.1: Merge Sales Data with Complete Calendar\n",
    "# Source: w01_d03_EDA_quality_preprocessing.ipynb\n",
    "# Left join: calendar with actual sales, fill missing = 0\n",
    "\n",
    "print(\"Merging actual sales with calendar scaffold...\")\n",
    "\n",
    "# Merge calendar with train data\n",
    "df = calendar.merge(\n",
    "    train[['date', 'store_nbr', 'item_nbr', 'unit_sales', 'onpromotion']],\n",
    "    on=['date', 'store_nbr', 'item_nbr'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing sales with 0 (no sales that day)\n",
    "df['unit_sales'] = df['unit_sales'].fillna(0)\n",
    "df['onpromotion'] = df['onpromotion'].fillna(0).astype(int)\n",
    "\n",
    "print(f\"\\nComplete dataset:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n",
    "\n",
    "print(f\"\\nData density:\")\n",
    "original_rows = len(train)\n",
    "complete_rows = len(df)\n",
    "density = (original_rows / complete_rows * 100)\n",
    "print(f\"  Original rows: {original_rows:,}\")\n",
    "print(f\"  Complete rows: {complete_rows:,}\")\n",
    "print(f\"  Density: {density:.1f}% (sparsity: {100-density:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87f513",
   "metadata": {},
   "source": [
    "The dataset shows 62.6% sparsity, which is much better than the 99.1% mentioned in the user's memories. This makes sense because we're working with the complete Guayas dataset (all families) rather than the sampled one.\n",
    "Now I need to add metadata back to the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1262d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata to complete dataset...\n",
      "\n",
      "Complete dataset with metadata:\n",
      "  Shape: (5592560, 12)\n",
      "  Columns: ['date', 'store_nbr', 'item_nbr', 'unit_sales', 'onpromotion', 'family', 'class', 'perishable', 'city', 'state', 'type', 'cluster']\n",
      "  Memory: 1767.5 MB\n",
      "\n",
      "Sample rows:\n",
      "      date  store_nbr  item_nbr  unit_sales  onpromotion       family  class  perishable      city  state type  cluster\n",
      "2013-09-01         24     96995         2.0            0    GROCERY I   1093           0 Guayaquil Guayas    D        1\n",
      "2013-09-01         24    103665         3.0            0 BREAD/BAKERY   2712           1 Guayaquil Guayas    D        1\n",
      "2013-09-01         24    105574         8.0            0    GROCERY I   1045           0 Guayaquil Guayas    D        1\n"
     ]
    }
   ],
   "source": [
    "### Section 5.2: Merge Metadata Back\n",
    "# Source: w01_d03_EDA_quality_preprocessing.ipynb\n",
    "# Add items and stores metadata to complete calendar\n",
    "\n",
    "print(\"Adding metadata to complete dataset...\")\n",
    "\n",
    "# Merge items metadata\n",
    "df = df.merge(\n",
    "    items[['item_nbr', 'family', 'class', 'perishable']],\n",
    "    on='item_nbr',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge stores metadata\n",
    "df = df.merge(\n",
    "    stores[['store_nbr', 'city', 'state', 'type', 'cluster']],\n",
    "    on='store_nbr',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\nComplete dataset with metadata:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Columns: {df.columns.tolist()}\")\n",
    "print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n",
    "\n",
    "print(f\"\\nSample rows:\")\n",
    "print(df.head(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fbed86",
   "metadata": {},
   "source": [
    "Now we have a complete dataset with 5.6M rows. The sparsity is 62.6%, which is consistent with retail data where not all items sell every day in every store.\n",
    "Next steps for feature engineering:\n",
    "\n",
    "Temporal features (lag features)\n",
    "Rolling statistics\n",
    "Calendar features\n",
    "Aggregations\n",
    "\n",
    "Following DEC-014, we need to create 33 features excluding:\n",
    "\n",
    "Rolling std features\n",
    "Oil features\n",
    "Promotion interaction features\n",
    "\n",
    "Let me proceed with the lag features first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27d72e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting data for time series operations...\n",
      "\n",
      "Dataset sorted:\n",
      "  Shape: (5592560, 12)\n",
      "  Order: store_nbr, item_nbr, date\n",
      "\n",
      "Sample sorted data (first store-item pair):\n",
      "      date  store_nbr  item_nbr  unit_sales\n",
      "2013-09-01         24     96995         2.0\n",
      "2013-09-02         24     96995         2.0\n",
      "2013-09-03         24     96995         2.0\n",
      "2013-09-04         24     96995         0.0\n",
      "2013-09-05         24     96995         0.0\n"
     ]
    }
   ],
   "source": [
    "### Section 6: Prepare for Feature Engineering\n",
    "# Source: w02_d01_FE_lags.ipynb\n",
    "# Sort by store, item, date to enable lag/rolling calculations\n",
    "\n",
    "print(\"Sorting data for time series operations...\")\n",
    "\n",
    "df = df.sort_values(['store_nbr', 'item_nbr', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset sorted:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Order: store_nbr, item_nbr, date\")\n",
    "\n",
    "print(f\"\\nSample sorted data (first store-item pair):\")\n",
    "sample = df.head(5)[['date', 'store_nbr', 'item_nbr', 'unit_sales']]\n",
    "print(sample.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0596d1",
   "metadata": {},
   "source": [
    "The data is sorted by store, item, and date. Now we need to start feature engineering. According to DEC-014, we need 33 features:\n",
    "Temporal (8):\n",
    "\n",
    "unit_sales_lag1, lag7, lag14, lag30\n",
    "unit_sales_7d_avg, 14d_avg, 30d_avg\n",
    "unit_sales_lag1_7d_corr\n",
    "\n",
    "Calendar (7):\n",
    "\n",
    "year, month, day, dayofweek, dayofyear, weekofyear, quarter\n",
    "\n",
    "Holiday (4):\n",
    "\n",
    "holiday_proximity, is_holiday, holiday_period, days_to_next_holiday\n",
    "\n",
    "Promotion (2):\n",
    "\n",
    "onpromotion, promo_item_interaction\n",
    "\n",
    "Store/Item (7):\n",
    "\n",
    "cluster, store_avg_sales, item_avg_sales, item_store_avg\n",
    "cluster_avg_sales, family_avg_sales, city_avg_sales\n",
    "\n",
    "Derived (5):\n",
    "\n",
    "perishable, weekend, month_start, month_end, is_payday\n",
    "\n",
    "Let me start with lag features. The user wants cell-by-cell execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd6bffce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lag features...\n",
      "\n",
      "Lag features created:\n",
      "  unit_sales_lag1, lag7, lag14, lag30\n",
      "\n",
      "NaN counts (expected at start of series):\n",
      "  unit_sales_lag1: 26,380 (0.47%)\n",
      "  unit_sales_lag7: 184,660 (3.30%)\n",
      "  unit_sales_lag14: 369,320 (6.60%)\n",
      "  unit_sales_lag30: 791,400 (14.15%)\n",
      "\n",
      "Sample with lags (first store-item pair):\n",
      "      date  unit_sales  unit_sales_lag1  unit_sales_lag7  unit_sales_lag14  unit_sales_lag30\n",
      "2013-09-01         2.0              NaN              NaN               NaN               NaN\n",
      "2013-09-02         2.0              2.0              NaN               NaN               NaN\n",
      "2013-09-03         2.0              2.0              NaN               NaN               NaN\n",
      "2013-09-04         0.0              2.0              NaN               NaN               NaN\n",
      "2013-09-05         0.0              0.0              NaN               NaN               NaN\n",
      "2013-09-06         3.0              0.0              NaN               NaN               NaN\n",
      "2013-09-07         1.0              3.0              NaN               NaN               NaN\n",
      "2013-09-08         1.0              1.0              2.0               NaN               NaN\n",
      "2013-09-09         1.0              1.0              2.0               NaN               NaN\n",
      "2013-09-10         0.0              1.0              2.0               NaN               NaN\n",
      "2013-09-11         2.0              0.0              0.0               NaN               NaN\n",
      "2013-09-12         0.0              2.0              0.0               NaN               NaN\n",
      "2013-09-13         1.0              0.0              3.0               NaN               NaN\n",
      "2013-09-14         2.0              1.0              1.0               NaN               NaN\n",
      "2013-09-15         2.0              2.0              1.0               2.0               NaN\n",
      "2013-09-16         1.0              2.0              1.0               2.0               NaN\n",
      "2013-09-17         3.0              1.0              0.0               2.0               NaN\n",
      "2013-09-18         2.0              3.0              2.0               0.0               NaN\n",
      "2013-09-19         3.0              2.0              0.0               0.0               NaN\n",
      "2013-09-20         2.0              3.0              1.0               3.0               NaN\n",
      "2013-09-21         1.0              2.0              2.0               1.0               NaN\n",
      "2013-09-22         0.0              1.0              2.0               1.0               NaN\n",
      "2013-09-23         3.0              0.0              1.0               1.0               NaN\n",
      "2013-09-24         0.0              3.0              3.0               0.0               NaN\n",
      "2013-09-25         0.0              0.0              2.0               2.0               NaN\n",
      "2013-09-26         1.0              0.0              3.0               0.0               NaN\n",
      "2013-09-27         4.0              1.0              2.0               1.0               NaN\n",
      "2013-09-28         3.0              4.0              1.0               2.0               NaN\n",
      "2013-09-29         0.0              3.0              0.0               2.0               NaN\n",
      "2013-09-30         1.0              0.0              3.0               1.0               NaN\n",
      "2013-10-01         1.0              1.0              0.0               3.0               2.0\n",
      "2013-10-02         2.0              1.0              0.0               2.0               2.0\n",
      "2013-10-03         2.0              2.0              1.0               3.0               2.0\n",
      "2013-10-04         0.0              2.0              4.0               2.0               0.0\n",
      "2013-10-05         0.0              0.0              3.0               1.0               0.0\n"
     ]
    }
   ],
   "source": [
    "### Section 7: Feature Engineering - Lag Features\n",
    "# Source: w02_d01_FE_lags.ipynb\n",
    "# Create lag 1, 7, 14, 30 for autocorrelation patterns\n",
    "\n",
    "print(\"Creating lag features...\")\n",
    "\n",
    "# Group by store-item for time series operations\n",
    "grouped = df.groupby(['store_nbr', 'item_nbr'])['unit_sales']\n",
    "\n",
    "# Create lags\n",
    "df['unit_sales_lag1'] = grouped.shift(1)\n",
    "df['unit_sales_lag7'] = grouped.shift(7)\n",
    "df['unit_sales_lag14'] = grouped.shift(14)\n",
    "df['unit_sales_lag30'] = grouped.shift(30)\n",
    "\n",
    "print(f\"\\nLag features created:\")\n",
    "print(f\"  unit_sales_lag1, lag7, lag14, lag30\")\n",
    "\n",
    "print(f\"\\nNaN counts (expected at start of series):\")\n",
    "for col in ['unit_sales_lag1', 'unit_sales_lag7', 'unit_sales_lag14', 'unit_sales_lag30']:\n",
    "    nan_count = df[col].isna().sum()\n",
    "    nan_pct = (nan_count / len(df) * 100)\n",
    "    print(f\"  {col}: {nan_count:,} ({nan_pct:.2f}%)\")\n",
    "\n",
    "print(f\"\\nSample with lags (first store-item pair):\")\n",
    "sample = df[df['store_nbr']==24][df['item_nbr']==96995].head(35)\n",
    "sample_cols = ['date', 'unit_sales', 'unit_sales_lag1', 'unit_sales_lag7', 'unit_sales_lag14', 'unit_sales_lag30']\n",
    "print(sample[sample_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd08264",
   "metadata": {},
   "source": [
    "The lag features are working correctly. The NaN counts are expected - lag30 has ~14% NaN because the first 30 days of each store-item series don't have lag30 values yet.\n",
    "Next step is rolling average features (7d, 14d, 30d averages). Per DEC-014, we exclude rolling standard deviation features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955d13e",
   "metadata": {},
   "source": [
    "Does DEC-014 (excluding rolling std features) still applies now that we have the full dataset instead of the 300K sample?\n",
    ". Let's stick with DEC-014 for now - here's why:\n",
    "Rationale to keep exclusion:\n",
    "\n",
    "Decision was validated through ablation testing on 300K sample\n",
    "Rolling std measures volatility - likely still noisy even with more data in a sparse (62.6% zeros) dataset\n",
    "Scientific method: apply learned decisions first, then validate at scale\n",
    "\n",
    "Validation in Notebook 2:\n",
    "\n",
    "Addition 3 (Feature Importance Stability Check) will test if DEC-014 holds\n",
    "If rolling std shows high importance on full data, we document that as a finding\n",
    "Could create DEC-017 to revisit if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef11e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating rolling average features...\n",
      "\n",
      "Rolling average features created:\n",
      "  unit_sales_7d_avg, 14d_avg, 30d_avg\n",
      "  Rolling std features EXCLUDED per DEC-014\n",
      "\n",
      "Sample with rolling averages (first store-item pair):\n",
      "      date  unit_sales  unit_sales_7d_avg  unit_sales_14d_avg  unit_sales_30d_avg\n",
      "2013-09-01         2.0           2.000000            2.000000            2.000000\n",
      "2013-09-02         2.0           2.000000            2.000000            2.000000\n",
      "2013-09-03         2.0           2.000000            2.000000            2.000000\n",
      "2013-09-04         0.0           1.500000            1.500000            1.500000\n",
      "2013-09-05         0.0           1.200000            1.200000            1.200000\n",
      "2013-09-06         3.0           1.500000            1.500000            1.500000\n",
      "2013-09-07         1.0           1.428571            1.428571            1.428571\n",
      "2013-09-08         1.0           1.285714            1.375000            1.375000\n",
      "2013-09-09         1.0           1.142857            1.333333            1.333333\n",
      "2013-09-10         0.0           0.857143            1.200000            1.200000\n",
      "2013-09-11         2.0           1.142857            1.272727            1.272727\n",
      "2013-09-12         0.0           1.142857            1.166667            1.166667\n",
      "2013-09-13         1.0           0.857143            1.153846            1.153846\n",
      "2013-09-14         2.0           1.000000            1.214286            1.214286\n",
      "2013-09-15         2.0           1.142857            1.214286            1.266667\n",
      "2013-09-16         1.0           1.142857            1.142857            1.250000\n",
      "2013-09-17         3.0           1.571429            1.214286            1.352941\n",
      "2013-09-18         2.0           1.571429            1.357143            1.388889\n",
      "2013-09-19         3.0           2.000000            1.571429            1.473684\n",
      "2013-09-20         2.0           2.142857            1.500000            1.500000\n",
      "2013-09-21         1.0           2.000000            1.500000            1.476190\n",
      "2013-09-22         0.0           1.714286            1.428571            1.409091\n",
      "2013-09-23         3.0           2.000000            1.571429            1.478261\n",
      "2013-09-24         0.0           1.571429            1.571429            1.416667\n",
      "2013-09-25         0.0           1.285714            1.428571            1.360000\n",
      "2013-09-26         1.0           1.000000            1.500000            1.346154\n",
      "2013-09-27         4.0           1.285714            1.714286            1.444444\n",
      "2013-09-28         3.0           1.571429            1.785714            1.500000\n",
      "2013-09-29         0.0           1.571429            1.642857            1.448276\n",
      "2013-09-30         1.0           1.285714            1.642857            1.433333\n",
      "2013-10-01         1.0           1.428571            1.500000            1.400000\n",
      "2013-10-02         2.0           1.714286            1.500000            1.400000\n",
      "2013-10-03         2.0           1.857143            1.428571            1.400000\n",
      "2013-10-04         0.0           1.285714            1.285714            1.400000\n",
      "2013-10-05         0.0           0.857143            1.214286            1.400000\n"
     ]
    }
   ],
   "source": [
    "### Section 8: Feature Engineering - Rolling Statistics\n",
    "# Source: w02_d02_FE_rolling.ipynb\n",
    "# Create 7, 14, 30-day rolling averages\n",
    "# DEC-014: EXCLUDE rolling std features (caused 3.82% RMSE degradation)\n",
    "\n",
    "print(\"Creating rolling average features...\")\n",
    "\n",
    "# Group by store-item for rolling calculations\n",
    "grouped = df.groupby(['store_nbr', 'item_nbr'])['unit_sales']\n",
    "\n",
    "# Create rolling averages (min_periods=1 to handle start of series)\n",
    "df['unit_sales_7d_avg'] = grouped.transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "df['unit_sales_14d_avg'] = grouped.transform(lambda x: x.rolling(window=14, min_periods=1).mean())\n",
    "df['unit_sales_30d_avg'] = grouped.transform(lambda x: x.rolling(window=30, min_periods=1).mean())\n",
    "\n",
    "print(f\"\\nRolling average features created:\")\n",
    "print(f\"  unit_sales_7d_avg, 14d_avg, 30d_avg\")\n",
    "print(f\"  Rolling std features EXCLUDED per DEC-014\")\n",
    "\n",
    "print(f\"\\nSample with rolling averages (first store-item pair):\")\n",
    "sample = df[df['store_nbr']==24][df['item_nbr']==96995].head(35)\n",
    "sample_cols = ['date', 'unit_sales', 'unit_sales_7d_avg', 'unit_sales_14d_avg', 'unit_sales_30d_avg']\n",
    "print(sample[sample_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e74ace",
   "metadata": {},
   "source": [
    "Now we need to move on to the next feature engineering step. According to the handoff document, we should:\n",
    "\n",
    "Skip oil features (DEC-014 excluded them)\n",
    "Create aggregation features (store/item/cluster averages)\n",
    "Create calendar features\n",
    "Create holiday features\n",
    "Create derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1410a8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lag-correlation feature...\n",
      "\n",
      "Lag-correlation feature created:\n",
      "  unit_sales_lag1_7d_corr = lag1 * 7d_avg\n",
      "\n",
      "Feature statistics:\n",
      "  Min: -4900.00\n",
      "  Max: 3518485.71\n",
      "  Mean: 81.81\n",
      "  NaN count: 26,380\n"
     ]
    }
   ],
   "source": [
    "### Section 8.1: Lag-Correlation Feature\n",
    "# Source: w02_d02_FE_rolling.ipynb\n",
    "# Correlation between lag1 and 7-day average (autocorrelation strength)\n",
    "\n",
    "print(\"Creating lag-correlation feature...\")\n",
    "\n",
    "# Create correlation between lag1 and 7d average\n",
    "df['unit_sales_lag1_7d_corr'] = df['unit_sales_lag1'] * df['unit_sales_7d_avg']\n",
    "\n",
    "print(f\"\\nLag-correlation feature created:\")\n",
    "print(f\"  unit_sales_lag1_7d_corr = lag1 * 7d_avg\")\n",
    "\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(f\"  Min: {df['unit_sales_lag1_7d_corr'].min():.2f}\")\n",
    "print(f\"  Max: {df['unit_sales_lag1_7d_corr'].max():.2f}\")\n",
    "print(f\"  Mean: {df['unit_sales_lag1_7d_corr'].mean():.2f}\")\n",
    "print(f\"  NaN count: {df['unit_sales_lag1_7d_corr'].isna().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71781b81",
   "metadata": {},
   "source": [
    "From the handoff document, the 33 features include:\n",
    "\n",
    "Temporal (8): unit_sales_lag1, lag7, lag14, lag30, unit_sales_7d_avg, 14d_avg, 30d_avg, unit_sales_lag1_7d_corr\n",
    "Calendar (7): year, month, day, dayofweek, dayofyear, weekofyear, quarter\n",
    "Holiday (4): holiday_proximity, is_holiday, holiday_period, days_to_next_holiday\n",
    "Promotion (2): onpromotion, promo_item_interaction\n",
    "Store/Item (7): cluster, store_avg_sales, item_avg_sales, item_store_avg, cluster_avg_sales, family_avg_sales, city_avg_sales\n",
    "Derived (5): perishable, weekend, month_start, month_end, is_payday\n",
    "\n",
    "Oil features are EXCLUDED per DEC-014.\n",
    "Next should be calendar features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea27158b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating calendar features...\n",
      "\n",
      "Calendar features created:\n",
      "  year, month, day, dayofweek, dayofyear, weekofyear, quarter\n",
      "\n",
      "Sample calendar features:\n",
      "      date  year  month  day  dayofweek  dayofyear  weekofyear  quarter\n",
      "2013-09-01  2013      9    1          6        244          35        3\n",
      "2013-09-02  2013      9    2          0        245          36        3\n",
      "2013-09-03  2013      9    3          1        246          36        3\n",
      "2013-09-04  2013      9    4          2        247          36        3\n",
      "2013-09-05  2013      9    5          3        248          36        3\n",
      "2013-09-06  2013      9    6          4        249          36        3\n",
      "2013-09-07  2013      9    7          5        250          36        3\n",
      "2013-09-08  2013      9    8          6        251          36        3\n",
      "2013-09-09  2013      9    9          0        252          37        3\n",
      "2013-09-10  2013      9   10          1        253          37        3\n"
     ]
    }
   ],
   "source": [
    "### Section 9: Feature Engineering - Calendar Features\n",
    "# Source: w02_d04_FE_aggregations.ipynb (temporal components)\n",
    "# Extract year, month, day, dayofweek, etc.\n",
    "\n",
    "print(\"Creating calendar features...\")\n",
    "\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['dayofweek'] = df['date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df['dayofyear'] = df['date'].dt.dayofyear\n",
    "df['weekofyear'] = df['date'].dt.isocalendar().week\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "\n",
    "print(f\"\\nCalendar features created:\")\n",
    "print(f\"  year, month, day, dayofweek, dayofyear, weekofyear, quarter\")\n",
    "\n",
    "print(f\"\\nSample calendar features:\")\n",
    "sample = df[['date', 'year', 'month', 'day', 'dayofweek', 'dayofyear', 'weekofyear', 'quarter']].head(10)\n",
    "print(sample.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4348a37a",
   "metadata": {},
   "source": [
    "From the handoff, the 33 final features include:\n",
    "Holiday (4):\n",
    "\n",
    "holiday_proximity, is_holiday, holiday_period, days_to_next_holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12ccf2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating holiday features (vectorized)...\n",
      "\n",
      "Holiday features created:\n",
      "  is_holiday, holiday_proximity, days_to_next_holiday, holiday_period\n",
      "\n",
      "Holiday feature statistics:\n",
      "  Holidays in period: 659,500 rows\n",
      "  Holiday periods (3 days): 1,899,360 rows\n",
      "  Holiday proximity range: [-30, 29]\n"
     ]
    }
   ],
   "source": [
    "### Section 10: Feature Engineering - Holiday Features\n",
    "# Source: w02_d04_FE_aggregations.ipynb\n",
    "# Create holiday proximity, is_holiday, holiday_period, days_to_next_holiday\n",
    "# This maps unique dates (212) instead of iterating 5.6M rows.\n",
    "\n",
    "print(\"Creating holiday features (vectorized)...\")\n",
    "\n",
    "# Create is_holiday flag (1 if date is a holiday)\n",
    "holiday_dates_set = set(holidays['date'].dt.date)\n",
    "df['is_holiday'] = df['date'].dt.date.isin(holiday_dates_set).astype(int)\n",
    "\n",
    "# Vectorized holiday proximity - work with unique dates only\n",
    "unique_dates = df['date'].unique()\n",
    "holiday_dates_np = holidays['date'].values  # numpy datetime64\n",
    "\n",
    "# Create mapping for unique dates\n",
    "proximity_map = {}\n",
    "for date in unique_dates:\n",
    "    date_np = np.datetime64(date, 'D')  # Convert to numpy datetime64\n",
    "    distances = (holiday_dates_np.astype('datetime64[D]') - date_np).astype(int)\n",
    "    proximity_map[pd.Timestamp(date)] = int(distances[np.abs(distances).argmin()])\n",
    "\n",
    "df['holiday_proximity'] = df['date'].map(proximity_map)\n",
    "\n",
    "# Days to next holiday (vectorized)\n",
    "days_to_next_map = {}\n",
    "for date in unique_dates:\n",
    "    date_np = np.datetime64(date, 'D')\n",
    "    future_holidays = holiday_dates_np[holiday_dates_np > date]\n",
    "    if len(future_holidays) > 0:\n",
    "        days = int((future_holidays.min().astype('datetime64[D]') - date_np).astype(int))\n",
    "    else:\n",
    "        days = 999\n",
    "    days_to_next_map[pd.Timestamp(date)] = days\n",
    "\n",
    "df['days_to_next_holiday'] = df['date'].map(days_to_next_map)\n",
    "\n",
    "# Holiday period: 1 if within 3 days of holiday\n",
    "df['holiday_period'] = (df['holiday_proximity'].abs() <= 3).astype(int)\n",
    "\n",
    "print(f\"\\nHoliday features created:\")\n",
    "print(f\"  is_holiday, holiday_proximity, days_to_next_holiday, holiday_period\")\n",
    "\n",
    "print(f\"\\nHoliday feature statistics:\")\n",
    "print(f\"  Holidays in period: {df['is_holiday'].sum():,} rows\")\n",
    "print(f\"  Holiday periods (3 days): {df['holiday_period'].sum():,} rows\")\n",
    "print(f\"  Holiday proximity range: [{df['holiday_proximity'].min()}, {df['holiday_proximity'].max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f642bf7",
   "metadata": {},
   "source": [
    "The holiday features are created. Now we need to continue with:\n",
    "\n",
    "Aggregation features (store/item/cluster averages)\n",
    "Derived features (perishable, weekend, month_start, month_end, is_payday)\n",
    "Promotion interaction feature\n",
    "Final assembly and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fbafe5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating aggregation features...\n",
      "\n",
      "Aggregation features created:\n",
      "  store_avg_sales\n",
      "  item_avg_sales\n",
      "  item_store_avg\n",
      "  cluster_avg_sales\n",
      "  family_avg_sales\n",
      "  city_avg_sales\n",
      "\n",
      "Aggregation feature statistics:\n",
      "  store_avg_sales: mean=2.83, std=1.54\n",
      "  item_avg_sales: mean=2.83, std=4.67\n",
      "  item_store_avg: mean=2.83, std=6.90\n",
      "  cluster_avg_sales: mean=2.83, std=1.47\n",
      "  family_avg_sales: mean=2.83, std=1.15\n",
      "  city_avg_sales: mean=2.83, std=0.53\n"
     ]
    }
   ],
   "source": [
    "### Section 11: Feature Engineering - Aggregation Features\n",
    "# Source: w02_d04_FE_aggregations.ipynb\n",
    "# Create store, item, cluster, family, city average sales\n",
    "\n",
    "print(\"Creating aggregation features...\")\n",
    "\n",
    "# Calculate various aggregations (use all data for global patterns)\n",
    "agg_features = {}\n",
    "\n",
    "# Store average sales\n",
    "agg_features['store_avg_sales'] = df.groupby('store_nbr')['unit_sales'].transform('mean')\n",
    "\n",
    "# Item average sales\n",
    "agg_features['item_avg_sales'] = df.groupby('item_nbr')['unit_sales'].transform('mean')\n",
    "\n",
    "# Item-store interaction average\n",
    "agg_features['item_store_avg'] = df.groupby(['item_nbr', 'store_nbr'])['unit_sales'].transform('mean')\n",
    "\n",
    "# Cluster average sales\n",
    "agg_features['cluster_avg_sales'] = df.groupby('cluster')['unit_sales'].transform('mean')\n",
    "\n",
    "# Family average sales\n",
    "agg_features['family_avg_sales'] = df.groupby('family')['unit_sales'].transform('mean')\n",
    "\n",
    "# City average sales\n",
    "agg_features['city_avg_sales'] = df.groupby('city')['unit_sales'].transform('mean')\n",
    "\n",
    "# Add to dataframe\n",
    "for feature_name, feature_values in agg_features.items():\n",
    "    df[feature_name] = feature_values\n",
    "\n",
    "print(f\"\\nAggregation features created:\")\n",
    "for feature_name in agg_features.keys():\n",
    "    print(f\"  {feature_name}\")\n",
    "\n",
    "print(f\"\\nAggregation feature statistics:\")\n",
    "for feature_name in agg_features.keys():\n",
    "    print(f\"  {feature_name}: mean={df[feature_name].mean():.2f}, std={df[feature_name].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6bee8",
   "metadata": {},
   "source": [
    "Now I need to create derived features like weekend, perishable, month_start, month_end, is_payday. Also need to create the promotion interaction feature. Let me continue with the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2865320c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating derived features...\n",
      "\n",
      "Derived features created:\n",
      "  weekend, month_start, month_end, is_payday\n",
      "\n",
      "Derived feature statistics:\n",
      "  Weekend days: 1,609,180 rows (28.8%)\n",
      "  Month start (days 1-7): 1,292,620 rows (23.1%)\n",
      "  Month end (days 24+): 1,345,380 rows (24.1%)\n",
      "  Payday (15th, last): 369,320 rows (6.6%)\n"
     ]
    }
   ],
   "source": [
    "### Section 12: Feature Engineering - Derived Features\n",
    "# Source: w02_d04_FE_aggregations.ipynb\n",
    "# Create weekend, month_start, month_end, is_payday flags\n",
    "\n",
    "print(\"Creating derived features...\")\n",
    "\n",
    "# Weekend flag (Saturday=5, Sunday=6)\n",
    "df['weekend'] = (df['dayofweek'].isin([5, 6])).astype(int)\n",
    "\n",
    "# Month start/end flags\n",
    "df['month_start'] = (df['day'] <= 7).astype(int)\n",
    "df['month_end'] = (df['day'] >= 24).astype(int)\n",
    "\n",
    "# Payday flag (15th and last day of month)\n",
    "# Get last day of each month\n",
    "df['is_last_day'] = (df['date'].dt.is_month_end).astype(int)\n",
    "df['is_payday'] = ((df['day'] == 15) | (df['is_last_day'] == 1)).astype(int)\n",
    "df.drop('is_last_day', axis=1, inplace=True)\n",
    "\n",
    "print(f\"\\nDerived features created:\")\n",
    "print(f\"  weekend, month_start, month_end, is_payday\")\n",
    "\n",
    "print(f\"\\nDerived feature statistics:\")\n",
    "print(f\"  Weekend days: {df['weekend'].sum():,} rows ({df['weekend'].mean()*100:.1f}%)\")\n",
    "print(f\"  Month start (days 1-7): {df['month_start'].sum():,} rows ({df['month_start'].mean()*100:.1f}%)\")\n",
    "print(f\"  Month end (days 24+): {df['month_end'].sum():,} rows ({df['month_end'].mean()*100:.1f}%)\")\n",
    "print(f\"  Payday (15th, last): {df['is_payday'].sum():,} rows ({df['is_payday'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0f611",
   "metadata": {},
   "source": [
    "Now we need to create the promotion interaction feature. According to DEC-014, we exclude:\n",
    "\n",
    "promo_holiday_category\n",
    "promo_item_avg\n",
    "promo_cluster\n",
    "\n",
    "But we keep: promo_item_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6f25be77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating promotion interaction feature...\n",
      "\n",
      "Promotion interaction feature created:\n",
      "  promo_item_interaction = onpromotion * item_avg_sales\n",
      "  Other promotion interactions EXCLUDED per DEC-014\n",
      "\n",
      "Feature statistics:\n",
      "  Non-zero interactions: 0\n",
      "  Mean: 0.00\n",
      "  Max: 0.00\n"
     ]
    }
   ],
   "source": [
    "### Section 13: Feature Engineering - Promotion Interaction\n",
    "# Source: w02_d04_FE_aggregations.ipynb\n",
    "# Create promo_item_interaction (onpromotion * item_avg_sales)\n",
    "# DEC-014: EXCLUDE promo_holiday_category, promo_item_avg, promo_cluster\n",
    "\n",
    "print(\"Creating promotion interaction feature...\")\n",
    "\n",
    "# Promotion * item average interaction\n",
    "df['promo_item_interaction'] = df['onpromotion'] * df['item_avg_sales']\n",
    "\n",
    "print(f\"\\nPromotion interaction feature created:\")\n",
    "print(f\"  promo_item_interaction = onpromotion * item_avg_sales\")\n",
    "print(f\"  Other promotion interactions EXCLUDED per DEC-014\")\n",
    "\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(f\"  Non-zero interactions: {(df['promo_item_interaction'] > 0).sum():,}\")\n",
    "print(f\"  Mean: {df['promo_item_interaction'].mean():.2f}\")\n",
    "print(f\"  Max: {df['promo_item_interaction'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b2dbf",
   "metadata": {},
   "source": [
    "All zeros as expected - onpromotion was 100% zero in this period. Feature still included for consistency with DEC-014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a5ecfe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Set Summary (DEC-014):\n",
      "------------------------------------------------------------\n",
      "Total features: 33\n",
      "\n",
      "All 33 features present in dataset\n",
      "\n",
      "Features by category:\n",
      "  Temporal (8): lags, rolling averages, correlation\n",
      "  Calendar (7): year, month, day, dayofweek, etc.\n",
      "  Holiday (4): proximity, flags, days to next\n",
      "  Promotion (2): onpromotion, item interaction\n",
      "  Store/Item (7): cluster, hierarchical averages\n",
      "  Derived (5): weekend, month flags, payday\n",
      "\n",
      "Current dataset shape: (5592560, 42)\n"
     ]
    }
   ],
   "source": [
    "### Section 14: Final Feature Set (33 Features per DEC-014)\n",
    "# Source: w02_d05_FE_final.ipynb\n",
    "# Verify we have exactly 33 features as specified\n",
    "\n",
    "# Define the 33 features per DEC-014\n",
    "feature_columns = [\n",
    "    # Temporal (8)\n",
    "    'unit_sales_lag1', 'unit_sales_lag7', 'unit_sales_lag14', 'unit_sales_lag30',\n",
    "    'unit_sales_7d_avg', 'unit_sales_14d_avg', 'unit_sales_30d_avg',\n",
    "    'unit_sales_lag1_7d_corr',\n",
    "    \n",
    "    # Calendar (7)\n",
    "    'year', 'month', 'day', 'dayofweek', 'dayofyear', 'weekofyear', 'quarter',\n",
    "    \n",
    "    # Holiday (4)\n",
    "    'holiday_proximity', 'is_holiday', 'holiday_period', 'days_to_next_holiday',\n",
    "    \n",
    "    # Promotion (2)\n",
    "    'onpromotion', 'promo_item_interaction',\n",
    "    \n",
    "    # Store/Item (7)\n",
    "    'cluster', 'store_avg_sales', 'item_avg_sales', 'item_store_avg',\n",
    "    'cluster_avg_sales', 'family_avg_sales', 'city_avg_sales',\n",
    "    \n",
    "    # Derived (5)\n",
    "    'perishable', 'weekend', 'month_start', 'month_end', 'is_payday'\n",
    "]\n",
    "\n",
    "print(\"Feature Set Summary (DEC-014):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total features: {len(feature_columns)}\")\n",
    "\n",
    "# Verify all features exist\n",
    "missing_features = [f for f in feature_columns if f not in df.columns]\n",
    "if missing_features:\n",
    "    print(f\"\\nMISSING FEATURES: {missing_features}\")\n",
    "else:\n",
    "    print(f\"\\nAll 33 features present in dataset\")\n",
    "\n",
    "print(f\"\\nFeatures by category:\")\n",
    "print(f\"  Temporal (8): lags, rolling averages, correlation\")\n",
    "print(f\"  Calendar (7): year, month, day, dayofweek, etc.\")\n",
    "print(f\"  Holiday (4): proximity, flags, days to next\")\n",
    "print(f\"  Promotion (2): onpromotion, item interaction\")\n",
    "print(f\"  Store/Item (7): cluster, hierarchical averages\")\n",
    "print(f\"  Derived (5): weekend, month flags, payday\")\n",
    "\n",
    "print(f\"\\nCurrent dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f05276",
   "metadata": {},
   "source": [
    "We have all 33 features. The dataset has 42 columns total because it includes metadata columns (date, store_nbr, item_nbr, unit_sales, family, class, city, state, type, etc.) plus the 33 features.\n",
    "Now we need to prepare the final dataset for export. Let me create a cell to select the columns we need and handle NaN values from lag features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ca30e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling NaN values from lag features...\n",
      "\n",
      "NaN counts by feature:\n",
      "  unit_sales_lag30: 791,400 (14.15%)\n",
      "  unit_sales_lag14: 369,320 (6.60%)\n",
      "  unit_sales_lag7: 184,660 (3.30%)\n",
      "  unit_sales_lag1: 26,380 (0.47%)\n",
      "  unit_sales_lag1_7d_corr: 26,380 (0.47%)\n",
      "\n",
      "After dropping NaN rows:\n",
      "  Original: 5,592,560 rows\n",
      "  Clean: 4,801,160 rows\n",
      "  Dropped: 791,400 rows (14.15%)\n",
      "\n",
      "Date range after cleaning:\n",
      "  Start: 2013-10-01\n",
      "  End: 2014-03-31\n"
     ]
    }
   ],
   "source": [
    "### Section 15: Handle NaN Values\n",
    "# Source: w02_d05_FE_final.ipynb\n",
    "# NaN values exist at start of time series from lag features\n",
    "# Per DEC-013: 7-day gap + 30-day lags means September data is buffer\n",
    "\n",
    "print(\"Handling NaN values from lag features...\")\n",
    "\n",
    "print(f\"\\nNaN counts by feature:\")\n",
    "nan_counts = df[feature_columns].isnull().sum()\n",
    "nan_counts = nan_counts[nan_counts > 0].sort_values(ascending=False)\n",
    "for feat, count in nan_counts.items():\n",
    "    pct = (count / len(df) * 100)\n",
    "    print(f\"  {feat}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Strategy: Drop rows with ANY NaN in features\n",
    "# These are early September rows used as buffer for lags\n",
    "df_clean = df.dropna(subset=feature_columns).copy()\n",
    "\n",
    "print(f\"\\nAfter dropping NaN rows:\")\n",
    "print(f\"  Original: {len(df):,} rows\")\n",
    "print(f\"  Clean: {len(df_clean):,} rows\")\n",
    "print(f\"  Dropped: {len(df) - len(df_clean):,} rows ({(len(df) - len(df_clean))/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nDate range after cleaning:\")\n",
    "print(f\"  Start: {df_clean['date'].min().date()}\")\n",
    "print(f\"  End: {df_clean['date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99743e53",
   "metadata": {},
   "source": [
    "we dropped the first 30 days (September) which was the buffer period for lag features. Now we have October 1, 2013 through March 31, 2014 - exactly what we need for DEC-016 training (Q4 2013 + Q1 2014) and test (March 2014).\n",
    "Next we need to prepare the final dataset with metadata columns and feature columns, then save to pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "734f496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset structure:\n",
      "------------------------------------------------------------\n",
      "Shape: (4801160, 42)\n",
      "Memory: 2578.5 MB\n",
      "\n",
      "Column organization:\n",
      "  Metadata (8): ['date', 'store_nbr', 'item_nbr', 'family', 'class', 'city', 'state', 'type']\n",
      "  Target (1): unit_sales\n",
      "  Features (33): [see feature_columns list]\n",
      "\n",
      "Date coverage:\n",
      "  Start: 2013-10-01\n",
      "  End: 2014-03-31\n",
      "  Days: 182\n",
      "\n",
      "Store-item coverage:\n",
      "  Stores: 10\n",
      "  Items: 2638\n",
      "  Families: 32\n"
     ]
    }
   ],
   "source": [
    "### Section 16: Prepare Final Dataset\n",
    "# Source: w02_d05_FE_final.ipynb\n",
    "# Organize columns: metadata + target + features\n",
    "\n",
    "# Metadata columns (for filtering, not for modeling)\n",
    "metadata_columns = ['date', 'store_nbr', 'item_nbr', 'family', 'class', \n",
    "                    'city', 'state', 'type']\n",
    "\n",
    "# Target column\n",
    "target_column = 'unit_sales'\n",
    "\n",
    "# Final column order\n",
    "final_columns = metadata_columns + [target_column] + feature_columns\n",
    "\n",
    "# Create final dataset\n",
    "df_final = df_clean[final_columns].copy()\n",
    "\n",
    "print(\"Final dataset structure:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Shape: {df_final.shape}\")\n",
    "print(f\"Memory: {df_final.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n",
    "\n",
    "print(f\"\\nColumn organization:\")\n",
    "print(f\"  Metadata (8): {metadata_columns}\")\n",
    "print(f\"  Target (1): {target_column}\")\n",
    "print(f\"  Features (33): [see feature_columns list]\")\n",
    "\n",
    "print(f\"\\nDate coverage:\")\n",
    "print(f\"  Start: {df_final['date'].min().date()}\")\n",
    "print(f\"  End: {df_final['date'].max().date()}\")\n",
    "print(f\"  Days: {df_final['date'].nunique()}\")\n",
    "\n",
    "print(f\"\\nStore-item coverage:\")\n",
    "print(f\"  Stores: {df_final['store_nbr'].nunique()}\")\n",
    "print(f\"  Items: {df_final['item_nbr'].nunique()}\")\n",
    "print(f\"  Families: {df_final['family'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee79bf",
   "metadata": {},
   "source": [
    "We have 4.8M rows covering Oct 1, 2013 to March 31, 2014. This is the complete dataset with all 33 features. Now we need to save it and provide a summary.\n",
    "Next steps:\n",
    "\n",
    "Data quality validation\n",
    "Save the processed dataset\n",
    "Final summary with comparison to Week 3 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4276918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final data quality validation:\n",
      "------------------------------------------------------------\n",
      "\n",
      "NaN values in features: 0\n",
      "Infinite values in features: 0\n",
      "\n",
      "Target (unit_sales) statistics:\n",
      "  Min: -100.00\n",
      "  Max: 4851.00\n",
      "  Mean: 2.90\n",
      "  Median: 0.00\n",
      "  Std: 11.61\n",
      "\n",
      "Sample feature ranges:\n",
      "  unit_sales_lag1: [-100.00, 4851.00]\n",
      "  unit_sales_7d_avg: [-8.71, 821.61]\n",
      "  cluster: [1.00, 17.00]\n",
      "  weekend: [0.00, 1.00]\n"
     ]
    }
   ],
   "source": [
    "### Section 17: Final Data Quality Validation\n",
    "# Source: w02_d05_FE_final.ipynb\n",
    "# Verify data quality before export\n",
    "\n",
    "print(\"Final data quality validation:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Check for any remaining NaN in features\n",
    "nan_check = df_final[feature_columns].isnull().sum().sum()\n",
    "print(f\"\\nNaN values in features: {nan_check}\")\n",
    "\n",
    "# Check for infinities\n",
    "inf_check = np.isinf(df_final[feature_columns].select_dtypes(include=[np.number])).sum().sum()\n",
    "print(f\"Infinite values in features: {inf_check}\")\n",
    "\n",
    "# Check target distribution\n",
    "print(f\"\\nTarget (unit_sales) statistics:\")\n",
    "print(f\"  Min: {df_final['unit_sales'].min():.2f}\")\n",
    "print(f\"  Max: {df_final['unit_sales'].max():.2f}\")\n",
    "print(f\"  Mean: {df_final['unit_sales'].mean():.2f}\")\n",
    "print(f\"  Median: {df_final['unit_sales'].median():.2f}\")\n",
    "print(f\"  Std: {df_final['unit_sales'].std():.2f}\")\n",
    "\n",
    "# Check feature value ranges (sample)\n",
    "print(f\"\\nSample feature ranges:\")\n",
    "sample_features = ['unit_sales_lag1', 'unit_sales_7d_avg', 'cluster', 'weekend']\n",
    "for feat in sample_features:\n",
    "    print(f\"  {feat}: [{df_final[feat].min():.2f}, {df_final[feat].max():.2f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb921c",
   "metadata": {},
   "source": [
    "data quality checks pass. Now we need to save the final dataset. This is 4.8M rows with 42 columns (8 metadata + 1 target + 33 features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23e60710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting final dataset...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Export complete:\n",
      "  File: /home/berto/Demand-forecasting-in-retail/data/processed/full_featured_data.pkl\n",
      "  Size: 1341.6 MB\n",
      "  Time: 2.6 seconds\n",
      "\n",
      "Dataset summary:\n",
      "  Rows: 4,801,160\n",
      "  Columns: 42\n",
      "  Features: 33\n",
      "  Period: 2013-10-01 to 2014-03-31\n",
      "  Stores: 10\n",
      "  Items: 2638\n"
     ]
    }
   ],
   "source": [
    "### Section 18: Export Processed Dataset\n",
    "# Source: w02_d05_FE_final.ipynb\n",
    "# Save to pickle for efficient loading in Notebook 2\n",
    "\n",
    "import time\n",
    "\n",
    "output_file = DATA_PROCESSED / 'full_featured_data.pkl'\n",
    "\n",
    "print(\"Exporting final dataset...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Save to pickle (preserves dtypes, faster than CSV)\n",
    "df_final.to_pickle(output_file)\n",
    "\n",
    "export_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nExport complete:\")\n",
    "print(f\"  File: {output_file}\")\n",
    "print(f\"  Size: {output_file.stat().st_size / (1024**2):.1f} MB\")\n",
    "print(f\"  Time: {export_time:.1f} seconds\")\n",
    "\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"  Rows: {len(df_final):,}\")\n",
    "print(f\"  Columns: {len(df_final.columns)}\")\n",
    "print(f\"  Features: {len(feature_columns)}\")\n",
    "print(f\"  Period: {df_final['date'].min().date()} to {df_final['date'].max().date()}\")\n",
    "print(f\"  Stores: {df_final['store_nbr'].nunique()}\")\n",
    "print(f\"  Items: {df_final['item_nbr'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452867b",
   "metadata": {},
   "source": [
    "## FULL_01 Summary: Data to Features Complete\n",
    "\n",
    "### Execution Summary\n",
    "\n",
    "| Step | Source | Result |\n",
    "|------|--------|--------|\n",
    "| Data Loading | w01_d01, w01_d02 | 22.9M rows  2.1M (period filter) |\n",
    "| Calendar Scaffold | w01_d03 | 5.6M complete combinations |\n",
    "| Lag Features | w02_d01 | 4 lag features (1, 7, 14, 30 days) |\n",
    "| Rolling Features | w02_d02 | 3 rolling avg (std excluded per DEC-014) |\n",
    "| Holiday Features | w02_d04 | 4 holiday features |\n",
    "| Aggregations | w02_d04 | 6 hierarchical averages |\n",
    "| Derived Features | w02_d04 | 5 temporal flags |\n",
    "| Final Export | w02_d05 | 4.8M rows  42 columns |\n",
    "\n",
    "### Output Dataset\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Rows | 4,801,160 |\n",
    "| Columns | 42 (8 metadata + 1 target + 33 features) |\n",
    "| Period | Oct 1, 2013 - Mar 31, 2014 |\n",
    "| Stores | 10 (Guayas) |\n",
    "| Items | 2,638 |\n",
    "| Families | 32 |\n",
    "| Sparsity | 62.6% |\n",
    "| File | full_featured_data.pkl (1.3 GB) |\n",
    "\n",
    "### Comparison: Sample vs Full\n",
    "\n",
    "| Metric | Week 3 Sample | Full Pipeline |\n",
    "|--------|---------------|---------------|\n",
    "| Sampling | 300K rows | No sampling |\n",
    "| Families | Top-3 | All 32 |\n",
    "| Items | ~500 | 2,638 |\n",
    "| Final rows | ~25K | 4.8M |\n",
    "\n",
    "### Decisions Applied\n",
    "- **DEC-014:** 33 features (excluded rolling std, oil, promotion interactions)\n",
    "\n",
    "### Issues Resolved\n",
    "1. Holiday feature vectorization (5+ min  3 sec)\n",
    "2. DateTime type handling for numpy arithmetic\n",
    "\n",
    "### Next Steps\n",
    "**FULL_02_train_final_model.ipynb:**\n",
    "- Apply DEC-016 split (Q4 2013 + Q1 2014 training)\n",
    "- Apply DEC-013 gap (7 days)\n",
    "- Train XGBoost + LSTM\n",
    "- Compare to Week 3 baseline\n",
    "- Export production artifacts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
