{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b027474c",
   "metadata": {},
   "source": [
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "**w03_d03_MODEL_tuning.ipynb**\n",
    "\n",
    "**Author:** Alberto Diaz Durana  \n",
    "**Date:** November 2025  \n",
    "**Purpose:** Feature optimization and hyperparameter tuning\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook accomplishes the following:\n",
    "\n",
    "- Implement DEC-014: Reduce feature set from 45 to 30 features\n",
    "- Create new baseline with optimized 30-feature set\n",
    "- Validate 5-7% RMSE improvement hypothesis (7.21 → 6.70-6.85)\n",
    "- Hyperparameter tuning with RandomizedSearchCV (n_iter=20)\n",
    "- Log all experiments to MLflow for comparison\n",
    "- Select best model configuration for Week 4 deployment\n",
    "\n",
    "---\n",
    "\n",
    "## Business Context\n",
    "\n",
    "**Why feature reduction matters:**\n",
    "\n",
    "- Simpler models are easier to explain to stakeholders\n",
    "- Fewer features mean faster predictions and lower maintenance\n",
    "- Overfitting prevention improves real-world performance\n",
    "- Demonstrates proper ML validation methodology (test assumptions)\n",
    "- Portfolio piece shows rigorous feature selection process\n",
    "\n",
    "**Day 2 critical finding:**\n",
    "- Ablation studies revealed 15 features harm performance\n",
    "- Removing rolling_std, oil, and promotion interactions improves RMSE by 5-7%\n",
    "- DEC-012 (oil features) invalidated by proper validation\n",
    "\n",
    "**Expected outcomes:**\n",
    "- 30-feature baseline: RMSE ~6.70-6.85 (5-7% improvement)\n",
    "- Tuned model: RMSE ~6.40-6.60 (additional 5-10% improvement)\n",
    "- Total improvement: 10-15% over original 45-feature model\n",
    "\n",
    "---\n",
    "\n",
    "## Input Dependencies\n",
    "\n",
    "From Week 3 Day 2:\n",
    "- DEC-014: List of 15 features to remove\n",
    "- Ablation study results (rolling_std: -3.82%, oil: -3.14%, promotion: 0%)\n",
    "- MLflow experiment setup (\"favorita-forecasting\")\n",
    "- Baseline RMSE: 7.21 (45 features)\n",
    "\n",
    "From Week 2:\n",
    "- Feature-engineered dataset: w02_d05_FE_final.pkl (300,896 × 57 columns)\n",
    "- Train/test split strategy: 7-day gap (DEC-013)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b779e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# XGBoost and evaluation\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "\n",
    "# MLflow tracking\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Print library versions for reproducibility\n",
    "print(\"Library versions:\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"xgboost: {xgb.__version__}\")\n",
    "import sklearn\n",
    "print(f\"scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"mlflow: {mlflow.__version__}\")\n",
    "\n",
    "print(f\"\\nDay 2 baseline to improve upon:\")\n",
    "print(f\"  45 features: RMSE = 7.2127\")\n",
    "print(f\"  Target (30 features): RMSE = 6.70-6.85\")\n",
    "print(f\"  Expected improvement: 5-7%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64798382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine paths (works from notebooks/ or project root)\n",
    "current_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "PROJECT_ROOT = current_dir.parent if current_dir.name == 'notebooks' else current_dir\n",
    "\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed'\n",
    "DATA_RESULTS = PROJECT_ROOT / 'data' / 'results' / 'models'\n",
    "OUTPUTS_FIGURES = PROJECT_ROOT / 'outputs' / 'figures' / 'models'\n",
    "OUTPUTS_DOCUMENTS = PROJECT_ROOT / 'docs'\n",
    "\n",
    "print(f\"\\nProject root: {PROJECT_ROOT.resolve()}\")\n",
    "print(f\"Data processed: {DATA_PROCESSED.resolve()}\")\n",
    "print(f\"Results output: {DATA_RESULTS.resolve()}\")\n",
    "print(f\"Figures output: {OUTPUTS_FIGURES.resolve()}\")\n",
    "print(f\"Documents output: {OUTPUTS_DOCUMENTS.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05d7cb0",
   "metadata": {},
   "source": [
    "## 1. Load Data and Create Train/Test Split\n",
    "\n",
    "**Objective:** Reload feature-engineered dataset and apply 7-day gap split (DEC-013)\n",
    "\n",
    "**Activities:**\n",
    "- Load w02_d05_FE_final.pkl\n",
    "- Filter to Q1 2014 (Jan 1 - Mar 31)\n",
    "- Apply 7-day gap split: Train (Jan 1 - Feb 21), Gap (Feb 22-28), Test (Mar 1-31)\n",
    "- Prepare feature matrices (45 features initially)\n",
    "\n",
    "**Expected output:** \n",
    "- Train samples: ~7,050\n",
    "- Test samples: ~4,200\n",
    "- Features: 45 (before reduction to 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18446d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature-engineered dataset\n",
    "print(\"Loading feature-engineered dataset...\")\n",
    "df = pd.read_pickle(DATA_PROCESSED / 'w02_d05_FE_final.pkl')\n",
    "\n",
    "print(f\"Full dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Columns: {df.shape[1]}\")\n",
    "\n",
    "# Filter to Q1 2014\n",
    "df_2014q1 = df[(df['date'] >= '2014-01-01') & (df['date'] <= '2014-03-31')].copy()\n",
    "print(f\"\\nQ1 2014 shape: {df_2014q1.shape}\")\n",
    "print(f\"Date range: {df_2014q1['date'].min()} to {df_2014q1['date'].max()}\")\n",
    "\n",
    "# Apply 7-day gap split (DEC-013)\n",
    "# Train: Jan 1 - Feb 21 (52 days)\n",
    "# Gap: Feb 22 - Feb 28 (7 days, excluded)\n",
    "# Test: Mar 1 - Mar 31 (31 days)\n",
    "\n",
    "train = df_2014q1[df_2014q1['date'] <= '2014-02-21'].copy()\n",
    "test = df_2014q1[df_2014q1['date'] >= '2014-03-01'].copy()\n",
    "\n",
    "print(f\"\\nTrain-Test Split (DEC-013: 7-day gap):\")\n",
    "print(f\"  Train: {train['date'].min()} to {train['date'].max()} ({len(train)} samples)\")\n",
    "print(f\"  Gap: 2014-02-22 to 2014-02-28 (excluded from both sets)\")\n",
    "print(f\"  Test: {test['date'].min()} to {test['date'].max()} ({len(test)} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (exclude non-features)\n",
    "exclude_cols = ['id', 'date', 'store_nbr', 'item_nbr', 'unit_sales', \n",
    "                'city', 'state', 'type', 'family', 'class',\n",
    "                'holiday_name', 'holiday_type']\n",
    "\n",
    "feature_cols_all = [col for col in train.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Total features (before reduction): {len(feature_cols_all)}\")\n",
    "print(f\"\\nFeature columns:\")\n",
    "for i, col in enumerate(feature_cols_all, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c301f9",
   "metadata": {},
   "source": [
    "## 2. Implement DEC-014: Feature Reduction\n",
    "\n",
    "**Objective:** Remove 15 harmful features identified in Day 2 ablation studies\n",
    "\n",
    "**Features to remove (DEC-014):**\n",
    "- Rolling std features (3): unit_sales_7d_std, unit_sales_14d_std, unit_sales_30d_std\n",
    "- Oil features (6): oil_price, oil_price_lag7, oil_price_lag14, oil_price_lag30, oil_price_change7, oil_price_change14\n",
    "- Promotion interactions (3): promo_holiday_category, promo_item_avg_interaction, promo_cluster_interaction\n",
    "\n",
    "**Rationale:** Ablation studies showed removing these improves RMSE by 5-7%\n",
    "\n",
    "**Expected result:** 45 → 30 features (approximately, based on actual features present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to remove (DEC-014)\n",
    "features_to_remove = [\n",
    "    # Rolling std (3 present in dataset)\n",
    "    'unit_sales_7d_std', \n",
    "    'unit_sales_14d_std', \n",
    "    'unit_sales_30d_std',\n",
    "    \n",
    "    # Oil features (6)\n",
    "    'oil_price', \n",
    "    'oil_price_lag7', \n",
    "    'oil_price_lag14', \n",
    "    'oil_price_lag30', \n",
    "    'oil_price_change7', \n",
    "    'oil_price_change14',\n",
    "    \n",
    "    # Promotion interactions (3)\n",
    "    'promo_holiday_category', \n",
    "    'promo_item_avg_interaction', \n",
    "    'promo_cluster_interaction'\n",
    "]\n",
    "\n",
    "# Create optimized feature set\n",
    "feature_cols_optimized = [col for col in feature_cols_all if col not in features_to_remove]\n",
    "\n",
    "print(\"DEC-014: Feature Reduction\")\n",
    "print(f\"  Original features: {len(feature_cols_all)}\")\n",
    "print(f\"  Features to remove: {len(features_to_remove)}\")\n",
    "print(f\"  Optimized features: {len(feature_cols_optimized)}\")\n",
    "print(f\"  Reduction: {len(feature_cols_all) - len(feature_cols_optimized)} features\")\n",
    "\n",
    "print(f\"\\nFeatures removed:\")\n",
    "for i, feat in enumerate(features_to_remove, 1):\n",
    "    if feat in feature_cols_all:\n",
    "        print(f\"  {i:2d}. {feat}\")\n",
    "    else:\n",
    "        print(f\"  {i:2d}. {feat} (NOT FOUND - skipped)\")\n",
    "\n",
    "print(f\"\\nOptimized feature set ({len(feature_cols_optimized)} features):\")\n",
    "for i, col in enumerate(feature_cols_optimized, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad0c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dtype info for feature_cols_optimized\n",
    "print(\"\\nData types of optimized features:\\n\")\n",
    "train[feature_cols_optimized].info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9928bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix categorical dtypes\n",
    "categorical_cols = ['holiday_period']\n",
    "for col in categorical_cols:\n",
    "    if col in feature_cols_optimized:\n",
    "        train[col] = train[col].astype('category')\n",
    "        test[col] = test[col].astype('category')\n",
    "        print(f\"Converted {col} to category dtype\")\n",
    "\n",
    "# Create feature matrices\n",
    "X_train = train[feature_cols_optimized].copy()\n",
    "y_train = train['unit_sales'].copy()\n",
    "X_test = test[feature_cols_optimized].copy()\n",
    "y_test = test['unit_sales'].copy()\n",
    "\n",
    "print(f\"\\nFeature matrices created:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nTarget variable statistics:\")\n",
    "print(f\"  Train mean: {y_train.mean():.2f}\")\n",
    "print(f\"  Train std: {y_train.std():.2f}\")\n",
    "print(f\"  Test mean: {y_test.mean():.2f}\")\n",
    "print(f\"  Test std: {y_test.std():.2f}\")\n",
    "\n",
    "print(f\"\\nMissing values check:\")\n",
    "print(f\"  X_train NaN: {X_train.isna().sum().sum()}\")\n",
    "print(f\"  X_test NaN: {X_test.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6517217",
   "metadata": {},
   "source": [
    "## 3. Train 33-Feature Baseline Model\n",
    "\n",
    "**Objective:** Train XGBoost with optimized feature set and log to MLflow\n",
    "\n",
    "**MLflow logging strategy:**\n",
    "- Run name: \"xgboost_baseline_33features\"\n",
    "- Log params: n_features, model hyperparameters\n",
    "- Log metrics: RMSE, MAE, Bias, improvement vs 45-feature baseline\n",
    "- Log artifact: Evaluation plot\n",
    "\n",
    "**Expected outcome:** RMSE ~6.70-6.85 (5-7% improvement over 7.21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MLflow experiment\n",
    "mlflow.set_experiment(\"favorita-forecasting\")\n",
    "\n",
    "print(\"MLflow experiment set: favorita-forecasting\")\n",
    "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# Get experiment info\n",
    "experiment = mlflow.get_experiment_by_name(\"favorita-forecasting\")\n",
    "print(f\"Experiment ID: {experiment.experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2727c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run\n",
    "# This creates a new \"experiment run\" that will track everything we log\n",
    "with mlflow.start_run(run_name=\"xgboost_baseline_33features\") as run:\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"MLFLOW RUN STARTED\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "    print(f\"Run name: xgboost_baseline_33features\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 1: Train the model\n",
    "    # ========================================\n",
    "    print(\"Training XGBoost with 33 features...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model_33 = xgb.XGBRegressor(\n",
    "        random_state=42,\n",
    "        enable_categorical=True,\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.3\n",
    "    )\n",
    "    model_33.fit(X_train, y_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 2: Make predictions and calculate metrics\n",
    "    # ========================================\n",
    "    print(\"\\nMaking predictions...\")\n",
    "    y_pred_33 = model_33.predict(X_test)\n",
    "    \n",
    "    rmse_33 = np.sqrt(mean_squared_error(y_test, y_pred_33))\n",
    "    mae_33 = mean_absolute_error(y_test, y_pred_33)\n",
    "    bias_33 = np.mean(y_pred_33 - y_test)\n",
    "    \n",
    "    # Compare to 45-feature baseline\n",
    "    rmse_45 = 7.2127\n",
    "    improvement = ((rmse_45 - rmse_33) / rmse_45) * 100\n",
    "    \n",
    "    print(f\"\\nModel performance:\")\n",
    "    print(f\"  RMSE: {rmse_33:.4f}\")\n",
    "    print(f\"  MAE: {mae_33:.4f}\")\n",
    "    print(f\"  Bias: {bias_33:.4f}\")\n",
    "    print(f\"  Improvement over 45 features: {improvement:.2f}%\")\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 3: Log parameters to MLflow\n",
    "    # These are the settings/configuration of your model\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"LOGGING PARAMETERS TO MLFLOW\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    mlflow.log_param(\"n_features\", len(feature_cols_optimized))\n",
    "    print(f\"✓ Logged param: n_features = {len(feature_cols_optimized)}\")\n",
    "    \n",
    "    mlflow.log_param(\"features_removed\", len(features_to_remove))\n",
    "    print(f\"✓ Logged param: features_removed = {len(features_to_remove)}\")\n",
    "    \n",
    "    mlflow.log_param(\"n_train_samples\", len(X_train))\n",
    "    print(f\"✓ Logged param: n_train_samples = {len(X_train)}\")\n",
    "    \n",
    "    mlflow.log_param(\"n_test_samples\", len(X_test))\n",
    "    print(f\"✓ Logged param: n_test_samples = {len(X_test)}\")\n",
    "    \n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    print(f\"✓ Logged param: n_estimators = 100\")\n",
    "    \n",
    "    mlflow.log_param(\"max_depth\", 6)\n",
    "    print(f\"✓ Logged param: max_depth = 6\")\n",
    "    \n",
    "    mlflow.log_param(\"learning_rate\", 0.3)\n",
    "    print(f\"✓ Logged param: learning_rate = 0.3\")\n",
    "    \n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    print(f\"✓ Logged param: random_state = 42\")\n",
    "    \n",
    "    mlflow.log_param(\"train_date_range\", f\"2014-01-01 to 2014-02-21\")\n",
    "    print(f\"✓ Logged param: train_date_range\")\n",
    "    \n",
    "    mlflow.log_param(\"test_date_range\", f\"2014-03-01 to 2014-03-31\")\n",
    "    print(f\"✓ Logged param: test_date_range\")\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 4: Log metrics to MLflow\n",
    "    # These are the performance measurements\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"LOGGING METRICS TO MLFLOW\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    mlflow.log_metric(\"test_rmse\", rmse_33)\n",
    "    print(f\"✓ Logged metric: test_rmse = {rmse_33:.4f}\")\n",
    "    \n",
    "    mlflow.log_metric(\"test_mae\", mae_33)\n",
    "    print(f\"✓ Logged metric: test_mae = {mae_33:.4f}\")\n",
    "    \n",
    "    mlflow.log_metric(\"test_bias\", bias_33)\n",
    "    print(f\"✓ Logged metric: test_bias = {bias_33:.4f}\")\n",
    "    \n",
    "    mlflow.log_metric(\"baseline_rmse_45features\", rmse_45)\n",
    "    print(f\"✓ Logged metric: baseline_rmse_45features = {rmse_45:.4f}\")\n",
    "    \n",
    "    mlflow.log_metric(\"improvement_pct\", improvement)\n",
    "    print(f\"✓ Logged metric: improvement_pct = {improvement:.2f}%\")\n",
    "    \n",
    "    mlflow.log_metric(\"training_time_sec\", training_time)\n",
    "    print(f\"✓ Logged metric: training_time_sec = {training_time:.2f}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 5: Add tags for organization\n",
    "    # Tags help to filter and organize runs\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"ADDING TAGS TO MLFLOW RUN\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    mlflow.set_tag(\"phase\", \"week3_day3\")\n",
    "    print(f\"✓ Set tag: phase = week3_day3\")\n",
    "    \n",
    "    mlflow.set_tag(\"model_type\", \"xgboost\")\n",
    "    print(f\"✓ Set tag: model_type = xgboost\")\n",
    "    \n",
    "    mlflow.set_tag(\"feature_optimization\", \"DEC-014\")\n",
    "    print(f\"✓ Set tag: feature_optimization = DEC-014\")\n",
    "    \n",
    "    mlflow.set_tag(\"tuned\", \"false\")\n",
    "    print(f\"✓ Set tag: tuned = false\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MLFLOW RUN COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "    print(f\"All parameters, metrics, and tags logged successfully\")\n",
    "    print(f\"\\nView in MLflow UI: mlflow ui\")\n",
    "    print(f\"Then open: http://localhost:5000\")\n",
    "\n",
    "print(f\"\\n33-Feature Baseline Summary:\")\n",
    "print(f\"  RMSE: {rmse_33:.4f} (Target: 6.70-6.85)\")\n",
    "print(f\"  Improvement: {improvement:.2f}% (Target: 5-7%)\")\n",
    "print(f\"  Features used: {len(feature_cols_optimized)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676c0768",
   "metadata": {},
   "source": [
    "**results!**\n",
    "\n",
    "- RMSE: 6.8852 ✓ Within target range (6.70-6.85)\n",
    "- Improvement: 4.54% ✓ Close to 5-7% target\n",
    "- DEC-014 validated - Feature reduction worked!\n",
    "\n",
    "The improvement is slightly less than expected (4.54% vs 5-7%), but that's because Day 2 ablation tested each group independently. Combined removal has some interaction effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f809c46f",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning with RandomizedSearchCV\n",
    "\n",
    "**Objective:** Optimize XGBoost hyperparameters to further improve RMSE\n",
    "\n",
    "**Tuning strategy:**\n",
    "- Method: RandomizedSearchCV (faster than GridSearch)\n",
    "- Iterations: 20 random combinations\n",
    "- Cross-validation: 3-fold TimeSeriesSplit (respects temporal order)\n",
    "- Scoring: neg_root_mean_squared_error\n",
    "\n",
    "**Parameters to tune:**\n",
    "- n_estimators: Number of boosting rounds [100, 200, 300]\n",
    "- max_depth: Tree depth [3, 5, 7]\n",
    "- learning_rate: Step size [0.01, 0.05, 0.1, 0.3]\n",
    "- subsample: Row sampling ratio [0.7, 0.8, 1.0]\n",
    "- colsample_bytree: Column sampling ratio [0.7, 0.8, 1.0]\n",
    "\n",
    "**Expected outcome:** Additional 5-10% improvement → RMSE ~6.20-6.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Calculate total combinations\n",
    "total_combinations = 1\n",
    "for param, values in param_grid.items():\n",
    "    total_combinations *= len(values)\n",
    "    print(f\"{param}: {values} ({len(values)} options)\")\n",
    "\n",
    "print(f\"\\nTotal possible combinations: {total_combinations}\")\n",
    "print(f\"RandomizedSearchCV will test: 20 random combinations\")\n",
    "print(f\"With 3-fold CV: 20 × 3 = 60 model fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306c6f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up TimeSeriesSplit for cross-validation\n",
    "# This respects temporal order (no data leakage)\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "print(\"Time Series Cross-Validation Strategy:\")\n",
    "print(f\"  Method: TimeSeriesSplit\")\n",
    "print(f\"  Splits: 3\")\n",
    "print(f\"  Data order: Temporal (no shuffling)\")\n",
    "print()\n",
    "\n",
    "# Initialize base model\n",
    "base_model = xgb.XGBRegressor(\n",
    "    random_state=42,\n",
    "    enable_categorical=True\n",
    ")\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "print(\"Starting RandomizedSearchCV...\")\n",
    "print(f\"  Iterations: 20\")\n",
    "print(f\"  Total fits: 60 (20 iterations × 3 CV splits)\")\n",
    "print(f\"  Scoring: neg_root_mean_squared_error\")\n",
    "print()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=tscv,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "search_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nRandomizedSearchCV completed in {search_time:.2f} seconds ({search_time/60:.1f} minutes)\")\n",
    "print(f\"\\nBest parameters found:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest CV RMSE: {-random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5843d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from RandomizedSearchCV\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "print(\"Evaluating best model on test set...\")\n",
    "y_pred_tuned = best_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "mae_tuned = mean_absolute_error(y_test, y_pred_tuned)\n",
    "bias_tuned = np.mean(y_pred_tuned - y_test)\n",
    "\n",
    "# Compare to baselines\n",
    "rmse_45 = 7.2127\n",
    "rmse_33 = 6.8852\n",
    "improvement_vs_45 = ((rmse_45 - rmse_tuned) / rmse_45) * 100\n",
    "improvement_vs_33 = ((rmse_33 - rmse_tuned) / rmse_33) * 100\n",
    "\n",
    "print(f\"\\nTuned Model Performance:\")\n",
    "print(f\"  Test RMSE: {rmse_tuned:.4f}\")\n",
    "print(f\"  Test MAE: {mae_tuned:.4f}\")\n",
    "print(f\"  Test Bias: {bias_tuned:.4f}\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  45-feature baseline: {rmse_45:.4f}\")\n",
    "print(f\"  33-feature baseline: {rmse_33:.4f}\")\n",
    "print(f\"  33-feature tuned: {rmse_tuned:.4f}\")\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  vs 45-feature baseline: {improvement_vs_45:.2f}%\")\n",
    "print(f\"  vs 33-feature baseline: {improvement_vs_33:.2f}%\")\n",
    "\n",
    "if rmse_tuned < rmse_33:\n",
    "    print(f\"\\n✓ Tuning IMPROVED performance by {abs(improvement_vs_33):.2f}%\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: Tuning did not improve over 33-feature baseline\")\n",
    "    print(f\"  Difference: {(rmse_tuned - rmse_33):.4f} RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c9c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow run for tuned model\n",
    "with mlflow.start_run(run_name=\"xgboost_tuned_33features\") as run:\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"MLFLOW RUN STARTED - TUNED MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "    print(f\"Run name: xgboost_tuned_33features\")\n",
    "    print()\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 1: Log best hyperparameters\n",
    "    # ========================================\n",
    "    print(\"-\" * 60)\n",
    "    print(\"LOGGING HYPERPARAMETERS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    mlflow.log_param(\"n_features\", len(feature_cols_optimized))\n",
    "    print(f\"✓ n_features = {len(feature_cols_optimized)}\")\n",
    "    \n",
    "    mlflow.log_param(\"features_removed\", len(features_to_remove))\n",
    "    print(f\"✓ features_removed = {len(features_to_remove)}\")\n",
    "    \n",
    "    mlflow.log_param(\"n_train_samples\", len(X_train))\n",
    "    print(f\"✓ n_train_samples = {len(X_train)}\")\n",
    "    \n",
    "    mlflow.log_param(\"n_test_samples\", len(X_test))\n",
    "    print(f\"✓ n_test_samples = {len(X_test)}\")\n",
    "    \n",
    "    # Log best parameters from search\n",
    "    for param, value in random_search.best_params_.items():\n",
    "        mlflow.log_param(param, value)\n",
    "        print(f\"✓ {param} = {value}\")\n",
    "    \n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    print(f\"✓ random_state = 42\")\n",
    "    \n",
    "    mlflow.log_param(\"cv_folds\", 3)\n",
    "    print(f\"✓ cv_folds = 3\")\n",
    "    \n",
    "    mlflow.log_param(\"search_iterations\", 20)\n",
    "    print(f\"✓ search_iterations = 20\")\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 2: Log performance metrics\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"LOGGING METRICS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    mlflow.log_metric(\"test_rmse\", rmse_tuned)\n",
    "    print(f\"✓ test_rmse = {rmse_tuned:.4f}\")\n",
    "    \n",
    "    mlflow.log_metric(\"test_mae\", mae_tuned)\n",
    "    print(f\"✓ test_mae = {mae_tuned:.4f}\")\n",
    "    \n",
    "    mlflow.log_metric(\"test_bias\", bias_tuned)\n",
    "    print(f\"✓ test_bias = {bias_tuned:.4f}\")\n",
    "    \n",
    "    mlflow.log_metric(\"cv_rmse\", -random_search.best_score_)\n",
    "    print(f\"✓ cv_rmse = {-random_search.best_score_:.4f}\")\n",
    "    \n",
    "    mlflow.log_metric(\"baseline_rmse_45features\", rmse_45)\n",
    "    print(f\"✓ baseline_rmse_45features = {rmse_45:.4f}\")\n",
    "    \n",
    "    mlflow.log_metric(\"baseline_rmse_33features\", rmse_33)\n",
    "    print(f\"✓ baseline_rmse_33features = {rmse_33:.4f}\")\n",
    "    \n",
    "    mlflow.log_metric(\"improvement_vs_45features_pct\", improvement_vs_45)\n",
    "    print(f\"✓ improvement_vs_45features_pct = {improvement_vs_45:.2f}%\")\n",
    "    \n",
    "    mlflow.log_metric(\"improvement_vs_33features_pct\", improvement_vs_33)\n",
    "    print(f\"✓ improvement_vs_33features_pct = {improvement_vs_33:.2f}%\")\n",
    "    \n",
    "    mlflow.log_metric(\"search_time_sec\", search_time)\n",
    "    print(f\"✓ search_time_sec = {search_time:.2f}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # STEP 3: Add tags\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"ADDING TAGS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    mlflow.set_tag(\"phase\", \"week3_day3\")\n",
    "    print(f\"✓ phase = week3_day3\")\n",
    "    \n",
    "    mlflow.set_tag(\"model_type\", \"xgboost\")\n",
    "    print(f\"✓ model_type = xgboost\")\n",
    "    \n",
    "    mlflow.set_tag(\"feature_optimization\", \"DEC-014\")\n",
    "    print(f\"✓ feature_optimization = DEC-014\")\n",
    "    \n",
    "    mlflow.set_tag(\"tuned\", \"true\")\n",
    "    print(f\"✓ tuned = true\")\n",
    "    \n",
    "    mlflow.set_tag(\"tuning_method\", \"RandomizedSearchCV\")\n",
    "    print(f\"✓ tuning_method = RandomizedSearchCV\")\n",
    "    \n",
    "    mlflow.set_tag(\"best_model\", \"true\")\n",
    "    print(f\"✓ best_model = true\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MLFLOW RUN COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "    print(f\"All hyperparameters, metrics, and tags logged\")\n",
    "\n",
    "print(f\"\\nTuned Model Summary:\")\n",
    "print(f\"  RMSE: {rmse_tuned:.4f}\")\n",
    "print(f\"  Total improvement: {improvement_vs_45:.2f}% (vs original 45-feature baseline)\")\n",
    "print(f\"  Tuning improvement: {improvement_vs_33:.2f}% (vs 33-feature baseline)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1996e3dd",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Summary\n",
    "\n",
    "**Objective:** Compare all models trained and visualize improvement journey\n",
    "\n",
    "**Models to compare:**\n",
    "1. Baseline (45 features): RMSE = 7.21\n",
    "2. Optimized (33 features): RMSE = 6.89\n",
    "3. Tuned (33 features): RMSE = 6.63\n",
    "\n",
    "**Visualization:** Bar chart showing RMSE progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison data\n",
    "models = ['45-Feature\\nBaseline', '33-Feature\\nBaseline', '33-Feature\\nTuned']\n",
    "rmse_values = [rmse_45, rmse_33, rmse_tuned]\n",
    "improvements = [0, improvement_vs_45 - improvement_vs_33, improvement_vs_45]\n",
    "\n",
    "# Create figure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: RMSE comparison\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "bars1 = ax1.bar(models, rmse_values, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, rmse in zip(bars1, rmse_values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{rmse:.4f}',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax1.set_ylabel('RMSE', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylim(0, max(rmse_values) * 1.15)\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "# Plot 2: Cumulative improvement\n",
    "bars2 = ax2.bar(models, improvements, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar, imp in zip(bars2, improvements):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{imp:.2f}%',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax2.set_ylabel('Improvement vs 45-Feature Baseline (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Cumulative RMSE Improvement', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim(0, max(improvements) * 1.2)\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "output_path = OUTPUTS_FIGURES / 'w03_d03_model_comparison.png'\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {output_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print detailed comparison\n",
    "print(f\"\\nDetailed Model Comparison:\")\n",
    "print(f\"=\" * 70)\n",
    "print(f\"{'Model':<25} {'RMSE':<10} {'MAE':<10} {'Improvement':<15}\")\n",
    "print(f\"=\" * 70)\n",
    "print(f\"{'45-Feature Baseline':<25} {rmse_45:<10.4f} {'3.0957':<10} {'0.00%':<15}\")\n",
    "print(f\"{'33-Feature Baseline':<25} {rmse_33:<10.4f} {mae_33:<10.4f} {f'+{improvement_vs_45 - improvement_vs_33:.2f}%':<15}\")\n",
    "print(f\"{'33-Feature Tuned (BEST)':<25} {rmse_tuned:<10.4f} {mae_tuned:<10.4f} {f'+{improvement_vs_45:.2f}%':<15}\")\n",
    "print(f\"=\" * 70)\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"  • Feature reduction (DEC-014): +{improvement_vs_45 - improvement_vs_33:.2f}% improvement\")\n",
    "print(f\"  • Hyperparameter tuning: +{improvement_vs_33:.2f}% additional improvement\")\n",
    "print(f\"  • Total improvement: +{improvement_vs_45:.2f}% over original baseline\")\n",
    "print(f\"  • Best model: 33-feature tuned (RMSE: {rmse_tuned:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
