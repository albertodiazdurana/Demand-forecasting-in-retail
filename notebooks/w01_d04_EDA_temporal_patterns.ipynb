{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9fe2cf",
   "metadata": {},
   "source": [
    "# Corporación Favorita Grocery Sales Forecasting\n",
    "**w01_d04_EDA_temporal_patterns.ipynb**\n",
    "\n",
    "**Author:** Alberto Diaz Durana  \n",
    "**Date:** November 2025  \n",
    "**Purpose:** Engineer temporal features, visualize time series patterns, analyze product dynamics\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook accomplishes the following:\n",
    "\n",
    "- Create rolling statistics features (7/14/30-day moving averages)\n",
    "- Visualize overall sales trends (2013-2017)\n",
    "- Generate year-month heatmap for seasonality\n",
    "- Analyze autocorrelation for lag selection\n",
    "- Deep dive into day-of-week patterns by family\n",
    "- Investigate payday effects (1st, 15th of month)\n",
    "- Classify fast vs slow movers\n",
    "- Perform Pareto (80/20) analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Business Context\n",
    "\n",
    "**Why temporal patterns matter:**\n",
    "\n",
    "Understanding when sales occur enables:\n",
    "- Accurate forecasting of seasonal peaks/troughs\n",
    "- Optimal inventory allocation by day-of-week\n",
    "- Promotional timing optimization (payday effects)\n",
    "- Resource planning (staffing for high-traffic days)\n",
    "\n",
    "**Why product dynamics matter:**\n",
    "\n",
    "Identifying fast vs slow movers enables:\n",
    "- Differentiated inventory strategies\n",
    "- Focus on high-impact items (80/20 rule)\n",
    "- Better demand forecasting accuracy\n",
    "- Reduced waste for slow movers\n",
    "\n",
    "**Deliverables:**\n",
    "- Rolling average features (7/14/30-day windows)\n",
    "- Time series visualizations (trend, seasonality)\n",
    "- Autocorrelation analysis (lag structure)\n",
    "- Day-of-week and payday patterns\n",
    "- Fast/slow mover classification\n",
    "- Pareto chart (sales concentration)\n",
    "\n",
    "---\n",
    "\n",
    "## Input Dependencies\n",
    "\n",
    "From Day 3:\n",
    "- Clean dataset with temporal features (300K rows, 29 columns)\n",
    "- Store metadata merged (type, city, cluster)\n",
    "- No missing values, outliers flagged\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b605b",
   "metadata": {},
   "source": [
    "## 1. Feature Engineering - Rolling Statistics\n",
    "\n",
    "**Objective:** Create smoothed sales features using rolling windows\n",
    "\n",
    "**Activities:**\n",
    "- Load clean dataset from Day 3\n",
    "- Sort by (store_nbr, item_nbr, date) for temporal order\n",
    "- Calculate 7-day, 14-day, 30-day rolling means\n",
    "- Handle edge cases (min_periods parameter)\n",
    "- Visualize raw vs smoothed for sample items\n",
    "\n",
    "**Expected output:** \n",
    "- 3 new rolling average columns\n",
    "- Smoothing visualization showing effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ceb269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "# Configure environment\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"Package Versions:\")\n",
    "print(f\"  pandas: {pd.__version__}\")\n",
    "print(f\"  numpy: {np.__version__}\")\n",
    "print(f\"  matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"  seaborn: {sns.__version__}\")\n",
    "print(f\"  scipy: {scipy.__version__}\")\n",
    "print(\"\\nOK - Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa575de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine paths\n",
    "current_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "project_root = current_dir.parent if current_dir.name == 'notebooks' else current_dir\n",
    "\n",
    "# Define path constants\n",
    "DATA_PROCESSED = project_root / 'data' / 'processed'\n",
    "OUTPUTS = project_root / 'outputs' / 'figures' / 'eda'\n",
    "\n",
    "# Verify paths\n",
    "assert DATA_PROCESSED.exists(), f\"ERROR - Path not found: {DATA_PROCESSED}\"\n",
    "assert OUTPUTS.exists(), f\"ERROR - Path not found: {OUTPUTS}\"\n",
    "\n",
    "print(\"OK - Paths validated:\")\n",
    "print(f\"  Project root: {project_root.resolve()}\")\n",
    "print(f\"  DATA_PROCESSED: {DATA_PROCESSED.resolve()}\")\n",
    "print(f\"  OUTPUTS: {OUTPUTS.resolve()}\")\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(f\"\\nRandom seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4307e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean dataset from Day 3\n",
    "print(\"Loading clean dataset from Day 3...\")\n",
    "\n",
    "# Note: We'll load from pickle for speed since it has all Day 3 processing\n",
    "# But Day 3 didn't save a new pickle, so we need to reload and recreate\n",
    "\n",
    "# Check if we have a processed dataset from Day 3\n",
    "day3_pkl = DATA_PROCESSED / 'guayas_clean_day3.pkl'\n",
    "\n",
    "if day3_pkl.exists():\n",
    "    df = pd.read_pickle(day3_pkl)\n",
    "    print(f\"OK - Loaded from Day 3 pickle\")\n",
    "else:\n",
    "    # Load original sample and note we need to reapply Day 3 transformations\n",
    "    df = pd.read_pickle(DATA_PROCESSED / 'guayas_sample_300k.pkl')\n",
    "    print(\"NOTE - Loading original sample (Day 3 transformations need to be reapplied)\")\n",
    "    print(\"       This is expected if Day 3 notebook didn't export final dataset\")\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2928b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reapplication of Day 3 transformations\n",
    "print(\"Reapplying Day 3 transformations...\")\n",
    "\n",
    "# 1. Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# 2. Fill onpromotion NaN with 0\n",
    "df['onpromotion'] = df['onpromotion'].fillna(0.0)\n",
    "\n",
    "# 3. Load and merge store metadata\n",
    "df_stores = pd.read_csv(project_root / 'data' / 'raw' / 'stores.csv')\n",
    "df = df.merge(df_stores[['store_nbr', 'city', 'state', 'type', 'cluster']], \n",
    "              on='store_nbr', how='left')\n",
    "\n",
    "# 4. Create temporal features\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['day_of_month'] = df['date'].dt.day\n",
    "df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "\n",
    "print(f\"OK - Transformations applied\")\n",
    "print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "\n",
    "print(\"\\nDataset ready for feature engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d53b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data for rolling window calculations\n",
    "print(\"Preparing data for rolling statistics...\")\n",
    "print(\"Sorting by (store_nbr, item_nbr, date) for temporal order...\")\n",
    "\n",
    "# Critical: Sort by store, item, date for proper rolling windows\n",
    "df = df.sort_values(['store_nbr', 'item_nbr', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"OK - Data sorted\")\n",
    "print(f\"\\nFirst 5 rows (after sorting):\")\n",
    "print(df[['date', 'store_nbr', 'item_nbr', 'unit_sales', 'family']].head())\n",
    "\n",
    "print(f\"\\nLast 5 rows (after sorting):\")\n",
    "print(df[['date', 'store_nbr', 'item_nbr', 'unit_sales', 'family']].tail())\n",
    "\n",
    "# Verify temporal order within groups\n",
    "sample_group = df[(df['store_nbr'] == 24) & (df['item_nbr'] == df['item_nbr'].iloc[0])]\n",
    "print(f\"\\nSample group (Store 24, Item {df['item_nbr'].iloc[0]}):\")\n",
    "print(f\"  Rows: {len(sample_group)}\")\n",
    "print(f\"  Date range: {sample_group['date'].min().date()} to {sample_group['date'].max().date()}\")\n",
    "print(f\"  Dates in order: {sample_group['date'].is_monotonic_increasing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f95dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling statistics\n",
    "print(\"Computing rolling statistics...\")\n",
    "print(\"This may take 1-2 minutes for 300K rows...\\n\")\n",
    "\n",
    "# Calculate 7-day, 14-day, and 30-day rolling means per store-item group\n",
    "# Note: With sparse data, rolling windows operate on actual sales dates only\n",
    "df['unit_sales_7d_avg'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
    "    lambda x: x.rolling(window=7, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "df['unit_sales_14d_avg'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
    "    lambda x: x.rolling(window=14, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "df['unit_sales_30d_avg'] = df.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
    "    lambda x: x.rolling(window=30, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "print(f\"OK - Rolling statistics computed\")\n",
    "print(f\"\\nNew columns created:\")\n",
    "print(f\"  • unit_sales_7d_avg (7-day moving average)\")\n",
    "print(f\"  • unit_sales_14d_avg (14-day moving average)\")\n",
    "print(f\"  • unit_sales_30d_avg (30-day moving average)\")\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Display sample with rolling features\n",
    "print(\"\\nSample rows with rolling features:\")\n",
    "sample = df[df['item_nbr'] == df['item_nbr'].iloc[0]].head(10)\n",
    "print(sample[['date', 'store_nbr', 'item_nbr', 'unit_sales', \n",
    "              'unit_sales_7d_avg', 'unit_sales_14d_avg', 'unit_sales_30d_avg']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rolling statistics effectiveness\n",
    "print(\"Visualizing rolling statistics (raw vs smoothed)...\")\n",
    "\n",
    "# Select a high-volume item for clear visualization\n",
    "high_vol_items = df.groupby('item_nbr')['unit_sales'].sum().nlargest(5).index\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "for idx, item in enumerate(high_vol_items[:4]):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Get data for this item (all stores combined for clarity)\n",
    "    item_data = df[df['item_nbr'] == item].groupby('date').agg({\n",
    "        'unit_sales': 'sum',\n",
    "        'unit_sales_7d_avg': 'mean',\n",
    "        'unit_sales_14d_avg': 'mean',\n",
    "        'unit_sales_30d_avg': 'mean',\n",
    "        'family': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Sort by date\n",
    "    item_data = item_data.sort_values('date')\n",
    "    \n",
    "    # Plot\n",
    "    ax.plot(item_data['date'], item_data['unit_sales'], \n",
    "            alpha=0.3, color='gray', label='Raw Sales', linewidth=0.8)\n",
    "    ax.plot(item_data['date'], item_data['unit_sales_7d_avg'], \n",
    "            color='#2ca02c', label='7-day MA', linewidth=2)\n",
    "    ax.plot(item_data['date'], item_data['unit_sales_14d_avg'], \n",
    "            color='#ff7f0e', label='14-day MA', linewidth=2)\n",
    "    ax.plot(item_data['date'], item_data['unit_sales_30d_avg'], \n",
    "            color='#d62728', label='30-day MA', linewidth=2)\n",
    "    \n",
    "    ax.set_title(f\"Item #{item} ({item_data['family'].iloc[0]})\", \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Date', fontsize=10)\n",
    "    ax.set_ylabel('Unit Sales', fontsize=10)\n",
    "    ax.legend(loc='best', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Rolling Statistics - Smoothing Effect on Top 4 Items', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / '04_rolling_statistics_smoothing.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOK - Visualization saved to outputs/figures/eda/04_rolling_statistics_smoothing.png\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SECTION 1 COMPLETE - Feature Engineering (Rolling Statistics)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13ee45",
   "metadata": {},
   "source": [
    "## 2. Time Series Visualization\n",
    "\n",
    "**Objective:** Visualize overall sales trends and identify seasonal patterns\n",
    "\n",
    "**Activities:**\n",
    "- Aggregate total sales by date (300K → daily totals)\n",
    "- Plot 5-year time series (2013-2017)\n",
    "- Identify trend, seasonality, anomalies\n",
    "- Create year-month heatmap\n",
    "- Annotate major patterns (December peaks, Q1 lulls)\n",
    "\n",
    "**Expected output:** \n",
    "- Time series plot with trend analysis\n",
    "- Year-month heatmap\n",
    "- Pattern interpretation report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate sales by date for overall trend\n",
    "print(\"Time Series Visualization\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nAggregating sales by date...\")\n",
    "daily_sales = df.groupby('date').agg({\n",
    "    'unit_sales': 'sum',\n",
    "    'store_nbr': 'nunique',\n",
    "    'item_nbr': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "daily_sales.columns = ['date', 'total_sales', 'active_stores', 'active_items']\n",
    "\n",
    "print(f\"OK - Daily aggregation complete\")\n",
    "print(f\"  Date range: {daily_sales['date'].min().date()} to {daily_sales['date'].max().date()}\")\n",
    "print(f\"  Total days: {len(daily_sales)}\")\n",
    "print(f\"  Total sales: {daily_sales['total_sales'].sum():,.0f} units\")\n",
    "\n",
    "print(\"\\nDaily sales statistics:\")\n",
    "print(daily_sales[['total_sales', 'active_stores', 'active_items']].describe())\n",
    "\n",
    "print(\"\\nSample daily data:\")\n",
    "print(daily_sales.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffed9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overall time series trend\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Plot daily sales\n",
    "ax.plot(daily_sales['date'], daily_sales['total_sales'], \n",
    "        color='steelblue', linewidth=1, alpha=0.6, label='Daily Sales')\n",
    "\n",
    "# Add 30-day moving average for trend\n",
    "ma_30 = daily_sales['total_sales'].rolling(window=30, center=True).mean()\n",
    "ax.plot(daily_sales['date'], ma_30, \n",
    "        color='red', linewidth=3, label='30-day Moving Average', alpha=0.8)\n",
    "\n",
    "# Styling\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Total Sales (units)', fontsize=12)\n",
    "ax.set_title('Total Sales Time Series (2013-2017) - Guayas Region', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper left', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Format y-axis with comma separator\n",
    "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x):,}'))\n",
    "\n",
    "# Add vertical lines for year boundaries\n",
    "for year in range(2014, 2018):\n",
    "    ax.axvline(pd.Timestamp(f'{year}-01-01'), color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / '05_sales_time_series_overall.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOK - Time series plot saved\")\n",
    "\n",
    "# Identify patterns\n",
    "print(\"\\nPattern Analysis:\")\n",
    "print(f\"  Overall trend: {'Increasing' if ma_30.iloc[-100] > ma_30.iloc[100] else 'Stable/Decreasing'}\")\n",
    "print(f\"  Peak sales: {daily_sales['total_sales'].max():,.0f} on {daily_sales.loc[daily_sales['total_sales'].idxmax(), 'date'].date()}\")\n",
    "print(f\"  Lowest sales: {daily_sales['total_sales'].min():,.0f} on {daily_sales.loc[daily_sales['total_sales'].idxmin(), 'date'].date()}\")\n",
    "\n",
    "# Check for December peaks (holiday season)\n",
    "dec_sales = df[df['month'] == 12].groupby('date')['unit_sales'].sum()\n",
    "avg_dec = dec_sales.mean()\n",
    "avg_overall = daily_sales['total_sales'].mean()\n",
    "print(f\"  December avg: {avg_dec:.0f} vs Overall avg: {avg_overall:.0f} ({(avg_dec/avg_overall - 1)*100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create year-month heatmap for seasonality\n",
    "print(\"\\nCreating year-month heatmap...\")\n",
    "\n",
    "# Aggregate by year and month\n",
    "monthly_sales = df.groupby(['year', 'month'])['unit_sales'].sum().reset_index()\n",
    "monthly_pivot = monthly_sales.pivot(index='month', columns='year', values='unit_sales')\n",
    "\n",
    "# Calculate percentage of annual average for color scaling\n",
    "monthly_pct = monthly_pivot.div(monthly_pivot.mean(axis=0), axis=1) * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(monthly_pct, annot=True, fmt='.0f', cmap='RdYlGn', \n",
    "            center=100, vmin=70, vmax=130,\n",
    "            cbar_kws={'label': '% of Annual Average'},\n",
    "            linewidths=1, linecolor='white', ax=ax)\n",
    "\n",
    "# Styling\n",
    "ax.set_xlabel('Year', fontsize=12)\n",
    "ax.set_ylabel('Month', fontsize=12)\n",
    "ax.set_title('Sales Seasonality Heatmap (% of Annual Average)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Month labels\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "ax.set_yticklabels(month_names, rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / '06_sales_seasonality_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOK - Heatmap saved\")\n",
    "\n",
    "# Identify seasonal patterns\n",
    "print(\"\\nSeasonal Patterns:\")\n",
    "monthly_avg = df.groupby('month')['unit_sales'].sum().sort_values(ascending=False)\n",
    "print(f\"  Strongest months: {', '.join([month_names[m-1] for m in monthly_avg.head(3).index])}\")\n",
    "print(f\"  Weakest months: {', '.join([month_names[m-1] for m in monthly_avg.tail(3).index])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SECTION 2 COMPLETE - Time Series Visualization\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2de26d",
   "metadata": {},
   "source": [
    "## 3. Autocorrelation Analysis\n",
    "\n",
    "**Objective:** Assess temporal dependence to guide lag feature selection\n",
    "\n",
    "**Activities:**\n",
    "- Aggregate daily sales for autocorrelation calculation\n",
    "- Plot autocorrelation function (ACF)\n",
    "- Interpret lag significance\n",
    "- Document findings for Week 2 feature engineering\n",
    "\n",
    "**Expected output:** \n",
    "- Autocorrelation plot\n",
    "- Lag analysis interpretation\n",
    "- Recommendations for lag features (1, 7, 14, 30 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c7ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation analysis\n",
    "print(\"Autocorrelation Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nCalculating autocorrelation for daily sales...\")\n",
    "\n",
    "# Use daily aggregated sales for ACF\n",
    "daily_sales_sorted = daily_sales.sort_values('date')\n",
    "\n",
    "# Plot autocorrelation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# ACF plot\n",
    "ax1 = axes[0]\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(daily_sales_sorted['total_sales'], ax=ax1)\n",
    "ax1.set_title('Autocorrelation Function (ACF) - Daily Sales', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax1.set_xlabel('Lag (days)', fontsize=11)\n",
    "ax1.set_ylabel('Autocorrelation', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0, color='k', linestyle='-', linewidth=0.8)\n",
    "ax1.axhline(y=0.05, color='r', linestyle='--', alpha=0.5, label='±5% threshold')\n",
    "ax1.axhline(y=-0.05, color='r', linestyle='--', alpha=0.5)\n",
    "ax1.legend()\n",
    "\n",
    "# Manual ACF calculation for specific lags\n",
    "ax2 = axes[1]\n",
    "lags = [1, 7, 14, 30, 60, 90]\n",
    "acf_values = []\n",
    "\n",
    "for lag in lags:\n",
    "    corr = daily_sales_sorted['total_sales'].autocorr(lag=lag)\n",
    "    acf_values.append(corr)\n",
    "\n",
    "bars = ax2.bar(range(len(lags)), acf_values, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax2.set_xticks(range(len(lags)))\n",
    "ax2.set_xticklabels([f'{lag}d' for lag in lags])\n",
    "ax2.set_xlabel('Lag', fontsize=11)\n",
    "ax2.set_ylabel('Autocorrelation', fontsize=11)\n",
    "ax2.set_title('Autocorrelation at Key Lags', fontsize=13, fontweight='bold')\n",
    "ax2.axhline(y=0, color='k', linestyle='-', linewidth=0.8)\n",
    "ax2.axhline(y=0.05, color='r', linestyle='--', alpha=0.5)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, acf_values):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{val:.3f}', ha='center', va='bottom' if val > 0 else 'top', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / '07_autocorrelation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOK - Autocorrelation plot saved\")\n",
    "\n",
    "# Interpret results\n",
    "print(\"\\nAutocorrelation at Key Lags:\")\n",
    "for lag, acf in zip(lags, acf_values):\n",
    "    significance = \"Strong\" if abs(acf) > 0.3 else \"Moderate\" if abs(acf) > 0.1 else \"Weak\"\n",
    "    print(f\"  Lag {lag:>2}d: {acf:>6.3f} ({significance})\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  → Lag 1 (yesterday): Strong autocorrelation expected\")\n",
    "print(\"  → Lag 7 (week): Weekly patterns if significant\")\n",
    "print(\"  → Lag 30 (month): Monthly cycles if significant\")\n",
    "\n",
    "print(\"\\nRecommendations for Week 2:\")\n",
    "print(\"  • Include lag features: 1, 7, 14 days (strong temporal dependence)\")\n",
    "print(\"  • Consider day-of-week effects (weekly patterns)\")\n",
    "print(\"  • Monthly seasonality features useful (lag 30 moderate)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SECTION 3 COMPLETE - Autocorrelation Analysis\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6201db0",
   "metadata": {},
   "source": [
    "## 4. Temporal Deep Dive\n",
    "\n",
    "**Objective:** Uncover day-of-week, monthly, and payday patterns\n",
    "\n",
    "**Activities:**\n",
    "- Analyze day-of-week patterns by product family\n",
    "- Compare weekend vs weekday sales\n",
    "- Investigate payday effects (1st and 15th of month)\n",
    "- Monthly seasonality by family\n",
    "- End-of-month patterns\n",
    "\n",
    "**Expected output:** \n",
    "- Day-of-week analysis by family\n",
    "- Payday effect quantification\n",
    "- Monthly patterns comparison\n",
    "- Temporal insights for inventory planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6325976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day-of-week patterns analysis\n",
    "print(\"Temporal Deep Dive\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. Day-of-Week Patterns by Family:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Aggregate by day-of-week and family\n",
    "dow_family = df.groupby(['day_of_week', 'family'])['unit_sales'].sum().reset_index()\n",
    "dow_family_pivot = dow_family.pivot(index='day_of_week', columns='family', values='unit_sales')\n",
    "\n",
    "# Calculate percentage of weekly average\n",
    "dow_family_pct = dow_family_pivot.div(dow_family_pivot.mean(axis=0), axis=1) * 100\n",
    "\n",
    "print(\"\\nSales by Day-of-Week (% of weekly average):\")\n",
    "dow_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "dow_family_pct.index = dow_names\n",
    "print(dow_family_pct.round(1))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Set day names ONCE before plotting\n",
    "dow_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "dow_family_pivot.index = dow_names\n",
    "dow_family_pct.index = dow_names\n",
    "\n",
    "# Plot 1: Absolute sales by day-of-week\n",
    "ax1 = axes[0]\n",
    "dow_family_pivot.plot(kind='bar', ax=ax1, width=0.8, alpha=0.8)\n",
    "ax1.set_xlabel('Day of Week', fontsize=11)\n",
    "ax1.set_ylabel('Total Sales (units)', fontsize=11)\n",
    "ax1.set_title('Sales by Day-of-Week and Family', fontsize=13, fontweight='bold')\n",
    "ax1.legend(title='Family', fontsize=10, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.tick_params(axis='x', rotation=0)  # Use tick_params instead of set_xticklabels\n",
    "\n",
    "# Plot 2: Percentage of weekly average\n",
    "ax2 = axes[1]\n",
    "dow_family_pct.plot(kind='line', ax=ax2, marker='o', linewidth=2.5, markersize=8)\n",
    "ax2.set_xlabel('Day of Week', fontsize=11)\n",
    "ax2.set_ylabel('% of Weekly Average', fontsize=11)\n",
    "ax2.set_title('Day-of-Week Pattern (% of Average)', fontsize=13, fontweight='bold')\n",
    "ax2.axhline(y=100, color='gray', linestyle='--', alpha=0.5, label='Average')\n",
    "ax2.legend(title='Family', fontsize=10, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / '08_day_of_week_patterns.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOK - Day-of-week visualization saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2234da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekend vs Weekday comparison\n",
    "print(\"\\n2. Weekend vs Weekday Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "weekend_comparison = df.groupby(['is_weekend', 'family'])['unit_sales'].agg(['sum', 'mean', 'count']).reset_index()\n",
    "weekend_comparison['is_weekend'] = weekend_comparison['is_weekend'].map({0: 'Weekday', 1: 'Weekend'})\n",
    "\n",
    "print(\"\\nSales by Weekend/Weekday:\")\n",
    "for family in df['family'].unique():\n",
    "    family_data = weekend_comparison[weekend_comparison['family'] == family]\n",
    "    weekday_sales = family_data[family_data['is_weekend'] == 'Weekday']['sum'].values[0]\n",
    "    weekend_sales = family_data[family_data['is_weekend'] == 'Weekend']['sum'].values[0]\n",
    "    weekend_lift = (weekend_sales / weekday_sales - 1) * 100\n",
    "    print(f\"  {family:<15} Weekend lift: {weekend_lift:>+6.1f}%\")\n",
    "\n",
    "# Overall weekend effect\n",
    "overall_weekday = df[df['is_weekend'] == 0]['unit_sales'].sum()\n",
    "overall_weekend = df[df['is_weekend'] == 1]['unit_sales'].sum()\n",
    "overall_lift = (overall_weekend / overall_weekday - 1) * 100\n",
    "print(f\"  {'Overall':<15} Weekend lift: {overall_lift:>+6.1f}%\")\n",
    "\n",
    "print(f\"\\nWeekend accounts for: {df[df['is_weekend'] == 1]['unit_sales'].sum() / df['unit_sales'].sum() * 100:.1f}% of total sales\")\n",
    "print(f\"  (Expected if uniform: 28.6% = 2 days / 7 days)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e39c8",
   "metadata": {},
   "source": [
    "This is not right...\n",
    "The calculation is misleading: Comparing total weekend (2 days) vs total weekday (5 days) shows negative lift, BUT weekend accounts for 34.9% of sales (vs expected 28.6%).\n",
    "This means weekends ARE elevated (34.9% / 28.6% = 22% higher than expected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e2cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct weekend analysis - compare DAILY averages\n",
    "print(\"\\n3. Corrected Weekend Analysis (Daily Averages):\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate average daily sales\n",
    "weekday_daily_avg = df[df['is_weekend'] == 0]['unit_sales'].sum() / df[df['is_weekend'] == 0]['date'].nunique()\n",
    "weekend_daily_avg = df[df['is_weekend'] == 1]['unit_sales'].sum() / df[df['is_weekend'] == 1]['date'].nunique()\n",
    "\n",
    "print(f\"\\nAverage daily sales:\")\n",
    "print(f\"  Weekday: {weekday_daily_avg:>8,.1f} units/day\")\n",
    "print(f\"  Weekend: {weekend_daily_avg:>8,.1f} units/day\")\n",
    "print(f\"  Weekend lift: {(weekend_daily_avg / weekday_daily_avg - 1) * 100:>+6.1f}%\")\n",
    "\n",
    "# By family\n",
    "print(\"\\nWeekend lift by family (daily averages):\")\n",
    "for family in df['family'].unique():\n",
    "    family_df = df[df['family'] == family]\n",
    "    weekday_avg = family_df[family_df['is_weekend'] == 0]['unit_sales'].sum() / family_df[family_df['is_weekend'] == 0]['date'].nunique()\n",
    "    weekend_avg = family_df[family_df['is_weekend'] == 1]['unit_sales'].sum() / family_df[family_df['is_weekend'] == 1]['date'].nunique()\n",
    "    lift = (weekend_avg / weekday_avg - 1) * 100\n",
    "    print(f\"  {family:<15} {lift:>+6.1f}%\")\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"  → Weekends have significantly higher daily sales (~22% lift)\")\n",
    "print(\"  → Saturday/Sunday are peak shopping days (grocery restocking)\")\n",
    "print(\"  → Inventory should be elevated for weekend demand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f63f8a",
   "metadata": {},
   "source": [
    "CORRECTED: Weekend lift is +33.9% (BEVERAGES highest at +40.2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68291795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payday effects analysis (1st and 15th of month)\n",
    "print(\"\\n4. Payday Effects Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define payday windows (1st ±2 days, 15th ±2 days)\n",
    "df['is_payday_window'] = df['day_of_month'].isin([1, 2, 3, 14, 15, 16]).astype(int)\n",
    "\n",
    "payday_sales = df[df['is_payday_window'] == 1]['unit_sales'].sum()\n",
    "non_payday_sales = df[df['is_payday_window'] == 0]['unit_sales'].sum()\n",
    "\n",
    "payday_days = df[df['is_payday_window'] == 1]['date'].nunique()\n",
    "non_payday_days = df[df['is_payday_window'] == 0]['date'].nunique()\n",
    "\n",
    "payday_daily_avg = payday_sales / payday_days\n",
    "non_payday_daily_avg = non_payday_sales / non_payday_days\n",
    "\n",
    "print(f\"\\nPayday window (1st ±2, 15th ±2 days of month):\")\n",
    "print(f\"  Payday window avg: {payday_daily_avg:>8,.1f} units/day ({payday_days} days)\")\n",
    "print(f\"  Non-payday avg:    {non_payday_daily_avg:>8,.1f} units/day ({non_payday_days} days)\")\n",
    "print(f\"  Payday lift:       {(payday_daily_avg / non_payday_daily_avg - 1) * 100:>+6.1f}%\")\n",
    "\n",
    "# Specific day analysis\n",
    "day_of_month_sales = df.groupby('day_of_month').agg({\n",
    "    'unit_sales': 'sum',\n",
    "    'date': 'nunique'\n",
    "}).reset_index()\n",
    "day_of_month_sales['daily_avg'] = day_of_month_sales['unit_sales'] / day_of_month_sales['date']\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "bars = ax.bar(day_of_month_sales['day_of_month'], day_of_month_sales['daily_avg'], \n",
    "              color='steelblue', alpha=0.7, edgecolor='black')\n",
    "\n",
    "# Highlight payday windows\n",
    "payday_days_list = [1, 2, 3, 14, 15, 16]\n",
    "for i, day in enumerate(day_of_month_sales['day_of_month']):\n",
    "    if day in payday_days_list:\n",
    "        bars[i].set_color('#2ca02c')\n",
    "        bars[i].set_alpha(0.9)\n",
    "\n",
    "ax.set_xlabel('Day of Month', fontsize=12)\n",
    "ax.set_ylabel('Average Daily Sales', fontsize=12)\n",
    "ax.set_title('Sales by Day of Month (Payday Effect)', fontsize=14, fontweight='bold')\n",
    "ax.axhline(y=day_of_month_sales['daily_avg'].mean(), color='red', linestyle='--', \n",
    "           alpha=0.5, label='Monthly Average')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / '09_payday_effects.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOK - Payday effects visualization saved\")\n",
    "\n",
    "print(\"\\nTop 5 days of month by sales:\")\n",
    "top_days = day_of_month_sales.nlargest(5, 'daily_avg')[['day_of_month', 'daily_avg']]\n",
    "for _, row in top_days.iterrows():\n",
    "    payday_flag = \" (PAYDAY)\" if row['day_of_month'] in payday_days_list else \"\"\n",
    "    print(f\"  Day {row['day_of_month']:>2}: {row['daily_avg']:>8,.1f} units/day{payday_flag}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SECTION 4 COMPLETE - Temporal Deep Dive\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa09728",
   "metadata": {},
   "source": [
    "## 5. Product Analysis - Fast vs Slow Movers\n",
    "\n",
    "**Objective:** Classify items by sales velocity and identify sales concentration\n",
    "\n",
    "**Activities:**\n",
    "- Calculate sales velocity per item (total sales / days active)\n",
    "- Classify fast movers (top 20%), slow movers (bottom 20%)\n",
    "- Perform Pareto (80/20) analysis\n",
    "- Analyze sales concentration by family\n",
    "- Identify hero products\n",
    "\n",
    "**Expected output:** \n",
    "- Fast/slow mover classification\n",
    "- Pareto chart showing sales concentration\n",
    "- Hero products list per family\n",
    "- Inventory recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fcfa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product analysis - Fast vs Slow movers\n",
    "print(\"Product Analysis - Fast vs Slow Movers\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nCalculating sales velocity per item...\")\n",
    "\n",
    "# Calculate total sales and days active per item\n",
    "item_performance = df.groupby('item_nbr').agg({\n",
    "    'unit_sales': 'sum',\n",
    "    'date': 'nunique',\n",
    "    'family': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "item_performance.columns = ['item_nbr', 'total_sales', 'days_active', 'family']\n",
    "\n",
    "# Calculate velocity (sales per active day)\n",
    "item_performance['velocity'] = item_performance['total_sales'] / item_performance['days_active']\n",
    "\n",
    "# Sort by total sales\n",
    "item_performance = item_performance.sort_values('total_sales', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f\"OK - Item performance calculated\")\n",
    "print(f\"  Total items: {len(item_performance):,}\")\n",
    "print(f\"  Total sales: {item_performance['total_sales'].sum():,.0f} units\")\n",
    "\n",
    "print(\"\\nTop 10 items by total sales:\")\n",
    "print(item_performance.head(10)[['item_nbr', 'family', 'total_sales', 'days_active', 'velocity']])\n",
    "\n",
    "print(\"\\nBottom 10 items by total sales:\")\n",
    "print(item_performance.tail(10)[['item_nbr', 'family', 'total_sales', 'days_active', 'velocity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07397b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify fast vs slow movers\n",
    "print(\"\\nClassifying Fast vs Slow Movers:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define thresholds (top 20% = fast, bottom 20% = slow)\n",
    "velocity_80th = item_performance['velocity'].quantile(0.80)\n",
    "velocity_20th = item_performance['velocity'].quantile(0.20)\n",
    "\n",
    "item_performance['mover_type'] = 'Medium'\n",
    "item_performance.loc[item_performance['velocity'] >= velocity_80th, 'mover_type'] = 'Fast'\n",
    "item_performance.loc[item_performance['velocity'] <= velocity_20th, 'mover_type'] = 'Slow'\n",
    "\n",
    "# Count by type\n",
    "mover_counts = item_performance['mover_type'].value_counts()\n",
    "print(f\"\\nItem classification:\")\n",
    "print(f\"  Fast movers (top 20%):    {mover_counts['Fast']:>5,} items ({mover_counts['Fast']/len(item_performance)*100:>5.1f}%)\")\n",
    "print(f\"  Medium movers (mid 60%):  {mover_counts['Medium']:>5,} items ({mover_counts['Medium']/len(item_performance)*100:>5.1f}%)\")\n",
    "print(f\"  Slow movers (bottom 20%): {mover_counts['Slow']:>5,} items ({mover_counts['Slow']/len(item_performance)*100:>5.1f}%)\")\n",
    "\n",
    "print(f\"\\nVelocity thresholds:\")\n",
    "print(f\"  Fast mover: ≥ {velocity_80th:.2f} units/day\")\n",
    "print(f\"  Slow mover: ≤ {velocity_20th:.2f} units/day\")\n",
    "\n",
    "# Sales contribution by mover type\n",
    "sales_by_type = item_performance.groupby('mover_type')['total_sales'].sum().sort_values(ascending=False)\n",
    "print(f\"\\nSales contribution:\")\n",
    "for mover_type, sales in sales_by_type.items():\n",
    "    pct = sales / item_performance['total_sales'].sum() * 100\n",
    "    print(f\"  {mover_type:<12} {sales:>10,.0f} units ({pct:>5.1f}%)\")\n",
    "\n",
    "# By family\n",
    "print(f\"\\nMover classification by family:\")\n",
    "family_movers = item_performance.groupby(['family', 'mover_type']).size().unstack(fill_value=0)\n",
    "print(family_movers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto (80/20) analysis\n",
    "print(\"\\nPareto (80/20) Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate cumulative sales\n",
    "item_performance['cumulative_sales'] = item_performance['total_sales'].cumsum()\n",
    "item_performance['cumulative_pct'] = (item_performance['cumulative_sales'] / \n",
    "                                       item_performance['total_sales'].sum() * 100)\n",
    "\n",
    "# Find 80% threshold\n",
    "items_for_80pct = (item_performance['cumulative_pct'] <= 80).sum()\n",
    "pct_items_for_80 = items_for_80pct / len(item_performance) * 100\n",
    "\n",
    "print(f\"\\nPareto Principle:\")\n",
    "print(f\"  {items_for_80pct:,} items ({pct_items_for_80:.1f}%) generate 80% of sales\")\n",
    "print(f\"  Top 20% of items ({int(len(item_performance)*0.2):,} items) generate {item_performance.head(int(len(item_performance)*0.2))['total_sales'].sum() / item_performance['total_sales'].sum() * 100:.1f}% of sales\")\n",
    "\n",
    "# Visualize Pareto chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Cumulative sales curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot(range(len(item_performance)), item_performance['cumulative_pct'], \n",
    "         color='steelblue', linewidth=2.5)\n",
    "ax1.axhline(y=80, color='red', linestyle='--', alpha=0.7, label='80% threshold')\n",
    "ax1.axvline(x=items_for_80pct, color='red', linestyle='--', alpha=0.7)\n",
    "ax1.fill_between(range(items_for_80pct), 0, 80, alpha=0.2, color='green', \n",
    "                  label=f'{items_for_80pct} items for 80%')\n",
    "ax1.set_xlabel('Number of Items (ranked by sales)', fontsize=11)\n",
    "ax1.set_ylabel('Cumulative Sales %', fontsize=11)\n",
    "ax1.set_title('Pareto Chart - Sales Concentration', fontsize=13, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.set_xlim(0, len(item_performance))\n",
    "ax1.set_ylim(0, 105)\n",
    "\n",
    "# Plot 2: Sales distribution by mover type\n",
    "ax2 = axes[1]\n",
    "colors_map = {'Fast': '#2ca02c', 'Medium': '#ff7f0e', 'Slow': '#d62728'}\n",
    "sales_data = item_performance.groupby('mover_type')['total_sales'].sum().reindex(['Fast', 'Medium', 'Slow'])\n",
    "bars = ax2.bar(range(len(sales_data)), sales_data.values, \n",
    "               color=[colors_map[x] for x in sales_data.index], alpha=0.7, edgecolor='black')\n",
    "ax2.set_xticks(range(len(sales_data)))\n",
    "ax2.set_xticklabels(sales_data.index)\n",
    "ax2.set_ylabel('Total Sales (units)', fontsize=11)\n",
    "ax2.set_title('Sales by Mover Type', fontsize=13, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for bar, (mover, sales) in zip(bars, sales_data.items()):\n",
    "    height = bar.get_height()\n",
    "    pct = sales / item_performance['total_sales'].sum() * 100\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{sales:,.0f}\\n({pct:.1f}%)', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUTS / '10_pareto_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOK - Pareto visualization saved\")\n",
    "\n",
    "# Hero products by family\n",
    "print(f\"\\nHero Products (Top 5 per family):\")\n",
    "for family in item_performance['family'].unique():\n",
    "    print(f\"\\n{family}:\")\n",
    "    top_items = item_performance[item_performance['family'] == family].head(5)\n",
    "    for idx, row in top_items.iterrows():\n",
    "        print(f\"  Item #{row['item_nbr']}: {row['total_sales']:>8,.0f} units ({row['velocity']:>6.2f} units/day) - {row['mover_type']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SECTION 5 COMPLETE - Product Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DAY 4 COMPLETE - Temporal Patterns & Product Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nKey Accomplishments:\")\n",
    "print(\"  ✓ Rolling statistics features (7/14/30-day)\")\n",
    "print(\"  ✓ Time series visualization (increasing trend, December +30%)\")\n",
    "print(\"  ✓ Autocorrelation analysis (strong at all lags)\")\n",
    "print(\"  ✓ Day-of-week patterns (weekends +34% lift)\")\n",
    "print(\"  ✓ Payday effects (+11% lift, Day 1 peak)\")\n",
    "print(\"  ✓ Fast/slow mover classification (20/60/20 split)\")\n",
    "print(\"  ✓ Pareto analysis (20% items = 58% sales)\")\n",
    "\n",
    "print(f\"\\nTime spent: ~3.5 hours / 5 hours allocated\")\n",
    "print(f\"Status: 1.5 hours under budget! ✓\")\n",
    "\n",
    "print(\"\\nReady for Day 5:\")\n",
    "print(\"  → Holiday impact analysis\")\n",
    "print(\"  → Promotion effectiveness\")\n",
    "print(\"  → Perishable deep dive\")\n",
    "print(\"  → Final dataset export\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
